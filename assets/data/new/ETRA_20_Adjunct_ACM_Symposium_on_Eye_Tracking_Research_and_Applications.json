[
  {
    "title": "GIUPlayer: A Gaze Immersive YouTube Player Enabling Eye Control and Attention Analysis",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Demos",
    "data": "June 2020",
    "authors": [
      "Ramin Hedeshy Hedeshy",
      "Chandan Kumar",
      "Raphael Menges",
      "Steffen Staab"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391984",
    "citation": "0",
    "abstract": "We developed a gaze immersive YouTube player, called GIUPlayer, with two objectives: First to enable eye-controlled interaction with video content, to support people with motor disabilities. Second to enable the prospect of quantifying attention when users view video content, which can be used to estimate natural viewing behaviour. In this paper, we illustrate the functionality and design of GIUPlayer, and the visualization of video viewing pattern. The long-term perspective of this work could lead to the realization of eye control and attention based recommendations in online video platforms and smart TV applications that record eye tracking data."
  },
  {
    "title": "Demo of a Visual Gaze Analysis System for Virtual Board Games",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Demos",
    "data": "June 2020",
    "authors": [
      "Tanja Munz",
      "Noel Schaefer",
      "Tanja Blascheck",
      "Kuno Kurzhals",
      "Eugene Zhang",
      "Daniel Weiskopf"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391985",
    "citation": "1",
    "abstract": "We demonstrate a system for the visual analysis of eye movement data of competitive and collaborative virtual board games two persons play. Our approach uses methods to temporally synchronize and spatially register gaze and mouse recordings from two possibly different eye tracking devices. Analysts can then examine such fused data with a combination of visualizations. We demonstrate our methods for the competitive game Go, which is especially complex for the analysis of strategies of individual players."
  },
  {
    "title": "Synopticon: Sensor Fusion for Automated Gaze Analysis",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Demos",
    "data": "June 2020",
    "authors": [
      "Michael Hildebrandt",
      "Jens-Patrick Langstrand",
      "Hoa Thi Nguyen"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391986",
    "citation": "0",
    "abstract": "This demonstration presents Synopticon, an open-source software system for automatic, real-time gaze object detection for mobile eye tracking. The system merges gaze data from eye tracking glasses with position data from a motion capture system and projects the resulting gaze vector onto a 3D model of the environment."
  },
  {
    "title": "Demo of the EyeSAC System for Visual Synchronization, Cleaning, and Annotation of Eye Movement Data",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Demos",
    "data": "June 2020",
    "authors": [
      "Ayush Kumar",
      "Debesh Mohanty",
      "Kuno Kurzhals",
      "Fabian Beck",
      "Daniel Weiskopf",
      "Klaus Mueller"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391988",
    "citation": "2",
    "abstract": "Eye movement data analysis plays an important role in examining human cognitive processes and perceptions. Such analysis at times needs data recording from additional sources too during experiments. In this paper, we study a pair programming based collaboration using two eye trackers, stimulus recording, and an external camera recording. To analyze the collected data, we introduce the EyeSAC system that synchronizes the data from different sources and that removes the noisy and missing gazes from eye tracking data with the help of visual feedback from the external recording. The synchronized and cleaned data is further annotated using our system and then exported for further analysis."
  },
  {
    "title": "Eye Tracking in Ocular Proton Therapy",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Videos",
    "data": "June 2020",
    "authors": [
      "Riccardo Via",
      "Giovanni Fattori",
      "Alessia Pica",
      "Antony Lomax",
      "Guido Baroni",
      "Damien Charles Weber",
      "Jan Hrbacek"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391991",
    "citation": "0",
    "abstract": "Optical eye tracking solutions can play a role in patient alignment and real-time monitoring of the eye movements during ocular proton therapy (OPT) and replace the current clinical standard based on radiographic imaging of surgical clips implanted in the patient's eye. The aim of this study is to compare the performance of an eye tracking solution specifically developed for OPT applications to this clinical standard. The eye tracking system (ETS) was used to pre-align the patient to the treatment position using information based solely on the pupil position before correction administered through X-ray imaging. As a result, we compared the geometrical accuracy achieved with the ETS to X-ray imaging of clips. In addition, we evaluated the ability of the ETS in determining the real position of the pupil through a comparison with a geometrical eye model. Pupil-based patient alignment performed, on average, worse than the conventional approach based on clips and a patient-specific bias was observed in the assessment of the pupil center position between ETS and the eye model. The limited accuracy of the ETS id due to the adoption of a simplified eye tracking approach and current investigation are focused into integrating gaze direction estimation in the process."
  },
  {
    "title": "Looking Outside the Box: Gaze-Pointing for Augmentative and Alternative Communication",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Videos",
    "data": "June 2020",
    "authors": ["Iyad Aldaqre"],
    "DOI": "https://doi.org/10.1145/3379157.3391987",
    "citation": "0",
    "abstract": "Augmentative and Alternative Communication (AAC) provides different methods for people with disabilities to communicate. By employing eye tracking, these methods allowed the use of aided AAC autonomously, without the need for a caregiver to assist the user in choosing what they want to convey. However, these methods focus on verbal communication, which typically covers only a small portion of our daily communication. We present a system that can be integrated in modern AAC devices to allow users with impaired communication and mobility to take a picture of what is in front of them, zoom-in at a specific portion of the picture and share it with others. Such a simple solution could provide an alternative to pointing gestures, allowing users to express preferences to real objects in their environment. Other use cases of this system are discussed."
  },
  {
    "title": "MIRA – A Gaze-based Serious Game for Continuous Estimation of Alzheimer's Mental State",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Videos",
    "data": "June 2020",
    "authors": [
      "Lucas Paletta",
      "Martin Pszeida",
      "Amir Dini",
      "Silvia Russegger",
      "Sandra Schuessler",
      "Anna Jos",
      "Eva Schuster",
      "Josef Steiner",
      "Maria Fellner"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391989",
    "citation": "6",
    "abstract": "Persons with Alzheimer's disease demonstrate a dysfunctionality in the continuous tracking of stimuli and are characterized with a significant impairment of their inhibitory functionality of eye movements. In previous work several methodologies of attention analytics were developed with laboratory based eye tracking technology but there is still a lack in providing opportunity for pervasive and continuous tracking of mental state for people still living at home. This work proposes a playful cognitive assessment method based on the antisaccade task. The performance scores of the serious game were analyzed in a field trial with 15 participants being diagnosed with light degree of Alzheimer's disease within a period of 10 weeks. The results present a statistically significant correlation between the game outcome scores and the Montreal Cognitive Assessment (MoCA) score, the golden standard for the analysis of executive functions in early Alzheimer's disease. This indicates first successful steps towards the daily use of serious games for pervasive assessment of Alzheimer's mental state."
  },
  {
    "title": "RIT-Eyes: realistically rendered eye images for eye-tracking applications",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Videos",
    "data": "June 2020",
    "authors": [
      "Nitinraj Nair",
      "Aayush Kumar Chaudhary",
      "Rakshit Sunil Kothari",
      "Gabriel Jacob Diaz",
      "Jeff B Pelz",
      "Reynold Bailey"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391990",
    "citation": "6",
    "abstract": "Convolutional neural network-based solutions for video oculography require large quantities of accurately labeled eye images acquired under a wide range of image quality, surrounding environmental reflections, feature occlusion, and varying gaze orientations. Manually annotating such a dataset is challenging, time-consuming, and error-prone. To alleviate these limitations, this work introduces an improved eye image rendering pipeline designed in Blender. RIT-Eyes provides access to realistic eye imagery with error-free annotations in 2D and 3D which can be used for developing gaze estimation algorithms. Furthermore, RIT-Eyes is capable of generating novel temporal sequences with realistic blinks and mimicking eye and head movements derived from publicly available datasets."
  },
  {
    "title": "RAEMAP: Real-Time Advanced Eye Movements Analysis Pipeline",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "June 2020",
    "authors": ["Gavindya Jayawardena"],
    "DOI": "https://doi.org/10.1145/3379157.3391992",
    "citation": "3",
    "abstract": "Eye-tracking measures enable means to understand the underlying covert processes engaged during inhibitory tasks which rely on attention allocation. We propose Real-Time Advanced Eye Movements Analysis Pipeline (RAEMAP) to utilize eye tracking measures as a valid psychophysiological measure. RAEMAP will include realtime analysis of the traditional positional gaze metrics as well as advanced metrics such as ambient/focal coefficient κ, gaze transition entropy, and index of pupillary activity (IPA). RAEMAP will also provide visualizations of calculated eye gaze metrics, heatmaps, and dynamic AOI generation in real-time. This paper will outline the proposed architecture of RAEMAP in terms of distributed computing, incorporation of machine learning models, and the evaluation to prove the utility of RAEMAP to diagnose ADHD in real-time."
  },
  {
    "title": "Scanning behavior in hemianopia: The Next Step: A study protocol",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "June 2020",
    "authors": [
      "Josephien L. Jansen",
      "Gera A. de Haan",
      "Joost H.C. Heutink",
      "Frans W. Cornelissen"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391993",
    "citation": "0",
    "abstract": "Homonymous visual field defects (HVFDs) are the largest group of visual disorders after acquired brain injury. Homonymous Hemianopia (HH), the most common form of HVFD, occurs in 8-31% of all stroke patients. HH can have a large influence on daily living, quality of life and patient's participation in society. People with HH mainly experience difficulties in reading, orientation and mobility. They benefit from training aimed to decrease the impact of the visual field deficit through optimizing visual scanning. Therefore, it is of utmost importance to inform patients about the way their scanning behavior relates to difficulties they experience in daily life and how they can improve their scanning behavior to overcome these difficulties. Knowledge about which scanning behavior in different situations is optimal, however, is mostly based on experiences and assumptions of professionals, and not supported by scientific literature and empirical data. The current project (September 2019 to September 2023) aims to examine the relationship between scanning behavior and performance on various daily life activities (i.e. mobility and search activities) in people with HH, people with simulated HH and a control group with normal vision. Innovative techniques such as eye-tracking and Virtual Reality (VR) will be used to examine scanning behavior in a standardized manner. Prototypes of these techniques, developed in a pilot project, were seen as useful additions to vision rehabilitation therapy by people with HH and rehabilitation therapists. Apart from providing insight into scanning behavior and its relation with different task demands, this project will help to develop innovative measures for scanning behavior that can be used in clinical practice. Data-collection will begin in the autumn of 2020 and will end approximately two years later. The current project is a PhD project, which means that it will result in a PhD thesis with at least four publications in international, scientific and peer-reviewed journals."
  },
  {
    "title": "Identifying Reading Patterns with Eye-tracking Visual Analytics",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "June 2020",
    "authors": ["Chia-Kai Yang"],
    "DOI": "https://doi.org/10.1145/3379157.3391994",
    "citation": "0",
    "abstract": "Eye-tracking studies can yield insight into patterns of reading strategies, but identifying patterns in eye-tracking visualizations is a cognitively demanding task. My dissertation explores how visual analytics approaches support analysts detecting sequential patterns in the eye-tracking data. To demonstrate the effectiveness of our visual analytics, I apply it to the datasets from a series of eye-tracking studies, and gather an empirical understanding about how research articles are read on paper and other media."
  },
  {
    "title": "Low Power Scanned Laser Eye Tracking for Retinal Projection AR Glasses",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "June 2020",
    "authors": [
      "Johannes Meyer",
      "Thomas Schlebusch",
      "Thomas Kuebler",
      "Enkelejda Kasneci"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391995",
    "citation": "0",
    "abstract": "Next generation AR glasses require a highly integrated, high-resolution near-eye display technique such as focus-free retinal projection to enhance usability. Combined with low-power eye-tracking, such glasses enable better user experience and performance. This research work focuses on low power eye tracking sensor technology for integration into retinal projection systems. In our approach, a MEMS micro mirror scans an IR laser beam over the eye region and the scattered light is received by a photodiode. The advantages of our approach over typical VOG systems are its high integration capability and low-power consumption, which qualify our approach for next generation AR glasses."
  },
  {
    "title": "Analysis and visualization tool for motion and gaze",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "June 2020",
    "authors": ["Anjali K Jogeshwar"],
    "DOI": "https://doi.org/10.1145/3379157.3391996",
    "citation": "2",
    "abstract": "Observers’ gaze is studied as a marker of attention, and by tracking the eyes, one can obtain gaze data. Attention of an individual performing natural tasks such as making a sandwich, playing squash, or teaching a class can be studied with the help of eye-tracking. Data analysis of real world interaction is challenging and time-consuming as it consists of varying or undefined environments, massive amounts of video data and unrestricted movement. To approach these challenges, my research aims to create an interactive four-dimensional (x,y,z,t) tool for the analysis and visualization of observer motion and gaze data, of one or more observers performing natural day-to-day tasks. Three solutions are necessary to achieve this goal: simulation of the environment with the ability to vary viewpoint, gaze visualization from two-dimensional scene to three-dimensions, and tracing of the observer(s) motion. The approaches to these challenges are described in the following sections."
  },
  {
    "title": "Visual processing of social stimuli in children with autism spectrum disorder",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "June 2020",
    "authors": ["Irena Komendova", "Katarína Kemková"],
    "DOI": "https://doi.org/10.1145/3379157.3391997",
    "citation": "0",
    "abstract": "Despite being a part of the official diagnostics of autism spectrum disorder (asd), whether people with this disease do avoid eye-contact and the view on human faces remains uncertain. An extensive body of eye-tracking research tries to answer this question, but results are inconsistent probably due to different approaches and other methodological specifics. This is the reason for my focus on the topic, where I try to determine factors contributing to this uncertainity and to come closer to the answer. Currently, I am working on a meta-analysis and preparing quasi-experiment, where I compare children with asd and children of the same age with neurotypical development, using eye-tracking technology and pictures with social context, specifically human faces."
  },
  {
    "title": "StARe: Gaze-Assisted Face-to-Face Communication in Augmented Reality",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN: Gaze Communication",
    "data": "June 2020",
    "authors": [
      "Radiah Rivu",
      "Yasmeen Abdrabou",
      "Ken Pfeuffer",
      "Augusto Esteves",
      "Stefanie Meitner",
      "Florian Alt"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3388930",
    "citation": "16",
    "abstract": "This research explores the use of eye-tracking during Augmented Reality (AR) - supported conversations. In this scenario, users can obtain information that supports the conversation, without augmentations distracting the actual conversation.We propose using gaze that allows users to gradually reveal information on demand. Information is indicated around user’s head, which becomes fully visible when other’s visual attention explicitly falls upon the area. We describe the design of such an AR UI and present an evaluation of the feasibility of the concept. Results show that despite gaze inaccuracies, users were positive about augmenting their conversations with contextual information and gaze interactivity. We provide insights into the trade-offs between focusing on the task at hand (i.e., the conversation), and consuming AR information. These findings are useful for future use cases of eye based AR interactions by contributing to a better understanding of the intricate balance between informative AR and information overload."
  },
  {
    "title": "Comparison of three dwell-time-based gaze text entry methods: Extended Abstract",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN: Gaze Communication",
    "data": "June 2020",
    "authors": ["Jacek Matulewski", "Mateusz Patera"],
    "DOI": "https://doi.org/10.1145/3379157.3388931",
    "citation": "2",
    "abstract": "Gaze text entry (GTE) with use of visual keyboards displayed on computer screen is an important topic for both the scientists dealing with the gaze interaction and potential users i.e. people with physical disabilities and their families. The most commonly used technique for GTE is based on dwell-time regions, at which the user needs to look longer to activate the associated action, in our case - entering the letter. In the article, we present the results of tests of three GTE systems (gaze keyboards) on a sample of 29 participants. We compare the objective measures of usability, namely the text entry rate and the number of errors, as well as subjective ones, obtained using SUS questionnaire. Additionally, two similar keyboards based on the ‘Qwerty’ buttons layout were compared in terms of time to the first fixation and its duration in the areas of interest (AOI) corresponding to the visual buttons. One of these gaze keyboards, the so called ’Molecular’ one, contains dynamic elements that have been designed and implemented in our laboratory, and which aim is to support the search for buttons by increasing the size of buttons with suggested letters, without significant change of their positions."
  },
  {
    "title": "Exploring Eye-Gaze Wheelchair Control",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN: Gaze Communication",
    "data": "June 2020",
    "authors": [
      "Jacopo M. Araujo",
      "Guangtao Zhang",
      "John Paulin Paulin Hansen",
      "Sadasivan Puthusserypady"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3388933",
    "citation": "17",
    "abstract": "Eye-gaze may potentially be used for steering wheelchairs or robots and thereby support independence in choosing where to move. This paper investigates the feasibility of gaze-controlled interfaces. We present an experiment with wheelchair control in a simulated, virtual reality (VR) driving experiment and a field study with five people using wheelchairs. In the VR experiment, three control interfaces were tested by 18 able-bodied subjects: (i) dwell buttons for direction commands on an overlay display, (ii) steering by continuous gaze point assessment on the ground plane in front of the driver, and (iii) waypoint navigation to targets placed on the ground plane. Results indicate that the waypoint method had superior performance, and it was also most preferred by the users, closely followed by the continuous-control interface. However, the field study revealed that our wheelchair users felt uncomfortable and excluded when they had to look down at the floor to steer a vehicle. Hence, our VR testing had a simplified representation of the steering task and ignored an important part of the use-context. In the discussion, we suggest potential improvements of simulation-based design of wheelchair gaze control interfaces."
  },
  {
    "title": "A Gaze-Based Web Browser with Multiple Methods for Link Selection",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN: Gaze Interaction",
    "data": "June 2020",
    "authors": ["Matteo Casarini", "Marco Porta", "Piercarlo Dondi"],
    "DOI": "https://doi.org/10.1145/3379157.3388929",
    "citation": "6",
    "abstract": "This paper presents a gaze-based web browser that allows hands-free navigation through five different link selection methods (namely, Menu, Discrete Cursor, Progressive Zoom, Quick Zoom, and Free Pointing) and two page scrolling techniques. For link selection, the purpose of this multi-approach solution is two-fold. On the one hand, we want users to be able to choose either their preferred methods or those that, in each specific case, are the most suitable (e.g., depending on the kind of link to activate). On the other hand, we wanted to assess the performance and appreciation level of the different approaches through formal tests, to identify their strengths and weaknesses. The browser, which is conceived as an assistive technology tool, also includes a built-in on-screen keyboard and the possibility to save and retrieve bookmarks."
  },
  {
    "title": "GIMIS: Gaze Input with Motor Imagery Selection",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN: Gaze Interaction",
    "data": "June 2020",
    "authors": [
      "Baosheng James Hou",
      "Per Bekgaard",
      "Scott MacKenzie",
      "John Paulin Paulin Hansen",
      "Sadasivan Puthusserypady"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3388932",
    "citation": "10",
    "abstract": "A hybrid gaze and brain-computer interface (BCI) was developed to accomplish target selection in a Fitts’ law experiment. The method, GIMIS, uses gaze input to steer the computer cursor for target pointing and motor imagery (MI) via the BCI to execute a click for target selection. An experiment (n = 15) compared three motor imagery selection methods: using the left-hand only, using the legs, and using either the left-hand or legs. The latter selection method (”either”) had the highest throughput (0.59 bps), the fastest selection time (2650 ms), and an error rate of 14.6%. Pupil size significantly increased with increased target width. We recommend the use of large targets, which significantly reduced error rate, and the ”either” option for BCI selection, which significantly increased throughput. BCI selection is slower compared to dwell time selection, but if gaze control is deteriorating, for example in a late stage of the ALS disease, GIMIS may be a way to gradually introduce BCI."
  },
  {
    "title": "Pupil Dilation Fulfills the Requirements for Dynamic Difficulty Adjustment in Gaming on the Example of Pong",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN: Gaze Interaction",
    "data": "June 2020",
    "authors": [
      "Christoph Strauch",
      "Michael Barthelmaes",
      "Elisa Altgassen",
      "Anke Huckauf"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3388934",
    "citation": "2",
    "abstract": "When games are too easy or too difficult, they are likely to be experienced as unpleasant. Therefore, identifying the ideal level of game difficulty is crucial for providing players with a positive experience during gaming. Performance data is typically used to determine how challenged a player is; however, this information is not always available. Pupil diameter has recently been suggested as a continuous option for tracking gaming appraisal. In this paper, we describe two experiments with in total 55 participants playing ’Pong’ under four levels of difficulty. Difficulty was manipulated via ball-speed (Experiment 1) and racket-size (Experiment 2) ranging from under- to overload. Pupil dilation and appraisal were maximal under medium difficulty (compared to easy and hard levels). These findings demonstrate the usefulness of pupil diameter as basis for psychophysiologically dynamic difficulty adjustment as it is sensitive both to under- and overload, hence underlining pupil dilation’s potential value for user-adaptive interfaces in general."
  },
  {
    "title": "A Shift-Based Data Augmentation Strategy for Improving Saccade Landing Point Prediction",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN: Gaze Interaction",
    "data": "June 2020",
    "authors": ["Henry Griffith", "Oleg Komogortsev"],
    "DOI": "https://doi.org/10.1145/3379157.3388935",
    "citation": "3",
    "abstract": "This paper demonstrates a technique for improving the performance of neural network-based models for saccade landing point estimation. Performance improvement is achieved by augmenting available training data with time-shifted replicates in order to improve prediction robustness to variations in saccade onset timing. The technique is validated for both long short-term memory (LSTM) and feed-forward neural network models for 5,893 saccades extracted from the recordings of 322 individuals during free-viewing of a movie trailer. The proposed augmentation strategy is demonstrated to improve the median accuracy of landing point estimates for LSTM models formulated using both the raw position and relative displacement of the gaze location."
  },
  {
    "title": "Landmark processing in topographic maps to support object location recall",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET4S: Maps and Wayfinding",
    "data": "June 2020",
    "authors": [
      "Julian Keil",
      "Dennis Edler",
      "Frank Dickmann",
      "Lars Kuchinke"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391416",
    "citation": "0",
    "abstract": "Object locations are memorized based on their relative position to other spatial elements. Landmarks, salient and static spatial elements, have been found to support the formation of spatial mental representations. However, it is still not totally understood which factors predict whether a landmark is a helpful reference point for object location memory. In this experiment, we assessed how the distance of landmarks to a to-be-learned object location affects fixations on the landmark and object location memory. Additionally, potential effects of visual map complexity on fixation patterns and object location memory were investigated. The findings indicate that distant landmarks are fixated less often and that location memory is better when the distance of the closest landmark to the to-be-learned object is smaller. In addition, location memory was more accurate in maps with high visual complexity. However, map complexity did not affect fixation patterns on landmarks. Thus, the availability of sufficient spatial reference points supports object location memory. In particular, the relevance of landmarks as a spatial reference point for object location memory seems to be inverse to its distance to the memorized location."
  },
  {
    "title": "Towards capturing focal/ambient attention during dynamic wayfinding",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET4S: Maps and Wayfinding",
    "data": "June 2020",
    "authors": ["Jakub Krukar", "Panagiotis Mavros", "Christoph Hoelscher"],
    "DOI": "https://doi.org/10.1145/3379157.3391417",
    "citation": "3",
    "abstract": "This work-in-progress paper reports on an ongoing experiment in which mobile eye-tracking is used to evaluate different wayfinding support systems. Specifically, it tackles the problem of detecting and isolating attentional demands of building layouts and signage systems in wayfinding tasks. The coefficient K has been previously established as a measure of focal/ambient attention for eye-tracking data. Here, we propose a novel method to compute coefficient K using eye-tracking from virtual reality experiments. We detail challenges associated with transforming a two-dimensional coefficient K concept to three-dimensional data, and the debatable theoretical equivalence of the concept after such a transformation. We present a preliminary implementation to experimental data and explore the possibilities of the method for novel insight in architectural analyses."
  },
  {
    "title": "Eye-tracking and Virtual Reality in 360-degrees: exploring two ways to assess attentional orienting in rear space",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET4S: Spatial Processing",
    "data": "June 2020",
    "authors": [
      "Rebai Soret",
      "Pom Charras",
      "Ines Khazar",
      "Christophe Hurter",
      "Vsevolod Peysakhovich"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391418",
    "citation": "2",
    "abstract": "The Posner cueing task is a classic experimental paradigm in cognitive science for measuring visual attention orienting abilities. Recently, it was suggested that this paradigm can be adapted in virtual reality (e.g. in an immersive and ecological environment) to evaluate the effectiveness of perceptual stimuli in directing attention and by extension to study the underlying cognitive processes. In this study, auditory and visual endogenous cue were used to voluntary orient attention at 360°. Two groups of participants (N=33 and N=28) equipped with a virtual reality headset including integrated eye-tracking performed a modified version of the Posner cueing task in a 360° immersive environment. In this task, participants had to destroy space objects, as quickly as possible, through eye interaction. Predictive visual or auditory informed participants about target location. The results show that these endogenous cues significantly improve performance even if the object to be destroyed occurred outside the visual field or through a mirror. This experiment provides one of the first demonstrations that attentional orienting mechanism can improve performances of visual information processing in an immersive and ecological 360° environment where information can appear in rear space."
  },
  {
    "title": "Cognitive Processing Stages During Mental Folding Are Reflected in Eye Movements",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET4S: Spatial Processing",
    "data": "June 2020",
    "authors": [
      "Kai Preuss",
      "Christopher Hilton",
      "Klaus Gramann",
      "Nele Russwinkel"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391415",
    "citation": "0",
    "abstract": "Distinct cognitive processing stages in mental spatial transformation tasks can be identified in oculomotor behavior. We recorded eye movements whilst participants performed a mental folding task. Gaze behaviour was analyzed to provide insights into the relationship of task difficulty, gaze proportion on each stimulus, gaze switches between stimuli, and reaction times. We found a monotonic decrease in switch frequency and reference object gaze proportions with increasing difficulty level. Further, we found that these measures of gaze behaviour are related to the time taken to perform the mental transformation. We propose that the observed patterns of eye movements are indicative of distinct cognitive stages during mental folding. Lastly, further exploratory analyses are discussed."
  },
  {
    "title": "Detecting ambient/focal visual attention in professional airline pilots with a modified Coefficient K: a full flight simulator study",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET4S: Operators",
    "data": "June 2020",
    "authors": [
      "Christophe Antony Lounis",
      "Almoctar Hassoumi",
      "Olivier Lefrancois",
      "Vsevolod Peysakhovich",
      "Mickael Causse"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391412",
    "citation": "2",
    "abstract": "Flight instruments, from which a pilot monitors an aircraft, usually serve as areas-of-interest (AOI) that help to investigate the dynamics of the visual behavior of pilots. Consequently, several meta-metrics have been proposed to provide more information than common variables such as the number of fixations and saccades, the fixation durations, the saccade amplitude, and the standard dwell time. Researchers are however still searching for the best metrics for better insights into eye movements during scene exploration or inspection. In this work, we propose extending the formerly well established κ-coefficient metric defined by Krejtz et al. [2016] that allows discerning ambient and focal attention. Using AOI and transitions between them, we have derived a new measure that enables assessment of the distribution of visual attention (via eye-tracking data). Professional pilots’ eye movements were recorded while they were performing a flight scenario with full automation, including phases of flight (take-off, cruise, landing). Our analysis suggests that the take-off, cruise, and landing phases call for checking of specific areas, evidenced by the number of fixations and their durations. Furthermore, we compare our metric to the standard κ-coefficient and validate our approach using data collected during an experiment with 11 certified aircraft pilots. Here, we were able to show that the derived metric can be an interesting alternative for visual behavior investigation. The modified κ-coefficient can be used as a metric to investigate visual attention distribution, with application in cockpit monitoring assessment during training sessions or potentially during real flights."
  },
  {
    "title": "Gaze - grabber distance in expert and novice forest machine operators: the effects of automatic boom control",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET4S: Operators",
    "data": "June 2020",
    "authors": ["Jani Koskinen", "Roman Bednarik"],
    "DOI": "https://doi.org/10.1145/3379157.3391414",
    "citation": "2",
    "abstract": "Eye-hand coordination is a central skill in both everyday and expert visuo-motor tasks. In forest machine cockpits during harvest, the operators need to perform the eye-hand coordination and spatial navigation effectively to control the boom of the forwarder smoothly and quickly to achieve high performance. Because it is largely unknown how this skill is acquired, we conducted a first eye-tracking study of its kind to uncover the strategies expert and novice operators use. In an authentic training situation, both groups used an industry standard machine with- and without intelligent boom control support, and we measured their gaze and boom control strategies."
  },
  {
    "title": "Eye-tracking Data Analyser (EDA): Web Application and Evaluation",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETWEB - Eye Tracking for the Web",
    "data": "June 2020",
    "authors": [
      "Abdulrahman Zakrt",
      "Abdulmalik Obaidah Elmahgiubi",
      "Beshir Alhomsi",
      "Sukru Eraslan",
      "Yeliz Yesilada"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391301",
    "citation": "0",
    "abstract": "Eye tracking is a growing field of research that is widely used to understand user behaviours and cognitive strategies. Eye-trackers typically collect an enormous amount of data that needs to be analysed, processed and interpreted to draw some conclusions. Collected data is typically examined with either commonly used statistical tools such as SPSS which are not designed specifically for eye-tracking data analysis, or bespoke tools provided by eye-tracker vendors. However, these tools may require extensive experience or they may be extremely expensive. To address these limitations, we propose an open-source web application called EDA (Eye-Tracking Data Analyser) which can be used to analyse eye-tracking data, in particular, to conduct comparative statistical tests between two groups. In this paper, we first present the overall architecture and implementation of this application and then present its evaluation conducted with ten people who are experts in eye-tracking research. The evaluation shows that the EDA application is easy to use, and its workload measure is low in terms of NASA NLX."
  },
  {
    "title": "Towards Real-time Webpage Relevance Prediction UsingConvex Hull Based Eye-tracking Features",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETWEB - Eye Tracking for the Web",
    "data": "June 2020",
    "authors": ["Nilavra Bhattacharya", "Somnath Rakshit", "Jacek Gwizdka"],
    "DOI": "https://doi.org/10.1145/3379157.3391302",
    "citation": "2",
    "abstract": "Browsing the web for finding answers to questions has become pervasive in our everyday lives. When users search the web to satisfy their information-needs, their on-screen eye movements can serve as a source of implicit relevance feedback. We analyze data collected from two eye-tracking studies, wherein participants read online news-articles, and judged whether they contained answers to factual questions. We propose two eye-tracking features, derived from the area of the convex hull of their eye fixations. We demonstrate that these features can well distinguish between eye-movements on news-articles perceived to be relevant vs. irrelevant, for containing the answer to a question. These features can potentially be used for predicting the user’s perceived-relevance in real-time. F1 scores as high as 0.80 are obtained using these proposed features only, and the performance is comparable to the combined predictive power of fifteen eye-tracking features established by prior literature."
  },
  {
    "title": "Attention-based Cross-Modal Unification of Visualized Text and Image Features: Understanding the influence of interface and user idiosyncrasies on unification for free-viewing",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETWEB - Eye Tracking for the Web",
    "data": "June 2020",
    "authors": [
      "Sandeep Vidyapu",
      "Vijaya Saradhi Vedula",
      "Michael Burch",
      "Samit Bhattacharya"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391303",
    "citation": "0",
    "abstract": "The attentional analysis on graphical user interfaces (GUIs) is shifting from Areas-of-Interest (AOIs) to Data-of-Interest (DOI). However, the heterogeneity data modalities on GUIs hinder the DOI-based analyses. To overcome this limitation, we present a Canonical Correlation Analysis (CCA) based approach to unify the heterogeneous modalities (text and images) concerning user attention. Especially, the influence of interface and user idiosyncrasies in establishing the cross-modal correlation is studied. The performance of the proposed approach is analyzed for free-viewing eye-tracking experiments conducted on bi-modal webpages. The results reveal: (i) Cross-modal text and image visual features are correlated when the interface idiosyncrasies, alone or along with user idiosyncrasies, are constrained. (ii) The font-families of text are comparable to color histogram visual features of images in drawing the users’ attention. (iii) Text and image visual features can delineate the attention of each other. Our approach finds applications in user-oriented webpage rendering and computational attention modeling."
  },
  {
    "title": "Mobile Consumer Behavior in Fashion m-Retail: An Eye Tracking Study to Understand Gender Differences",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETWEB - Eye Tracking for the Web",
    "data": "June 2020",
    "authors": ["Zofija Tupikovskaja-Omovie", "David J. Tyler"],
    "DOI": "https://doi.org/10.1145/3379157.3391305",
    "citation": "7",
    "abstract": "With exponential adoption of mobile devices, consumers increasingly use them for shopping. There is a need to understand the gender differences in mobile consumer behavior. This study used mobile eye tracking technology and mixed-method approach to analyze and compare how male and female mobile fashion consumers browse and shop on smartphones. Mobile eye tracking glasses recorded fashion consumers’ shopping experiences using smartphones for browsing and shopping on the actual fashion retailer’s website. 14 participants successfully completed this study, half of them were males and half females. Two different data analysis approaches were employed, namely a novel framework of the shopping journey, and semantic gaze mapping with 31 Areas of Interest (AOI) representing the elements of the shopping journey. The results showed that male and female users exhibited significantly different behavior patterns, which have implications for mobile website design and fashion m-retail. The shopping journey map framework proves useful for further application in market research."
  },
  {
    "title": "EyeCrowdata: Towards a Web-based Crowdsourcing Platform for Web-related Eye-Tracking Data",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETWEB - Eye Tracking for the Web",
    "data": "June 2020",
    "authors": [
      "Naziha Shekh.Khalil",
      "Ecem Dogruer",
      "Abdulmohimen K. O. Elosta",
      "Sukru Eraslan",
      "Yeliz Yesilada",
      "Simon Harper"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391304",
    "citation": "1",
    "abstract": "Eye-tracking studies are commonly used for identifying the usability problems of Web pages and gaining insights into how the design of Web pages can be improved for better user experience. Similar to other user studies, eye-tracking studies should be carefully designed and conducted by considering ethical issues and confounding factors, and therefore these studies typically require a considerable amount of time. Recruiting a large number of participants is also an important issue as eye-tracking sessions may not be conducted in parallel in case of limited resources such as equipment and researchers. Previous work highlighted the need for a Web-based platform to crowdsource Web-related eye-tracking data and facilitate data sharing, thus allowing the replication of existing analysis. Previous work also presented a preliminary structured literature review on what kinds of metrics are required for such a platform. In this paper, we also focus on Web-related eye-tracking studies, and we present an overview of the extended version of the structured literature review along with a prototype for a Web-based platform for crowdsourcing Web-related eye-tracking data called EyeCrowdata."
  },
  {
    "title": "Sequence Models in Eye Tracking: Predicting Pupil Diameter During Learning",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET-MM: SESSION 2",
    "data": "June 2020",
    "authors": [
      "Sharath C Koorathota",
      "Kaveri Thakoor",
      "Patrick Adelman",
      "Yaoli Mao",
      "Xueqing Liu",
      "Paul Sajda"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391653",
    "citation": "3",
    "abstract": "A deep learning framework for predicting pupil diameter using eye tracking data is described. Using a variety of input, such as fixation positions, durations, saccades and blink-related information, we assessed the performance of a sequence model in predicting future pupil diameter in a student population as they watched educational videos in a controlled setting. Through assessing student performance on a post-viewing test, we report that deep learning sequence models may be useful for separating components of pupil responses that are linked to luminance and accommodation from those that are linked to cognition and arousal."
  },
  {
    "title": "Gaze Estimation in the Dark with Generative Adversarial Networks",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET-MM: SESSION 2",
    "data": "June 2020",
    "authors": ["Jung-Hwa Kim", "Jin-Woo Jeong"],
    "DOI": "https://doi.org/10.1145/3379157.3391654",
    "citation": "1",
    "abstract": "In this paper, we propose to utilize generative adversarial networks (GANs) to achieve successful gaze estimation in interactive multimedia environments with low light conditions such as a digital museum or exhibition hall. The proposed approach utilizes a GAN to enhance user images captured under low-light conditions, thereby recovering missing information for gaze estimation. The recovered images are fed into the CNN architecture to estimate the direction of user gaze. The preliminary experimental results on the modified MPIIGaze dataset demonstrated an average performance improvement of 6.6 under various low light conditions, which is a promising step for further research."
  },
  {
    "title": "Analyzing Transferability of Happiness Detection via Gaze Tracking in Multimedia Applications",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET-MM: SESSION 2",
    "data": "June 2020",
    "authors": ["David Bethge", "Lewis Chuang", "Tobias Grosse-Puppendahl"],
    "DOI": "https://doi.org/10.1145/3379157.3391655",
    "citation": "1",
    "abstract": "How are strong positive affective states related to eye-tracking features and how can they be used to appropriately enhance well-being in multimedia consumption? In this paper, we propose a robust classification algorithm for predicting strong happy emotions from a large set of features acquired from wearable eye-tracking glasses. We evaluate the potential transferability across subjects and provide a model-agnostic interpretable feature importance metric. Our proposed algorithm achieves a true-positive-rate of 70% while keeping a low false-positive-rate of 10% with extracted features of the pupil diameter as most important features."
  },
  {
    "title": "Gaze Data for Quality Assessment of Foveated Video",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET-MM: SESSION 3",
    "data": "June 2020",
    "authors": ["Oliver Wiedemann", "Dietmar Saupe"],
    "DOI": "https://doi.org/10.1145/3379157.3391656",
    "citation": "0",
    "abstract": "This paper presents current methodologies and challenges in the context of subjective quality assessment with a focus on adaptively encoded video streams."
  },
  {
    "title": "Toward A Pervasive Gaze-Contingent Assistance System: Attention and Context-Awareness in Augmented Reality",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET-MM: SESSION 3",
    "data": "June 2020",
    "authors": ["Kenan Bektas"],
    "DOI": "https://doi.org/10.1145/3379157.3391657",
    "citation": "4",
    "abstract": "Mobile devices with high-speed connectivity provide us with access to gigabytes of high resolution images, videos, and graphics. For instance, a head-worn display can be used to augment the real view with digitized visual information (Figure 1). Eye tracking helps us to understand how we process visual information and it allows us to develop gaze-enabled interactive systems. For instance, foveated gaze-contingent displays (GCDs) dynamically adjust the level of detail according to the user’s point-of-interest. We propose that GCDs should take users’ attention and cognitive load into account, augment their vision with contextual information and provide personalized assistance in solving visual tasks. Grounded on existing literature, we identified several research questions that need to be discussed before developing such displays."
  },
  {
    "title": "Implications of Eye Tracking Research to Cinematic Virtual Reality",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ET-MM: SESSION 3",
    "data": "June 2020",
    "authors": ["Sylvia Rothe", "Lewis L. Chuang"],
    "DOI": "https://doi.org/10.1145/3379157.3391658",
    "citation": "1",
    "abstract": "While watching omnidirectional movies via head-mounted displays the viewer has an immersive viewing experience. Turning the head and looking around is a natural input technique to choose the visible part of the movie. For realizing scene changes depending on the viewing direction and for implementing non-linear story structures in cinematic virtual reality (CVR), selection methods are required to select the story branches. The input device should not disturb the viewing experience and the viewer should not be primarily aware of it. Eye- and head-based methods do not need additional devices and seem to be especially suitable. We investigate several techniques by using an own tool for analysing head and eye tracking data in CVR."
  },
  {
    "title": "Identifying users based on their eye tracker calibration data",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EyeSec 2020: Eye-Gaze for Security Applications",
    "data": "June 2020",
    "authors": ["Pawel Kasprowski"],
    "DOI": "https://doi.org/10.1145/3379157.3391419",
    "citation": "3",
    "abstract": "The purpose of the paper is to test the possibility of identifying people based on the input they provide to an eye tracker during the calibration process."
  },
  {
    "title": "Texture Feature Extraction From Free-Viewing Scan Paths Using Gabor Filters With Downsampling",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EyeSec 2020: Eye-Gaze for Security Applications",
    "data": "June 2020",
    "authors": ["Henry K. Griffith", "Oleg V. Komogortsev"],
    "DOI": "https://doi.org/10.1145/3379157.3391423",
    "citation": "5",
    "abstract": "Texture-based features computed on eye movement scan paths have recently been proposed for eye movement biometric applications. Feature vectors were extracted within this prior work by computing the mean and standard deviation of the resulting images obtained through application of a Gabor filter bank. This paper describes preliminary work exploring an alternative technique for extracting features from Gabor filtered scan path images. Namely, features vectors are obtained by downsampling the filtered images, thereby retaining structured spatial information within the feature vector. The proposed technique is validated at various downsampling scales for data collected from 94 subjects during free-viewing of a fantasy movie trailer. The approach is demonstrated to reduce EER versus the previously proposed statistical summary technique by 11.7% for the best evaluated downsampling parameter."
  },
  {
    "title": "Eye Movement Biometrics Using a New Dataset Collected in Virtual Reality",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EyeSec 2020: Eye-Gaze for Security Applications",
    "data": "June 2020",
    "authors": ["Dillon J Lohr", "Samantha Aziz", "Oleg Komogortsev"],
    "DOI": "https://doi.org/10.1145/3379157.3391420",
    "citation": "21",
    "abstract": "This paper introduces a novel eye movement dataset collected in virtual reality (VR) that contains both 2D and 3D eye movement data from over 400 subjects. We establish that this dataset is suitable for biometric studies by evaluating it with both statistical and machine learning–based approaches. For comparison, we also include results from an existing, similarly constructed dataset."
  },
  {
    "title": "Gaze-based Authentication in Virtual Reality",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EyeSec 2020: Eye-Gaze for Security Applications",
    "data": "June 2020",
    "authors": ["Jonathan Liebers", "Stefan Schneegass"],
    "DOI": "https://doi.org/10.1145/3379157.3391421",
    "citation": "6",
    "abstract": "Authentication in virtual reality (VR) is a challenging topic since the common input modalities in VR (e. g., hand-held controllers) are limited and easily observable from the perspective of a bystander. Yet, as applications in VR are increasingly allowing access to private information and commercial applications appear (e. g., virtual shopping, social media), the secure identification and verification of a person is a major concern. This challenge is aggravated, as the wearer of a head-mounted display (HMD) does not perceive the surrounding real environment through the HMD. As more and more HMDs are released to the market with built-in eye-tracking functionality, we seek to understand how we can seamlessly utilize gaze-based authentication and connected methods in VR applications."
  },
  {
    "title": "Are They Actually Looking? Identifying Smartphones Shoulder Surfing Through Gaze Estimation",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EyeSec 2020: Eye-Gaze for Security Applications",
    "data": "June 2020",
    "authors": [
      "Alia Saad",
      "Dina Hisham Elkafrawy",
      "Slim Abdennadher",
      "Stefan Schneegass"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3391422",
    "citation": "4",
    "abstract": "Mobile devices have evolved to be a crucial part of our everyday lives. However, they are subject to different types of user-centered attacks such as shoulder surfing attacks. Previous work focused on notifying the user with a potential shoulder surfer if an extra face is detected. Although it is a successful approach, it eliminated the possibility that the alleged attacker is just standing and not looking at the user’s device. In this work, we investigate estimating the gaze of potential attackers in order to verify if they are indeed looking at the user’s phone."
  },
  {
    "title": "Let It Snow: Adding pixel noise to protect the user’s identity",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: PrEThics - Privacy and Ethics in Eye Tracking",
    "data": "June 2020",
    "authors": [
      "Brendan John",
      "Ao Liu",
      "Lirong Xia",
      "Sanjeev Koppal",
      "Eakta Jain"
    ],
    "DOI": "https://doi.org/10.1145/3379157.3390512",
    "citation": "7",
    "abstract": "Optical eye trackers record images of the eye to estimate the gaze direction. These images contain the iris of the user. While useful for authentication, these images can be used for a spoofing attack if stolen. We propose to use pixel noise to break the iris signature while retaining gaze estimation. In this paper, we present an algorithm to add “snow” to the eye image and evaluate the privacy-utility tradeoff for the choice of noise parameter."
  },
  {
    "title": "Adversarial Attacks on Classifiers for Eye-based User Modelling",
    "conferenceTitle": "ETRA '20 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: PrEThics - Privacy and Ethics in Eye Tracking",
    "data": "June 2020",
    "authors": ["Inken Hagestedt", "Michael Backes", "Andreas Bulling"],
    "DOI": "https://doi.org/10.1145/3379157.3390511",
    "citation": "3",
    "abstract": "An ever-growing body of work has demonstrated the rich information content available in eye movements for user modelling, e.g. for predicting users’ activities, cognitive processes, or even personality traits. We show that state-of-the-art classifiers for eye-based user modelling are highly vulnerable to adversarial examples: small artificial perturbations in gaze input that can dramatically change a classifier’s predictions. On the sample task of eye-based document type recognition we study the success of adversarial attacks with and without targeting the attack to a specific class."
  },
  {
    "title": "Visual Search Target Inference in Natural Interaction Settings with Machine Learning",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Michael Barz", "Sven Stauden", "Daniel Sonntag"],
    "DOI": "https://doi.org/10.1145/3379155.3391314",
    "citation": "10",
    "abstract": "Visual search is a perceptual task in which humans aim at identifying a search target object such as a traffic sign among other objects. Search target inference subsumes computational methods for predicting this target by tracking and analyzing overt behavioral cues of that person, e.g., the human gaze and fixated visual stimuli. We present a generic approach to inferring search targets in natural scenes by predicting the class of the surrounding image segment. Our method encodes visual search sequences as histograms of fixated segment classes determined by SegNet, a deep learning image segmentation model for natural scenes. We compare our sequence encoding and model training (SVM) to a recent baseline from the literature for predicting the target segment. Also, we use a new search target inference dataset. The results show that, first, our new segmentation-based sequence encoding outperforms the method from the literature, and second, that it enables target inference in natural settings."
  },
  {
    "title": "Combining Gaze Estimation and Optical Flow for Pursuits Interaction",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Mihai Bace",
      "Vincent Becker",
      "Chenyang Wang",
      "Andreas Bulling"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391315",
    "citation": "6",
    "abstract": "Pursuit eye movements have become widely popular because they enable spontaneous eye-based interaction. However, existing methods to detect smooth pursuits require special-purpose eye trackers. We propose the first method to detect pursuits using a single off-the-shelf RGB camera in unconstrained remote settings. The key novelty of our method is that it combines appearance-based gaze estimation with optical flow in the eye region to jointly analyse eye movement dynamics in a single pipeline. We evaluate the performance and robustness of our method for different numbers of targets and trajectories in a 13-participant user study. We show that our method not only outperforms the current state of the art but also achieves competitive performance to a consumer eye tracker for a small number of targets. As such, our work points towards a new family of methods for pursuit interaction directly applicable to an ever-increasing number of devices readily equipped with cameras."
  },
  {
    "title": "Analyzing Gaze Behavior Using Object Detection and Unsupervised Clustering",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Pranav Venuprasad",
      "Li Xu",
      "Enoch Huang",
      "Andrew Gilman",
      "Leanne Chukoskie Ph.D.",
      "Pamela Cosman"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391316",
    "citation": "6",
    "abstract": "Gaze behavior is important in early development, and atypical gaze behavior is among the first symptoms of autism. Here we describe a system that quantitatively assesses gaze behavior using eye-tracking glasses. Objects in the subject’s field of view are detected using a deep learning model on the video captured by the glasses’ world-view camera, and a stationary frame of reference is estimated using the positions of the detected objects. The gaze positions relative to the new frame of reference are subjected to unsupervised clustering to obtain the time sequence of looks. The clustering method increases the accuracy of look detection on test videos compared against a previous algorithm, and is considerably more robust on videos with poor calibration."
  },
  {
    "title": "A MinHash approach for fast scanpath classification",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "David Geisler",
      "Nora Castner",
      "Gjergji Kasneci",
      "Enkelejda Kasneci"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391325",
    "citation": "3",
    "abstract": "The visual scanpath describes the shift of visual attention over time. Characteristic patterns in the attention shifts allow inferences about cognitive processes, performed tasks, intention, or expertise. To analyse such patterns, the scanpath is often represented as a sequence of symbols that can be used to calculate a similarity score to other scanpaths. However, as the length of the scanpath or the number of possible symbols increases, established methods for scanpath similarity become inefficient, both in terms of runtime and memory consumption. We present a MinHash approach for efficient scanpath similarity calculation. Our approach shows competitive results in clustering and classification of scanpaths compared to established methods such as Needleman-Wunsch, but at a fraction of the required runtime. Furthermore, with time complexity of and constant memory consumption, our approach is ideally suited for real-time operation or analyzing large amounts of data."
  },
  {
    "title": "Label Likelihood Maximisation: Adapting iris segmentation models using domain adaptation",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Anton Mølbjerg Eskildsen", "Dan Witzner Hansen"],
    "DOI": "https://doi.org/10.1145/3379155.3391327",
    "citation": "0",
    "abstract": "We propose to use unlabelled eye image data for domain adaptation of an iris segmentation network. Adaptation allows the model to be less reliant on its initial generality. This is beneficial due to the large variance exhibited by eye image data which makes training of robust models difficult. The method uses a label prior in conjunction with network predictions to produce pseudo-labels. These are used in place of ground-truth data to adapt a base model. A fully connected neural network performs the pixel-wise iris segmentation. The base model is trained on synthetic data and adapted to several existing datasets with real-world eye images. The adapted models improve the average pupil centre detection rates by 24% at a distance of 25 pixels. We argue that the proposed method, and domain adaptation in general, is an interesting direction for increasing robustness of eye feature detectors."
  },
  {
    "title": "Validation of a prototype hybrid eye-tracker against the DPI and the Tobii Spectrum",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Kenneth Holmqvist",
      "Saga Lee Orbom",
      "Michael Miller",
      "Albert Kashchenevsky",
      "Mark M. Shovman",
      "Mark W. Greenlee"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391330",
    "citation": "4",
    "abstract": "We benchmark a new hybrid eye-tracker system against the DPI (Dual Purkinje Imaging) tracker and the Tobii Spectrum in a series of three experiments. In a first within-subjects battery of tests, we show that the precision of the new eye-tracker is much better than that of both the DPI and the Spectrum, but that accuracy is not better. We also show that the new eye-tracker is insensitive to effects of pupil contraction on gaze direction (in contrast to both the DPI and the Spectrum), that it detects microsaccades on par with the DPI and better than the Spectrum, and that it can possibly record tremor. In the second experiment, sensors of the novel eye-tracker were integrated into the optical path of the DPI bench. Simultaneous recordings show that saccade dynamics, post-saccadic oscillations and measurements of translational movements are comparable to those of the DPI. In the third experiment, we show that the DPI and the new eye-tracker are capable of detecting 2 arcmin artificial-eye rotations while the Spectrum cannot. Results suggest that the new eye-tracker, in contrast to video-based P-CR systems [Holmqvist and Blignaut 2020], is suitable for studies that record small eye-movements under varying ambient light levels."
  },
  {
    "title": "Anticipating Averted Gaze in Dyadic Interactions",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Philipp Müller", "Ekta Sood", "Andreas Bulling"],
    "DOI": "https://doi.org/10.1145/3379155.3391332",
    "citation": "8",
    "abstract": "We present the first method to anticipate averted gaze in natural dyadic interactions. The task of anticipating averted gaze, i.e. that a person will not make eye contact in the near future, remains unsolved despite its importance for human social encounters as well as a number of applications, including human-robot interaction or conversational agents. Our multimodal method is based on a long short-term memory (LSTM) network that analyses non-verbal facial cues and speaking behaviour. We empirically evaluate our method for different future time horizons on a novel dataset of 121 YouTube videos of dyadic video conferences (74 hours in total). We investigate person-specific and person-independent performance and demonstrate that our method clearly outperforms baselines in both settings. As such, our work sheds light on the tight interplay between eye contact and other non-verbal signals and underlines the potential of computational modelling and anticipation of averted gaze for interactive applications."
  },
  {
    "title": "BimodalGaze: Seamlessly Refined Pointing with Gaze and Filtered Gestural Head Movement",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": [
      "Ludwig Sidenmark",
      "Diako Mardanbegi",
      "Argenis Ramirez Gomez",
      "Christopher Clarke",
      "Hans Gellersen"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391312",
    "citation": "23",
    "abstract": "Eye gaze is a fast and ergonomic modality for pointing but limited in precision and accuracy. In this work, we introduce BimodalGaze, a novel technique for seamless head-based refinement of a gaze cursor. The technique leverages eye-head coordination insights to separate natural from gestural head movement. This allows users to quickly shift their gaze to targets over larger fields of view with naturally combined eye-head movement, and to refine the cursor position with gestural head movement. In contrast to an existing baseline, head refinement is invoked automatically, and only if a target is not already acquired by the initial gaze shift. Study results show that users reliably achieve fine-grained target selection, but we observed a higher rate of initial selection errors affecting overall performance. An in-depth analysis of user performance provides insight into the classification of natural versus gestural head movement, for improvement of BimodalGaze and other potential applications."
  },
  {
    "title": "A Survey of Digital Eye Strain in Gaze-Based Interactive Systems",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": [
      "Teresa Hirzle",
      "Maurice Cordts",
      "Enrico Rukzio",
      "Andreas Bulling"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391313",
    "citation": "11",
    "abstract": "Display-based interfaces pose high demands on users’ eyes that can cause severe vision and eye problems, also known as digital eye strain (DES). Although these problems can become even more severe if the eyes are actively used for interaction, prior work on gaze-based interfaces has largely neglected these risks. We offer the first comprehensive account of DES in gaze-based interactive systems that is specifically geared to gaze interaction designers. Through an extensive survey of more than 400 papers published over the last 46 years, we first discuss the current role of DES in interactive systems. One key finding is that DES is only rarely considered when evaluating novel gaze interfaces and neglected in discussions of usability. We identify the main causes and solutions to DES and derive recommendations for interaction designers on how to guide future research on evaluating and alleviating DES."
  },
  {
    "title": "Detecting Relevance during Decision-Making from Eye Movements for UI Adaptation",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": [
      "Anna Maria Feit",
      "Lukas Vordemann",
      "Seonwook Park",
      "Caterina Berube",
      "Otmar Hilliges"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391321",
    "citation": "9",
    "abstract": "This paper proposes an approach to detect information relevance during decision-making from eye movements in order to enable user interface adaptation. This is a challenging task because gaze behavior varies greatly across individual users and tasks and ground-truth data is difficult to obtain. Thus, prior work has mostly focused on simpler target-search tasks or on establishing general interest, where gaze behavior is less complex. From the literature, we identify six metrics that capture different aspects of the gaze behavior during decision-making and combine them in a voting scheme. We empirically show, that this accounts for the large variations in gaze behavior and out-performs standalone metrics. Importantly, it offers an intuitive way to control the amount of detected information, which is crucial for different UI adaptation schemes to succeed. We show the applicability of our approach by developing a room-search application that changes the visual saliency of content detected as relevant. In an empirical study, we show that it detects up to 97% of relevant elements with respect to user self-reporting, which allows us to meaningfully adapt the interface, as confirmed by participants. Our approach is fast, does not need any explicit user input and can be applied independent of task and user."
  },
  {
    "title": "Bubble Gaze Cursor + Bubble Gaze Lens: Applying Area Cursor Technique to Eye-Gaze Interface",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": ["Myungguen Choi", "Daisuke Sakamoto", "Tetsuo Ono"],
    "DOI": "https://doi.org/10.1145/3379155.3391322",
    "citation": "8",
    "abstract": "We conducted two studies exploring how an area cursor technique can improve the eye-gaze interface. We first examined the bubble cursor technique. We developed an eye-gaze-based cursor called the bubble gaze cursor and compared it to a standard eye-gaze interface and a bubble cursor with a mouse. The results revealed that the bubble gaze cursor interface was faster than the standard point cursor-based eye-gaze interface. In addition, the usability and mental workload were significantly better than those of the standard interface. Next, we extended the bubble gaze cursor technique and developed a bubble gaze lens. The results indicated that the bubble gaze lens technique was faster than the bubble gaze cursor method and the error rate was reduced by 54.0%. The usability and mental workload were also considerably better than those of the bubble gaze cursor."
  },
  {
    "title": "Eye Gaze Controlled Robotic Arm for Persons with Severe Speech and Motor Impairment",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": [
      "Vinay Krishna Sharma",
      "Kamalpreet Saluja",
      "Vimal Mollyn",
      "Pradipta Biswas"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391324",
    "citation": "7",
    "abstract": "Recent advancements in the field of robotics offers new promises for people with different range of abilities although making a human robot interface for people with severe disabilities is challenging. This paper describes the design and development of an eye gaze controlled interface for users with severe speech and motor impairment to manipulate a robotic arm. Two user studies were reported on pick and drop and reachability studies involving users with severe speech and motor impairment. Using the eye gaze controlled interface users could undertake representative pick and drop task at an average duration less than 15 secs and reach a randomly designated target within 60 secs."
  },
  {
    "title": "Dataset for Eye Tracking on a Virtual Reality Platform",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 3: Virtual Reality",
    "data": "June 2020",
    "authors": [
      "Stephan Joachim Garbin",
      "Oleg Komogortsev",
      "Robert Cavin",
      "Gregory Hughes",
      "Yiru Shen",
      "Immo Schuetz",
      "Sachin S Talathi"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391317",
    "citation": "8",
    "abstract": "We present a large scale data set of eye-images captured using a virtual-reality (VR) head mounted display mounted with two synchronized eye-facing cameras at a frame rate of 200 Hz under controlled illumination. This dataset is compiled from video capture of the eye-region collected from 152 individual participants and is divided into four subsets: (i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil and sclera (ii) 252,690 unlabeled eye-images, (iii) 91,200 frames from randomly selected video sequences of 1.5 seconds in duration, and (iv) 143 pairs of left and right point cloud data compiled from corneal topography of eye regions collected from a subset, 143 out of 152, participants in the study. A baseline experiment has been evaluated on the dataset for the task of semantic segmentation of pupil, iris, sclera and background, with the mean intersection-over-union (mIoU) of 98.3 %. We anticipate that this dataset will create opportunities to researchers in the eye tracking community and the broader machine learning and computer vision community to advance the state of eye-tracking for VR applications, which in its turn will have greater implications in Human-Computer Interaction."
  },
  {
    "title": "Effect of a Constant Camera Rotation on the Visibility of Transsaccadic Camera Shifts",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 3: Virtual Reality",
    "data": "June 2020",
    "authors": ["Maryam Keyvanara", "Robert Allison"],
    "DOI": "https://doi.org/10.1145/3379155.3391318",
    "citation": "1",
    "abstract": "Often in 3D games and virtual reality, changes in fixation occur during locomotion or other simulated head movements. We investigated whether a constant camera rotation in a virtual scene modulates saccadic suppression. The users viewed 3D scenes from the vantage point of a virtual camera which was either stationary or rotated at a constant rate about a vertical axis (camera pan) or horizontal axis (camera tilt). During this motion, observers fixated an object that was suddenly displaced horizontally/vertically in the scene, triggering them to produce a saccade. During the saccade an additional sudden movement was applied to the virtual camera. We estimated discrimination thresholds for these transsaccadic camera shifts using a Bayesian adaptive procedure. With an ongoing camera pan, we found higher thresholds (less noticeability) for additional sudden horizontal camera motion. Likewise, during simulated vertical head movements (i.e. a camera tilt), vertical transsaccadic image displacements were better hidden from the users for both horizontal and vertical saccades. Understanding the effect of continuous movement on the visibility of a sudden transsaccadic change can help optimize the visual performance of gaze-contingent displays and improve user experience."
  },
  {
    "title": "Gaze-Adaptive Lenses for Feature-Rich Information Spaces",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 4: Visualization",
    "data": "June 2020",
    "authors": [
      "Fabian Goebel",
      "Kuno Kurzhals",
      "Victor R. Schinazi",
      "Peter Kiefer",
      "Martin Raubal"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391323",
    "citation": "2",
    "abstract": "The inspection of feature-rich information spaces often requires supportive tools that reduce visual clutter without sacrificing details. One common approach is to use focus+context lenses that provide multiple views of the data. While these lenses present local details together with global context, they require additional manual interaction. In this paper, we discuss the design space for gaze-adaptive lenses and present an approach that automatically displays additional details with respect to visual focus. We developed a prototype for a map application capable of displaying names and star-ratings of different restaurants. In a pilot study, we compared the gaze-adaptive lens to a mouse-only system in terms of efficiency, effectiveness, and usability. Our results revealed that participants were faster in locating the restaurants and more accurate in a map drawing task when using the gaze-adaptive lens. We discuss these results in relation to observed search strategies and inspected map areas."
  },
  {
    "title": "Visual Analytics and Annotation of Pervasive Eye Tracking Video",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 4: Visualization",
    "data": "June 2020",
    "authors": [
      "Kuno Kurzhals",
      "Nils Rodrigues",
      "Maurice Koch",
      "Michael Stoll",
      "Andres Bruhn",
      "Andreas Bulling",
      "Daniel Weiskopf"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391326",
    "citation": "3",
    "abstract": "We propose a new technique for visual analytics and annotation of long-term pervasive eye tracking data for which a combined analysis of gaze and egocentric video is necessary. Our approach enables two important tasks for such data for hour-long videos from individual participants: (1) efficient annotation and (2) direct interpretation of the results. Exemplary time spans can be selected by the user and are then used as a query that initiates a fuzzy search of similar time spans based on gaze and video features. In an iterative refinement loop, the query interface then provides suggestions for the importance of individual features to improve the search results. A multi-layered timeline visualization shows an overview of annotated time spans. We demonstrate the efficiency of our approach for analyzing activities in about seven hours of video in a case study and discuss feedback on our approach from novices and experts performing the annotation task."
  },
  {
    "title": "Teaching Eye Tracking Visual Analytics in Computer and Data Science Bachelor Courses",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 4: Visualization",
    "data": "June 2020",
    "authors": ["Michael Burch"],
    "DOI": "https://doi.org/10.1145/3379155.3391331",
    "citation": "3",
    "abstract": "Making students aware of eye tracking technologies can have a great benefit on the entire application field since they may build the next generation of eye tracking researchers. On the one hand students learn the usefulness and benefits of this technique for different scientific purposes like user evaluation to find design flaws or visual attention strategies, gaze-assisted interaction to enhance and augment traditional interaction techniques, or as a means to improve virtual reality experiences. However, on the other hand, the large amount of recorded data means a challenge for data analytics in order to find rules, patterns, but also anomalies in the data, finally leading to insights and knowledge to understand or predict eye movement patterns which can have synergy effects for both disciplines - eye tracking and visual analytics. In this paper we will describe the challenges of teaching eye tracking combined with visual analytics in a computer and data science bachelor course with 42 students in an active learning scenario following four teaching stages. Some of the student project results are shown to demonstrate learning outcomes with respect to eye tracking data analysis and visual analytics techniques."
  },
  {
    "title": "Deep semantic gaze embedding and scanpath comparison for expertise classification during OPT viewing",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 5: Applications",
    "data": "June 2020",
    "authors": [
      "Nora Castner",
      "Thomas C Kuebler",
      "Katharina Scheiter",
      "Juliane Richter",
      "Therese Eder",
      "Fabian Huettig",
      "Constanze Keutel",
      "Enkelejda Kasneci"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391320",
    "citation": "23",
    "abstract": "Modeling eye movement indicative of expertise behavior is decisive in user evaluation. However, it is indisputable that task semantics affect gaze behavior. We present a novel approach to gaze scanpath comparison that incorporates convolutional neural networks (CNN) to process scene information at the fixation level. Image patches linked to respective fixations are used as input for a CNN and the resulting feature vectors provide the temporal and spatial gaze information necessary for scanpath similarity comparison. We evaluated our proposed approach on gaze data from expert and novice dentists interpreting dental radiographs using a local alignment similarity score. Our approach was capable of distinguishing experts from novices with 93% accuracy while incorporating the image semantics. Moreover, our scanpath comparison using image patch features has the potential to incorporate task semantics from a variety of tasks."
  },
  {
    "title": "Eyes on URLs: Relating Visual Behavior to Safety Decisions",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 5: Applications",
    "data": "June 2020",
    "authors": [
      "Niveta Ramkumar",
      "Vijay Kothari",
      "Caitlin Mills",
      "Ross Koppel",
      "Jim Blythe",
      "Sean Smith",
      "Andrew L Kun"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391328",
    "citation": "4",
    "abstract": "Individual and organizational computer security rests on how people interpret and use the security information they are presented. One challenge is determining whether a given URL is safe or not. This paper explores the visual behaviors that users employ to gauge URL safety. We conducted a user study on 20 participants wherein participants classified URLs as safe or unsafe while wearing an eye tracker that recorded eye gaze (where they look) and pupil dilation (a proxy for cognitive effort). Among other things, our findings suggest that: users have a cap on the amount of cognitive resources they are willing to expend on vetting a URL; they tend to believe that the presence of www in the domain name indicates that the URL is safe; and they do not carefully parse the URL beyond what they perceive as the domain name."
  },
  {
    "title": "Modeling Metacomprehension Monitoring Accuracy with Eye Gaze on Informational Content in a Multimedia Learning Environment",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 5: Applications",
    "data": "June 2020",
    "authors": ["Megan D Wiedbusch", "Roger Azevedo"],
    "DOI": "https://doi.org/10.1145/3379155.3391329",
    "citation": "8",
    "abstract": "Multimedia learning environments support learners in developing self-regulated learning (SRL) strategies. However, capturing these strategies and cognitive processes can be difficult for researchers because cognition is often inferred, not directly measured. This study sought to model self-reported metacognitive judgments using eye-tracking from 60 undergraduate students as they learned about biological systems with MetaTutorIVH, a multimedia learning environment. We found that participants’ gaze behaviors were different between the perceived relevance of the instructional content provided regardless of the actual content relevance. Additionally, we fit a cumulative link mixed effects ordinal regression model to explain reported metacognitive judgments based on content fixations, relevance, and presentation type. Main effects were found for all variables and several interactions between both fixations and content relevance as well as content fixations and presentation type. Surprisingly, accurate metacognitive judgments did not explain performance. Implication for multimedia learning environment design are discussed."
  },
  {
    "title": "Selection of Eye-Tracking Stimuli for Prediction by Sparsely Grouped Input Variables for Neural Networks: towards Biomarker Refinement for Autism",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 5: Applications",
    "data": "June 2020",
    "authors": [
      "Beibin Li",
      "Erin Barney",
      "Caitlin Hudac",
      "Nicholas Nuechterlein",
      "Pamela Ventola",
      "Linda Shapiro",
      "Frederick Shic"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391334",
    "citation": "8",
    "abstract": "Eye tracking has become a powerful tool in the study of autism spectrum disorder (ASD). Current, large-scale efforts aim to identify specific eye-tracking stimuli to be used as biomarkers for ASD, with the intention of informing the diagnostic process, monitoring therapeutic response, predicting outcomes, or identifying subgroups with the spectrum. However, there are hundreds of candidate experimental paradigms, each of which contains dozens or even hundreds of individual stimuli. Each stimuli is associated with an array of potential derived outcome variables, thus the number of variables to consider can be enormous. Standard variable selection techniques are not applicable to this problem, because selection must be done at the level of stimuli and not individual variables. In other words, this is a grouped variable selection problem. In this work, we apply lasso, group lasso, and a new technique, Sparsely Grouped Input Variables for Neural Network (SGIN), to select experimental stimuli for group discrimination and regression with clinical variables. Using a dataset obtained from children with and without ASD who were administered a battery containing 109 different stimuli presentations involving 9647 features, we are able to retain strong group separation even with only 11 out of the 109 stimuli. This work sets the stage for concerted techniques designed around engines to iteratively refine and define next-generation biomarkers using eye tracking for psychiatric conditions. http://github.com/beibinli/SGIN"
  },
  {
    "title": "Towards inferring cognitive state changes from pupil size variations in real world conditions",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Cognition",
    "data": "June 2020",
    "authors": [
      "Naga Venkata Kartheek Medathati",
      "Ruta Desai",
      "James Hillis"
    ],
    "DOI": "https://doi.org/10.1145/3379155.3391319",
    "citation": "7",
    "abstract": "The ability to infer cognitive state from pupil size provides an opportunity to reduce friction in human-computer interaction. For example, the computer could automatically turn off notifications when it detects, using pupil size, that the user is deeply focused on a task. However, our ability to do so has been limited. A principal reason for this is that pupil size varies with multiple factors (e.g., luminance and vergence), so isolating variations due to cognitive processes is challenging. In particular, rigorous benchmarks to detect cognitively-driven pupillary event from continuous stream of data in real-world settings have not been well-established. Motivated by these challenges, we first performed visual search experiments at room scale, with natural indoor conditions with real stimuli where the timing of the detection event was user-controlled. In spite of the natural experimental conditions, we found that the mean pupil dilation response to a cognitive state change (i.e., search target detected) was qualitatively similar and consistent with more controlled laboratory studies. Next, to address the challenge of detecting state changes from continuous data, we fit discriminant models using Support Vector Machine (SVM) computed on short epochs of 1-2 seconds extracted using rolling windows. We tested three different features (descriptive statistics, baseline corrected pupil size, and local Z-score) with our models. We obtained best performance using local Z-score as a feature (mean Area under the Curve (AUC) of 0.6). Our naturalistic experiments and modeling results provide a baseline for future research aimed at leveraging pupillometry for real-world applications."
  },
  {
    "title": "Cognitive Load during Eye-typing",
    "conferenceTitle": "ETRA '20 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Cognition",
    "data": "June 2020",
    "authors": ["Tanya Bafna", "John Paulin Paulin Hansen", "Per Baekgaard"],
    "DOI": "https://doi.org/10.1145/3379155.3391333",
    "citation": "7",
    "abstract": "In this paper, we have measured cognitive load during an interactive eye-tracking task. Eye-typing was chosen as the task, because of its familiarity, ubiquitousness and ease. Experiments with 18 participants, where they memorized and eye-typed easy and difficult sentences over four days, were used to compare the difficulty levels of the tasks using subjective scores and eye-metrics like blink duration, frequency and interval and pupil dilation were explored, in addition to performance measures like typing speed, error rate and attended but not selected rate. Typing performance lowered with increased task difficulty, while blink frequency, duration and interval were higher for the difficult tasks. Pupil dilation indicated the memorization process, but did not demonstrate a difference between easy and difficult tasks."
  },
  {
    "title": "MutualEyeContact: A conversation analysis tool with focus on eye contact",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Alexander Schafer",
      "Tomoko Isomura",
      "Gerd Reis",
      "Katsumi Watanabe",
      "Didier Stricker"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391340",
    "citation": "0",
    "abstract": "Eye contact between individuals is particularly important for understanding human behaviour. To further investigate the importance of eye contact in social interactions, portable eye tracking technology seems to be a natural choice. However, the analysis of available data can become quite complex. Scientists need data that is calculated quickly and accurately. Additionally, the relevant data must be automatically separated to save time. In this work, we propose a tool called MutualEyeContact which excels in those tasks and can help scientists to understand the importance of (mutual) eye contact in social interactions. We combine state-of-the-art eye tracking with face recognition based on machine learning and provide a tool for analysis and visualization of social interaction sessions. This work is a joint collaboration of computer scientists and cognitive scientists. It combines the fields of social and behavioural science with computer vision and deep learning."
  },
  {
    "title": "Polarized Near-Infrared Light Emission for Eye Gaze Estimation",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Koki Koshikawa",
      "Masato Sasaki",
      "Takamasa Utsu",
      "Kentaro Takemura"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391342",
    "citation": "3",
    "abstract": "The number of near-infrared light-emitting diodes (LEDs) is increasing to improve the accuracy and robustness of eye-tracking methods, and it is necessary to determining the identifiers (IDs) of the LEDs when applying multiple light sources. Therefore, we propose polarized near-infrared light emissions for an eye gaze estimation. We succeeded in determining the IDs of LEDs using polarization information. In addition, we remove glints from the cornea for correctly detecting the pupil center. We confirmed the effectiveness of using polarized near-infrared light emissions through evaluation experiments."
  },
  {
    "title": "Estimating Point-of-Gaze using Smooth Pursuit Eye Movements without Implicit and Explicit User-Calibration",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Yuto Tamura", "Kentaro Takemura"],
    "DOI": "https://doi.org/10.1145/3379156.3391343",
    "citation": "2",
    "abstract": "Detecting the point-of-gaze in the real world is a challenging problem in eye-tracking applications. The point-of-gaze is estimated using geometry constraints, and user-calibration is required. In addition, the distances of the focused targets are variable and large in the real world. Therefore, a calibration-free approach without geometry constraints is needed to estimate the point-of-gaze. Recent studies have investigated smooth pursuit eye movements (smooth pursuits) for human-computer interaction applications, and we consider that these smooth pursuits can also be employed in eye tracking. Therefore, we developed a method for estimating the point-of-gaze using smooth pursuits without any requirement for implicit and explicit user-calibration. In this method, interest points are extracted from the scene image, and the point-of-gaze is detected using these points, which are strongly correlated with eye movements. We performed a comparative experiment in a real environment and demonstrated the feasibility of the proposed method."
  },
  {
    "title": "Neural networks for optical vector and eye ball parameter estimation",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Wolfgang Fuhl", "Hong Gao", "Enkelejda Kasneci"],
    "DOI": "https://doi.org/10.1145/3379156.3391346",
    "citation": "9",
    "abstract": "In this work we evaluate neural networks, support vector machines and decision trees for the regression of the center of the eyeball and the optical vector based on the pupil ellipse. In the evaluation we analyze single ellipses as well as window-based approaches as input. Comparisons are made regarding accuracy and runtime. The evaluation gives an overview of the general expected accuracy with different models and amounts of input ellipses. A simulator was implemented for the generation of the training and evaluation data. For a visual evaluation and to push the state of the art in optical vector estimation, the best model was applied to real data. This real data came from public data sets in which the ellipse is already annotated by an algorithm. The optical vectors on real data and the generator are made publicly available. Link to the generator and models."
  },
  {
    "title": "Tiny convolution, decision tree, and binary neuronal networks for robust and real time pupil outline estimation",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Wolfgang Fuhl", "Hong Gao", "Enkelejda Kasneci"],
    "DOI": "https://doi.org/10.1145/3379156.3391347",
    "citation": "9",
    "abstract": "In this work, we compare the use of convolution, binary, and decision tree layers in neural networks for the estimation of pupil landmarks. These landmarks are used for the computation of the pupil ellipse and have proven to be effective in previous research. The evaluated structure of the neural networks is the same for all layers and as small as possible to ensure a real-time application. The evaluations include the accuracy of the ellipse determination based on the Jaccard Index and the pupil center. Furthermore, the CPU runtime is considered to make statements about the real-time usability. The trained models are also optimized using pruning to improve the runtime. These optimized nets are also evaluated with respect to the Jaccard index and the accuracy of the pupil center estimation. Link to the framework and models."
  },
  {
    "title": "Protecting from Lunchtime Attack Using an Uncalibrated Eye Tracker Signal",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Pawel Kasprowski", "Katarzyna Harezlak"],
    "DOI": "https://doi.org/10.1145/3379156.3391348",
    "citation": "1",
    "abstract": "Eye movement-based biometric has been developed for over 15 years, but for now - to the authors’ knowledge - no commercial applications utilize this modality. There are many reasons for this, starting from still low accuracy and ending with the problematic setup. One of the essential elements of this setup is the calibration, as nearly every eye tracker needs to be calibrated before its first usage. This procedure makes any authentication based on eye movement a cumbersome and lengthy process."
  },
  {
    "title": "A Comparison of a Transition-based and a Sequence-based Analysis of AOI Transition Sequences",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Chia-Kai Yang", "Tanja Blascheck", "Chat Wacharamanotham"],
    "DOI": "https://doi.org/10.1145/3379156.3391349",
    "citation": "3",
    "abstract": "Several visual analytics (VA) systems are used for analyzing eye-tracking data because they synergize human-in-the-loop exploration with speed and accuracy of the computer. In the VA systems, the choices of visualization techniques could afford discovering certain types of insights while hindering others. Understanding these affordances and hindrances is essential to design effective VA systems. In this paper, we focus on two approaches for visualizing AOI transitions: the transition-based approach (exemplified by the radial transition graph, RTG) and the sequence-based approach (exemplified by the Alpscarf). We captured the insights generated by two analysts who individually use each visualization technique on the same dataset. Based on the results, we identify four phases of analytic activities and discuss opportunities that the two visualization approaches can complement each other. We point out design implications for VA systems that combine these visualization approaches."
  },
  {
    "title": "Detection of Saccades and Quick-Phases in Eye Movement Recordings with Nystagmus",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Sai Akanksha Punuganti", "Jorge Otero-Millan PhD"],
    "DOI": "https://doi.org/10.1145/3379156.3391353",
    "citation": "2",
    "abstract": "Benign Paroxysmal Positional Vertigo (BPPV) is the most common cause of vertigo and dizziness. Patients with those symptoms can be diagnosed by the presence of a specific pattern of nystagmus during the Dix-Hallpike maneuver. However, almost half of dizzy patients visiting Emergency Department (ED) are misdiagnosed, leading to significant morbidity and high medical costs. This can be attributed to the lack of specialized expertise of front-line physicians and to the lack of validated automatic commercial devices and software for nystagmus detection and quantification. Here we aim to enhance saccade detection thereby improving automatic nystagmus quantification. The proposed method is evaluated on a nystagmus dataset recorded from patients in the ED as they undergo the Dix-Hallpike maneuver. Additionally, the proposed method is also tested on a publicly available saccade dataset and compared with state-of-the-art eye movement detection methods."
  },
  {
    "title": "EyeLinks: Methods to compute reliable stereo mappings used for eye gaze tracking",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Moayad Mokatren", "Tsvi Kuflik", "Ilan Shimshoni"],
    "DOI": "https://doi.org/10.1145/3379156.3391354",
    "citation": "1",
    "abstract": "We present methods for extracting corneal images and estimating pupil centers continuously and reliably using head worn glasses that consists of two eye cameras. An existing CNN was modified for detecting pupils in IR and RGB images, and stereo vision together with 2D and 3D models are used. We confirm the feasibility of the proposed methods through user study results, which show that the methods can be used in future real gaze estimation systems."
  },
  {
    "title": "EOG-Based Ocular and Gaze Angle Estimation Using an Extended Kalman Filter",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Nathaniel Barbara",
      "Tracey A. Camilleri",
      "Kenneth P. Camilleri"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391357",
    "citation": "3",
    "abstract": "In this work, a novel method to estimate the ocular pose from electrooculography (EOG) signals is proposed. This method is based on an electrical battery model of the eye which relates the EOG potential to the distances between an electrode and the left/right cornea and retina centre points. In this work, this model is used to estimate the ocular angles (OAs), that is the orientation of the two ocular globes separately. Using this approach, an average cross-validated horizontal and vertical OA estimation error of 2.91 ± 0.86° and 2.42 ± 0.58° respectively was obtained. Furthermore, we show how these OA estimates may be used to estimate the gaze angles (GAs) without requiring the distance between the subject’s face-plane and the target-plane, as in previous work. Using the proposed method, a cross-validated horizontal and vertical GA estimation error of 2.13 ± 0.73° and 2.42 ± 0.58° respectively was obtained, which compares well with the previous distance-based GA estimation technique."
  },
  {
    "title": "Voluntary Pupil Control in Noisy Environments",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Jan Ehlers", "Annika Meinecke"],
    "DOI": "https://doi.org/10.1145/3379156.3391358",
    "citation": "0",
    "abstract": "Eyes-only interaction in HCI usually requires visual focusing to carry out input arguments. For complex items, however, input requests are mixed with visual inspection, increasing the likelihood of false positive entries. Recent research applied cognitive control of pupil dilations as an input mechanism that works independently from any fixation times. However, experiments were exclusively conducted under laboratory conditions. The present study investigates the potential of exerting control over pupil diameter in noisy environments. Participants explore various techniques in a controlled sequence of (real-time) feedback sessions. Thereafter, they follow a predefined course across indoor and outdoor stations to either induce dilations or perform a control task. Results indicate strong interindividual differences in performance. Outdoor pupil dynamics exhibit a high degree of variation including a considerable number of unintended dilations. Accordingly, environmental control seems to constitute a necessary condition to exert cognitive control on pupil diameter and enable pupil-based interaction in HCI."
  },
  {
    "title": "Challenges in Interpretability of Neural Networks for Eye Movement Data",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Ayush Kumar",
      "Prantik Howlader",
      "Rafael Garcia",
      "Daniel Weiskopf",
      "Klaus Mueller"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391361",
    "citation": "5",
    "abstract": "Many applications in eye tracking have been increasingly employing neural networks to solve machine learning tasks. In general, neural networks have achieved impressive results in many problems over the past few years, but they still suffer from the lack of interpretability due to their black-box behavior. While previous research on explainable AI has been able to provide high levels of interpretability for models in image classification and natural language processing tasks, little effort has been put into interpreting and understanding networks trained with eye movement datasets. This paper discusses the importance of developing interpretability methods specifically for these models. We characterize the main problems for interpreting neural networks with this type of data, how they differ from the problems faced in other domains, and why existing techniques are not sufficient to address all of these issues. We present preliminary experiments showing the limitations that current techniques have and how we can improve upon them. Finally, based on the evaluation of our experiments, we suggest future research directions that might lead to more interpretable and explainable neural networks for eye tracking."
  },
  {
    "title": "Predicting image influence on visual saliency distribution: the focal and ambient dichotomy",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Olivier Le Meur", "Pierre-Adrien Fons"],
    "DOI": "https://doi.org/10.1145/3379156.3391362",
    "citation": "4",
    "abstract": "The computational modelling of visual attention relies entirely on visual fixations that are collected during eye-tracking experiments. Although all fixations are assumed to follow the same attention paradigm, some studies suggest the existence of two visual processing modes, called ambient and focal. In this paper, we present the high discrepancy between focal and ambient saliency maps and propose an automatic method for inferring the degree of focalness of an image. This method opens new avenues for the computational modelling of saliency models and their benchmarking."
  },
  {
    "title": "Positional head-eye tracking outside the lab: an open-source solution",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Peter Hausamann", "Christian Sinnott", "Paul R. MacNeilage"],
    "DOI": "https://doi.org/10.1145/3379156.3391365",
    "citation": "12",
    "abstract": "Simultaneous head and eye tracking has traditionally been confined to a laboratory setting and real-world motion tracking limited to measuring linear acceleration and angular velocity. Recently available mobile devices such as the Pupil Core eye tracker and the Intel RealSense T265 motion tracker promise to deliver accurate measurements outside the lab. Here, the researchers propose a hard- and software framework that combines both devices into a robust, usable, low-cost head and eye tracking system. The developed software is open source and the required hardware modifications can be 3D printed. The researchers demonstrate the system’s ability to measure head and eye movements in two tasks: an eyes-fixed head rotation task eliciting the vestibulo-ocular reflex inside the laboratory, and a natural locomotion task where a subject walks around a building outside of the laboratory. The resultant head and eye movements are discussed, as well as future implementations of this system."
  },
  {
    "title": "The Perception Engineer’s Toolkit for Eye-Tracking data analysis",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Thomas C Kubler"],
    "DOI": "https://doi.org/10.1145/3379156.3391366",
    "citation": "3",
    "abstract": "Tools for eye-tracking data analysis are as of now either provided as proprietary software by the eye-tracker manufacturer or published by researchers under licenses that are problematic for some use-cases (e.g., GPL3). This lead to repeated re-implementation of the most basic building blocks, such as event filters, often resulting in incomplete, incomparable and even erroneous implementations."
  },
  {
    "title": "Gaze estimation problem tackled through synthetic images",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Gonzalo Garde",
      "Andoni Larumbe-Bergera",
      "Benoît Bossavit",
      "Rafael Cabeza",
      "Sonia Porta",
      "Arantxa Villanueva"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391368",
    "citation": "1",
    "abstract": "In this paper, we evaluate a synthetic framework to be used in the field of gaze estimation employing deep learning techniques. The lack of sufficient annotated data could be overcome by the utilization of a synthetic evaluation framework as far as it resembles the behavior of a real scenario. In this work, we use U2Eyes synthetic environment employing I2Head datataset as real benchmark for comparison based on alternative training and testing strategies. The results obtained show comparable average behavior between both frameworks although significantly more robust and stable performance is retrieved by the synthetic images. Additionally, the potential of synthetically pretrained models in order to be applied in user’s specific calibration strategies is shown with outstanding performances."
  },
  {
    "title": "A Calibration Framework for Photosensor-based Eye-Tracking System",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Dmytro Katrychuk", "Henry Griffith", "Oleg Komogortsev"],
    "DOI": "https://doi.org/10.1145/3379156.3391370",
    "citation": "0",
    "abstract": "The majority of eye-tracking systems require user-specific calibration to achieve suitable accuracy. Traditional calibration is performed by presenting targets at fixed locations that form a certain coverage of the device screen. If simple regression methods are used to learn a gaze map from the recorded data, the risk of overfitting is minimal. This is not the case if a gaze map is formed using neural networks, as is often employed in photosensor oculography (PSOG), which raises the question of careful design of calibration procedure. This paper evaluates different calibration data parsing approaches and the collection time-performance trade-off effect of grid density to build a calibration framework for PSOG with the use of video-based simulation framework."
  },
  {
    "title": "Getting more out of Area of Interest (AOI) analysis with SPLOT",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": ["Artem V. Belopolsky"],
    "DOI": "https://doi.org/10.1145/3379156.3391372",
    "citation": "0",
    "abstract": "To analyze eye-tracking data the viewed image is often divided into areas of interest (AOI). However, the temporal dynamics of eye movements towards the AOI is often lost either in favor of summary statistics (e.g., proportion of fixations or dwell time) or is significantly reduced by “binning” the data and computing the same summary statistic over each time bin. This paper introduces SPLOT: smoothed proportion of looks over time method for analyzing the eye movement dynamics across AOI. SPLOT comprises of a complete workflow, from visualization of the time-course to performing statistical analysis on it using cluster-based permutation testing. The possibilities of SPLOT are illustrated by applying it to an existing dataset of eye movements of radiologists diagnosing a chest X-ray."
  },
  {
    "title": "GazeMetrics: An Open-Source Tool for Measuring the Data Quality of HMD-based Eye Trackers",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Isayas B. Adhanom",
      "Samantha C. Lee",
      "Eelke Folmer",
      "Paul MacNeilage"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391374",
    "citation": "12",
    "abstract": "As virtual reality (VR) garners more attention for eye tracking research, knowledge of accuracy and precision of head-mounted display (HMD) based eye trackers becomes increasingly necessary. It is tempting to rely on manufacturer-provided information about the accuracy and precision of an eye tracker. However, unless data is collected under ideal conditions, these values seldom align with on-site metrics. Therefore, best practices dictate that accuracy and precision should be measured and reported for each study. To address this issue, we provide a novel open-source suite for rigorously measuring accuracy and precision for use with a variety of HMD-based eye trackers. This tool is customizable without having to alter the source code, but changes to the code allow for further alteration. The outputs are available in real time and easy to interpret, making eye tracking with VR more approachable for all users."
  },
  {
    "title": "Benefits of temporal information for appearance-based gaze estimation",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Methods",
    "data": "June 2020",
    "authors": [
      "Cristina Palmero Cantarino",
      "Oleg V. Komogortsev",
      "Sachin S Talathi"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391376",
    "citation": "7",
    "abstract": "State-of-the-art appearance-based gaze estimation methods, usually based on deep learning techniques, mainly rely on static features. However, temporal trace of eye gaze contains useful information for estimating a given gaze point. For example, approaches leveraging sequential eye gaze information when applied to remote or low-resolution image scenarios with off-the-shelf cameras are showing promising results. The magnitude of contribution from temporal gaze trace is yet unclear for higher resolution/frame rate imaging systems, in which more detailed information about an eye is captured. In this paper, we investigate whether temporal sequences of eye images, captured using a high-resolution, high-frame rate head-mounted virtual reality system, can be leveraged to enhance the accuracy of an end-to-end appearance-based deep-learning model for gaze estimation. Performance is compared against a static-only version of the model. Results demonstrate statistically-significant benefits of temporal information, particularly for the vertical component of gaze."
  },
  {
    "title": "Privacy Preserving Gaze Estimation using Synthetic Images via a Randomized Encoding Based Framework",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: Privacy and Security",
    "data": "June 2020",
    "authors": [
      "Efe Bozkir",
      "Ali Burak Ünal",
      "Mete Akgün",
      "Enkelejda Kasneci",
      "Nico Pfeifer"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391364",
    "citation": "12",
    "abstract": "Eye tracking is handled as one of the key technologies for applications that assess and evaluate human attention, behavior, and biometrics, especially using gaze, pupillary, and blink behaviors. One of the challenges with regard to the social acceptance of eye tracking technology is however the preserving of sensitive and personal information. To tackle this challenge, we employ a privacy-preserving framework based on randomized encoding to train a Support Vector Regression model using synthetic eye images privately to estimate the human gaze. During the computation, none of the parties learn about the data or the result that any other party has. Furthermore, the party that trains the model cannot reconstruct pupil, blinks or visual scanpath. The experimental results show that our privacy-preserving framework is capable of working in real-time, with the same accuracy as compared to non-private version and could be extended to other eye tracking related problems."
  },
  {
    "title": "Privacy-Preserving Eye Videos using Rubber Sheet Model",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: Privacy and Security",
    "data": "June 2020",
    "authors": ["Aayush Kumar Chaudhary", "Jeff B Pelz"],
    "DOI": "https://doi.org/10.1145/3379156.3391375",
    "citation": "5",
    "abstract": "Video-based eye trackers estimate gaze based on eye images/videos. As security and privacy concerns loom over technological advancements, tackling such challenges is crucial. We present a new approach to handle privacy issues in eye videos by replacing the current identifiable iris texture with a different iris template in the video capture pipeline based on the Rubber Sheet Model. We extend to image blending and median-value representations to demonstrate that videos can be manipulated without significantly degrading segmentation and pupil detection accuracy."
  },
  {
    "title": "Deep Audio-Visual Saliency: Baseline Model and Data",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 3: Saliency",
    "data": "June 2020",
    "authors": [
      "Hamed Rezazadegan Tavakoli",
      "Ali Borji",
      "Juho Kannala",
      "Esa Rahtu"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391337",
    "citation": "5",
    "abstract": "This paper introduces a conceptually simple and effective Deep Audio-Visual Embedding for dynamic saliency prediction dubbed “DAVE” in conjunction with our efforts towards building an Audio-Visual Eye-tracking corpus named “AVE”. Despite existing a strong relation between auditory and visual cues for guiding gaze during perception, video saliency models only consider visual cues and neglect the auditory information that is ubiquitous in dynamic scenes. Here, we propose a baseline deep audio-visual saliency model for multi-modal saliency prediction in the wild. Thus the proposed model is intentionally designed to be simple. A video baseline model is also developed on the same architecture to assess effectiveness of the audio-visual models on a fair basis. We demonstrate that audio-visual saliency model outperforms the video saliency models. The data and code are available at https://hrtavakoli.github.io/AVE/ and https://github.com/hrtavakoli/DAVE"
  },
  {
    "title": "Exploiting the GBVS for Saliency aware Gaze Heatmaps",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 3: Saliency",
    "data": "June 2020",
    "authors": [
      "David Geisler",
      "Daniel Weber",
      "Nora Castner",
      "Enkelejda Kasneci"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391367",
    "citation": "2",
    "abstract": "Analyzing visual perception in scene images is dominated by two different approaches: 1.) Eye Tracking, which allows us to measure the visual focus directly by mapping a detected fixation to a scene image, and 2.) Saliency maps, which predict the perceivability of a scene region by assessing the emitted visual stimulus with respect to the retinal feature extraction. One of the best-known algorithms for calculating saliency maps is GBVS. In this work, we propose a novel visualization method by generating a joint fixation-saliency heatmap. By incorporating a tracked gaze signal into the GBVS, the proposed method equilibrates the fixation frequency and duration to the scene stimulus, and thus visualizes the rate of the extracted visual stimulus by the spectator."
  },
  {
    "title": "A Psychophysics-inspired Model of Gaze Selection Performance",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 4: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": ["Immo Schuetz", "T. Scott Murdison", "Marina Zannoli"],
    "DOI": "https://doi.org/10.1145/3379156.3391336",
    "citation": "6",
    "abstract": "Eye gaze promises to be a fast and intuitive way of interacting with technology. Importantly, the performance of a gaze selection paradigm depends on the eye tracker used: Higher tracking accuracy allows for selection of smaller targets, and higher precision and sampling rate allow for faster and more robust interaction. Here we present a novel approach to predict the minimal eye tracker specifications required for gaze-based selection. We quantified selection performance for targets of different sizes while recording high-fidelity gaze data. Selection performance across target sizes was well modeled by a sigmoid similar to a psychometric function. We then simulated lower tracker fidelity by adding noise, a constant spatial bias, or temporal sub-sampling of the recorded data while re-fitting the model each time. Our approach can inform design by predicting performance for a given interface element and tracker fidelity or the minimal element size for a specific performance level."
  },
  {
    "title": "Optimizing user interfaces in food production: gaze tracking is more sensitive for A-B-testing than behavioral data alone",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 4: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": [
      "Daniel Walper",
      "Julia Kassau",
      "Philipp Methfessel",
      "Timo Pronold",
      "Wolfgang Einhauser"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391351",
    "citation": "2",
    "abstract": "Eye-tracking data often provide access to information about users’ strategies and preferences that extend beyond purely behavioral data. Thanks to modern eye-tracking technology, gaze can be tracked rather unobtrusively in real-world settings. Here we examine the usefulness of gaze tracking with a mobile eye-tracker for interface design in an industrial setting, specifically the operation of a food production line. We use a mock task that is similar in its interface usage to the actual production task in routine machine operation. We compare two interface designs to each other as well as two levels of user expertise. We do not find any effects of experience or interface type in the behavioral data - in particular, both user groups needed the same time to complete the task on average. However, gaze data reveals different strategies: users with high experience in using the interface spend significantly less time looking at the screen – that is, actually interacting with the interface – in absolute terms as well as expressed as fraction of the total time needed to complete the task. This exemplifies how gaze tracking can be utilized to uncover different user-dependent strategies that would not be accessible through behavioral data alone."
  },
  {
    "title": "Comparing Eye Movements Between Physical Rotation Interaction Techniques",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 4: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": ["Sven Bertel", "Stefanie Wetzel"],
    "DOI": "https://doi.org/10.1145/3379156.3391355",
    "citation": "0",
    "abstract": "Recent studies have shown a number of procedural similarities between solving problems in mental and in physical rotation. Such similarities open up the interesting option to study mental rotation indirectly through physical rotation, with the advantage that physical rotation processes can be much more easily observed than mental ones. To better assess where solution processes in mental and physical rotation differ, though, it is important to know what influence any specific interaction method in physical rotation will have. We present results from a comparison of two such interaction methods: a one-handed, touch-based and a two-handed, ball-based method. Our analysis focuses on fixation durations and saccade amplitudes as proxies for mental load. Results show, importantly, that the choice of interaction method seems to matter but little. We therefore suggest that the existing findings of past studies that have compared mental to physical rotation are likely highly comparable, despite the fact that different interaction techniques were used."
  },
  {
    "title": "Spontaneous Gaze Gesture Interaction in the Presence of Noises and Various Types of Eye Movements",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 4: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": ["Sunu Wibirama", "Suatmi Murnani", "Noor Akhmad Setiawan"],
    "DOI": "https://doi.org/10.1145/3379156.3391363",
    "citation": "10",
    "abstract": "Gaze gesture is a desirable technique for a spontaneous and pervasive gaze interaction due to its insensitivity to spatial accuracy. Unfortunately, gaze gesture-based object selection utilizing correlation coefficient is prone to a low object selection accuracy due to presence of noises. In addition, effect of various types of eye movements that present in gaze gesture-based object selection has not been tackled properly. To overcome these problems, we propose a denoising method for gaze gesture-based object selection using First Order IIR Filter and an event detection method based on the Hidden Markov Model. The experimental results show that the proposed method yielded the best object selection accuracy of . The result suggests that a spontaneous gaze gesture-based object selection is feasible to be developed in the presence of noises and various types of eye movements."
  },
  {
    "title": "GazeLockPatterns: Comparing Authentication Using Gaze and Touch for Entering Lock Patterns",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 4: User Interfaces and Interaction",
    "data": "June 2020",
    "authors": [
      "Yasmeen Abdrabou",
      "Ken Pfeuffer",
      "Mohamed Khamis",
      "Florian Alt"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391371",
    "citation": "1",
    "abstract": "In this work, we present a comparison between Android’s lock patterns for mobile devices (TouchLockPatterns) and an implementation of lock patterns that uses gaze input (GazeLockPatterns). We report on results of a between subjects study (N=40) to show that for the same layout of authentication interface, people employ comparable strategies for pattern composition. We discuss the pros and cons of adapting lock patterns to gaze-based user interfaces. We conclude by opportunities for future work, such as using data collected during authentication for calibrating eye trackers."
  },
  {
    "title": "Decoding Task From Oculomotor Behavior In Virtual Reality",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 5: Virtual Reality",
    "data": "June 2020",
    "authors": [
      "Ashima Keshava",
      "Anete Aumeistere",
      "Krzysztof Izdebski",
      "Peter Konig"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391338",
    "citation": "4",
    "abstract": "In the present study, we aim to explore whether and how well we can predict tasks based on eye movements in a virtual environment. We designed four different tasks in which participants had to align two cubes of different sizes. To define where participants looked, we used a ray-based method to calculate the point-of-regard (POR) on each cube at each time point. Using leave-one-subject-out cross-validation, our model performed well with an f1-score of 0.51 ± 0.17 (chance level 0.25) in predicting the four alignment types. Results suggest that the type of task can be decoded based on the aggregation of PORs. We further discuss the implications of object size on task inference and thus set an exciting road-map for how to design intention recognition experiments in virtual reality."
  },
  {
    "title": "A Novel -Eye-Tracking Sensor for AR Glasses Based on Laser Self-Mixing Showing Exceptional Robustness Against Illumination",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 5: Virtual Reality",
    "data": "June 2020",
    "authors": [
      "Johannes Meyer",
      "Thomas Schlebusch",
      "Hans Spruit",
      "Jochen Hellmig",
      "Enkelejda Kasneci"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391352",
    "citation": "8",
    "abstract": "The integration of eye-tracking sensors in next-generation AR glasses will increase usability and enable new interaction concepts. Consumer AR glasses emphasize however additional requirements to eye-tracking sensors, such as high integratability and robustness to ambient illumination. We propose a novel eye-tracking sensor based on the self-mixing interference (SMI) effect of lasers. In consequence, our sensor as small as a grain of sand shows exceptional robustness against ambient radiation compared to conventional camera-based eye trackers. In this paper, we evaluate ambient light robustness under different illumination conditions for video-based oculography, conventional scanned laser eye tracking as well as the SMI-based sensor."
  },
  {
    "title": "Towards Predicting Reading Comprehension From Gaze Behavior",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": [
      "Seoyoung Ahn",
      "Conor Kelton",
      "Aruna Balasubramanian",
      "Greg Zelinsky"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391335",
    "citation": "12",
    "abstract": "As readers of a language, we all agree to move our eyes in roughly the same way. Yet might there be hidden within this self-similar behavior subtle clues as to how a reader is understanding the material being read? Here we attempt to decode a reader’s eye movements to predict their level of text comprehension and related states. Eye movements were recorded from 95 people reading 4 published SAT passages, each followed by corresponding SAT questions and self-evaluation questionnaires. A sequence of 21 fixation-location (x,y), fixation-duration, and pupil-size features were extracted from the reading behavior and input to two deep networks (CNN/RNN), which were used to predict the reader’s comprehension level and other comprehension-related variables. The best overall comprehension prediction accuracy was 65% (cf. null accuracy = 54%) obtained by CNN. This prediction generalized well to fixations on new passages (64%) from the same readers, but did not generalize to fixations from new readers (41%), implying substantial individual differences in reading behavior. Our work is the first attempt to predict comprehension from fixations using deep networks, where we hope that our large reading dataset and our protocol for evaluation will benefit the development of new methods for predicting reading comprehension by decoding gaze behavior."
  },
  {
    "title": "Freezing of Gaze in Anticipation of Avoidable Threat: A threat-specific proxy for freezing-like behavior in humans",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": ["Alma-Sophia Merscher", "Matthias Gamer"],
    "DOI": "https://doi.org/10.1145/3379156.3391345",
    "citation": "0",
    "abstract": "A previous study demonstrated a freezing-like pattern of eye movements when participants could escape from aversive stimulation by a fast button press. Freezing of gaze was characterized by more centralized, fewer and longer fixations. Here, we aimed at examining whether 1) visual exploration is also reduced when a subsequent threat-reaction requires distributed attention and if so 2) whether this visual pattern is threat-specific. We measured gaze behavior while participants anticipated a certain, no or a potential aversive stimulation (study 1) or reward (study 2) that could be avoided or gained respectively via a fast joystick movement towards an indicated display side. In study 1, results replicated a centralization of gaze when participants expected an avoidable shock. In study 2, we did not find this pattern. These findings indicate that freezing of gaze is robust even when a subsequent reaction requires spatial attention. Furthermore, these visual dynamics seem to be threat-specific."
  },
  {
    "title": "Impact of evoked reward expectation on ocular information during a controlled card game",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": ["Minoru Nakayama", "Kohei Shoda"],
    "DOI": "https://doi.org/10.1145/3379156.3391350",
    "citation": "1",
    "abstract": "The possibility of assessing reward expectations was examined using the environment of a poker game which was controlled by the experimenters. Subjects were asked to express their degree of expectation of obtaining a reward from their bets at two stages of each hand. In the results, pupils dilated according to the extent of a subject’s reward expectation. In particular, as mean pupil sizes for the 2nd round of betting increased with the subject’s expectation of a reward during the game, there were significant differences between the three grades of expectations. Also, pupil sizes correlated with the change in expectation between the two rounds of each hand. Other indices such as saccade frequency and microsaccade frequency also responded to the expectation of a reward. These results provide evidence that metrics of oculo-motors can be an index of the level of reward expectation in the environment of a controlled card game."
  },
  {
    "title": "Sweet Pursuit: User Acceptance and Performance of a Smooth Pursuit controlled Candy Dispensing Machine in a Public Setting",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": ["Sarah-Christin Freytag"],
    "DOI": "https://doi.org/10.1145/3379156.3391356",
    "citation": "2",
    "abstract": "A prototypical smooth pursuit controlled candy dispensing machine was set up in a public area and evaluated regarding performance data, self reported joy of use, learnability, perceived stress and perceived usefulness. 359 sets of user data were collected from visitors ranging from eight to 75 years. The results show an overall high rate of successful interactions (89.8%), indicating no correlation between height, age, gender or the use of corrective glasses or lenses and the ease and success of interaction. Incorrectly entered digits occurred for 36.2% of all participants, with half attributing the error to their own incorrect entry and half reporting the system to have detected a false number. Users reported a generally high joy of use and found the system easy to learn. Users indicated interest to use similar interaction technologies in public settings if privacy requirements such as protection from observers, are met."
  },
  {
    "title": "Using Eye Tracking to Assess the Temporal Dynamics By Which Drivers Notice Cyclists in Daylight: Drivers Becoming Aware of Cyclists",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": [
      "Darlene E. Edewaard",
      "Richard A. Tyrrell",
      "Andrew T. Duchowski",
      "Ellen C. Szubski",
      "Savana S. King"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391360",
    "citation": "2",
    "abstract": "Bicyclists face the risk of being involved in collisions with motor vehicles on roadways. This experiment used eye tracking technology to better understand how drivers notice bicyclists in daylight. Participants searched for bicyclists while wearing a head-mounted eye tracker, as they were driven along an open-road route that included a test bicyclist during the daytime. Participants pressed buttons when they detected that a bicyclist might be present in or near the roadway and when they were confident that a bicyclist was present. Through the use of eye tracking technology, this experiment provided a better understanding of the differences among the distances from which participants glance at, detect, and recognize bicyclists in daytime roadways."
  },
  {
    "title": "ManiGaze: a Dataset for Evaluating Remote Gaze Estimator in Object Manipulation Situations",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": ["Remy Siegfried", "Bozorgmehr Aminian", "Jean-Marc Odobez"],
    "DOI": "https://doi.org/10.1145/3379156.3391369",
    "citation": "3",
    "abstract": "Gaze estimation allows robots to better understand users and thus to more precisely meet their needs. In this paper, we are interested in gaze sensing for analyzing collaborative tasks and manipulation behaviors in human-robot interactions (HRI), which differs from screen gazing and other communicative HRI settings. Our goal is to study the accuracy that remote vision gaze estimators can provide, as they are a promising alternative to current accurate but intrusive wearable sensors. In this view, our contributions are: 1) we collected and make public a labeled dataset involving manipulation tasks and gazing behaviors in an HRI context; 2) we evaluate the performance of a state-of-the-art gaze estimation system on this dataset. Our results show a low default accuracy, which is improved by calibration, but that more research is needed if one wishes to distinguish gazing at one object amongst a dozen on a table."
  },
  {
    "title": "Pilot Study of Audiovisual Speech-In-Noise (SIN) Performance of Young Adults with ADHD",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": [
      "Gavindya Jayawardena",
      "Anne Michalek",
      "Andrew Duchowski",
      "Sampath Jayarathna"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391373",
    "citation": "7",
    "abstract": "Adolescents with Attention-deficit/hyperactivity disorder (ADHD) have difficulty processing speech with background noise due to reduced inhibitory control and working memory capacity (WMC). This paper presents a pilot study of an audiovisual Speech-In-Noise (SIN) task for young adults with ADHD compared to age-matched controls using eye-tracking measures. The audiovisual SIN task consists of varying six levels of background babble, accompanied by visual cues. A significant difference between ADHD and neurotypical (NT) groups was observed at 15 dB signal-to-noise ratio (SNR). These results contribute to the literature of young adults with ADHD."
  },
  {
    "title": "Faces strongly attract early fixations in naturally sampled real-world stimulus materials",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": [
      "Anna Lisa Gert",
      "Benedikt V. Ehinger",
      "Tim C. Kietzmann",
      "Peter Konig"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391377",
    "citation": "1",
    "abstract": "Faces are an important and salient stimulus in our everyday life. They convey social information and, consequently, attract our attention easily. Here, we investigate this face-attraction-bias in detail and analyze the first fixations made in a free-viewing paradigm. We presented 20 participants with natural, head-centered, live-sized stimuli of indoor scenes, taken during unconstrained free-viewing in a real-world environment. About 70% of first fixations were made on human faces, rather than human heads, non-human faces or the background. This effect was present even though human faces constituted only about 5% of the stimulus area and occurred in a wide variety of positions. With a hierarchical logistic model, we identify behavioral and stimulus’ features that explain this bias. We conclude that the face-attraction bias replicates under more natural conditions, reflects high-level properties of faces, and discuss its implications on the measurement of brain dynamics."
  },
  {
    "title": "Data-Driven Classification of Dyslexia Using Eye-Movement Correlates of Natural Reading",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 6: Applications",
    "data": "June 2020",
    "authors": ["János Szalma", "Béla Weiss"],
    "DOI": "https://doi.org/10.1145/3379156.3391379",
    "citation": "4",
    "abstract": "Developmental dyslexia is a reading disability estimated to affect between 5 to 10 percent of the population. Current screening methods are limited as they tell very little about the oculomotor processes underlying natural reading. Investigation of eye-movement correlates of reading using machine learning could enhance detection of dyslexia. Here we used eye-tracking data collected during natural reading of 48 young adults (24 dyslexic, 24 control). We established a set of 67 features containing saccade-, glissade-, fixation-related measures and the reading speed. To detect participants with dyslexic reading patterns, we used a linear support vector machine with 10-fold stratified cross-validation repeated 10 times. For feature selection we used a recursive feature elimination method, and we also considered hyperparameter optimization, both with nested and regular cross-validation. The overall best model achieved a 90.1% classification accuracy, while the best nested model achieved a 75.75% accuracy."
  },
  {
    "title": "How Shared Visual Attention Patterns of Pairs Unfold Over Time when Workload Changes",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 7: Cognition",
    "data": "June 2020",
    "authors": ["Shannon Patricia Devlin", "Jake Ryan Flynn", "Sara Lu Riggs"],
    "DOI": "https://doi.org/10.1145/3379156.3391339",
    "citation": "2",
    "abstract": "Data rich environments rely on operator collaboration to manage workload changes. This work explores the relationship between operators’ visual attention and collaborative performance during these workload changes. Percent gaze overlap and percent recurrence were calculated over time for best and worst performing pairs of participants who experienced low and high workload in an unmanned aerial vehicle command and control testbed. It was found that the best performing pairs had higher values for both metrics after workload changed. These results suggest successful collaborative performance is dependent on both continuous high levels of synchronized visual attention and coordinated sequences of visual attention. This work has the potential to inform the design of real-time technology."
  },
  {
    "title": "One threshold to rule them all? Modification of the Index of Pupillary Activity to optimize the indication of cognitive load",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 7: Cognition",
    "data": "June 2020",
    "authors": ["Benedict C. O. F. Fehringer"],
    "DOI": "https://doi.org/10.1145/3379156.3391341",
    "citation": "0",
    "abstract": "Cognitive load is an important source of information in performance situations. One promising non-invasive method is pupillometry. The Index of Pupillary Activity [IPA, Duchowski et al. 2018] performs a wavelet transformation on changes of pupillary dilations to detect high frequencies. This index is inspired by the Index of Cognitive Activity [ICA, Marshall 2000]. The IPA value is the sum of peaks exceeding a predefined threshold. The present study shows that it appears reasonable to adapt this threshold corresponding to the task. Fifty-five participants performed a spatial thinking test with six difficulty levels and two simple fixation tasks. Six different IPA values resulting from different thresholds were computed. The distributions of these IPA values of the eight conditions were analyzed regarding the validity to indicate different levels of cognitive load, corresponding to accuracy data. The analyses revealed that different thresholds are sensitive for different cognitive load levels. Contra-intuitive results were also obtained."
  },
  {
    "title": "Insights into the processes underlying the early fixation-based memory effect",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 7: Cognition",
    "data": "June 2020",
    "authors": ["Charlotte Schwedes", "Oliver C. Raufeisen", "Dirk Wentura"],
    "DOI": "https://doi.org/10.1145/3379156.3391359",
    "citation": "0",
    "abstract": "Previous research has shown that already durations of second fixations reveal concealed knowledge of an object. This very early memory effect could potentially be useful in applied settings. However, in order to use this effect, it is necessary to understand the processes causing the early fixation-based memory effect and the context which is necessary to obtain the effect. In four experiments, we disentangled the contribution of a probability-sensitive orienting response (OR) from probability-insensitive recognition memory processes. The results showed that the early fixation-based memory effect only appeared if both processes were involved. Moreover, the feature that triggers the OR has to be task relevant."
  },
  {
    "title": "Pedestrians Egocentric Vision: Individual and Collective Analysis",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 7: Cognition",
    "data": "June 2020",
    "authors": [
      "Matteo Valsecchi",
      "Arash Akbarinia",
      "Raquel Gil-Rodriguez",
      "Karl R. Gegenfurtner"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391378",
    "citation": "2",
    "abstract": "Egocentric videos offer an ecological approach to study human gaze behaviour. We were interested in understanding what people look at while performing the natural task of navigating in urban environments. Is there a collective pattern among all participants or are there substantial individual differences? To this end, we recorded egocentric video and gaze data from forty-three pedestrians. Here, we present this dataset designed to benchmark future research. The content of these videos was examined with respect to the depth and category of attended objects. We observe noticeable individual differences in both factors. Following these criteria, individual gaze patterns form a number of clusters. The unique signature of each set remains to be explored, whether it is based on low-level visual features or high-level cognitive characteristics."
  },
  {
    "title": "Quantitative Perception Measurement of the Rotating Snakes Illusion Considering Temporal Dependence and Gaze Information",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 7: Cognition",
    "data": "June 2020",
    "authors": ["Yuki Kubota", "Tomohiko Hayakawa", "Masatoshi Ishikawa"],
    "DOI": "https://doi.org/10.1145/3379156.3391344",
    "citation": "0",
    "abstract": "Revealed discrepancies between a physical and perceptual image, called optical illusions, have been extensively researched to elucidate perceptual and cognitive functions in psychology and information engineering. While some optical illusions, including the Rotating Snakes Illusion (RSI), depend on gaze information of observers, eye movement information has not been actively used to measure this illusion. This study developed a method to quantitatively measure a spatial-dependent and temporal-dependent illusion using a dynamic perceptive compensation system incorporating an eye tracking device. A subject experiment comparing an illusory and a controlled image showed that the results for the illusory image only depended on the compensation time, while the results for the controlled image did not. Consequently, our method could measure the quantitative temporal dependence of RSI appropriately. Furthermore, the results suggest that the compensation algorithm need only be considered within 500 ms when controlling the illusion."
  },
  {
    "title": "A Visualization Tool for Eye Tracking Data Analysis in the Web",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Analysis Techniques",
    "data": "June 2020",
    "authors": [
      "Raphael Menges",
      "Sophia Kramer",
      "Stefan Hill",
      "Marius Nisslmueller",
      "Chandan Kumar",
      "Steffen Staab"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391831",
    "citation": "8",
    "abstract": "Usability analysis plays a significant role in optimizing Web interaction by understanding the behavior of end users. To support such analysis, we present a tool to visualize gaze and mouse data of Web site interactions. The proposed tool provides not only the traditional visualizations with fixations, scanpath, and heatmap, but allows for more detailed analysis with data clustering, demographic correlation, and advanced visualization like attention flow and 3D-scanpath. To demonstrate the usefulness of the proposed tool, we conducted a remote qualitative study with six analysts, using a dataset of 20 users browsing eleven real-world Web sites."
  },
  {
    "title": "Eye Tracking for Target Acquisition in Sparse Visualizations",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Analysis Techniques",
    "data": "June 2020",
    "authors": ["Feiyang Wang", "Adam James Bradley", "Christopher Collins"],
    "DOI": "https://doi.org/10.1145/3379156.3391834",
    "citation": "0",
    "abstract": "In this paper, we present a novel marker-free method for identifying screens of interest when using head-mounted eye tracking for visualization in cluttered and multi-screen environments. We offer a solution to discerning visualization entities from sparse backgrounds by incorporating edge-detection into the existing pipeline. Our system allows for both more efficient screen identification and improved accuracy over the state-of-the-art ORB algorithm."
  },
  {
    "title": "Sankeye: A Visualization Technique for AOI Transitions",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Analysis Techniques",
    "data": "June 2020",
    "authors": ["Michael Burch", "Neil Timmermans"],
    "DOI": "https://doi.org/10.1145/3379156.3391833",
    "citation": "4",
    "abstract": "Visually exploring AOI transitions aggregated from a group of eye tracked people is a challenging task. Many visualizations typically produce visual clutter or aggregate the temporal or visit order information in the data hiding the visual task solution strategies for the observer. In this paper we introduce the Sankeye technique that is based on the visual metaphor of Sankey diagrams applied to eye movement data, hence the name Sankeye. The technique encodes the frequencies of AOI transitions into differently thick rivers and subrivers. The distributions of the AOI transitions are visually represented by splitting and merging subrivers in a left-to-right reading direction. The technique allows to interactively adapt the number of predefined AOIs as well as the transition frequency number threshold with the goal to derive patterns and insights from eye movement data."
  },
  {
    "title": "Evaluation of Gaze Depth Estimation from Eye Tracking in Augmented Reality",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: Evaluation",
    "data": "June 2020",
    "authors": [
      "Seyda Oney",
      "Nils Rodrigues",
      "Michael Becher",
      "Thomas Ertl",
      "Guido Reina",
      "Michael Sedlmair",
      "Daniel Weiskopf"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391835",
    "citation": "5",
    "abstract": "Gaze tracking in 3D has the potential to improve interaction with objects and visualizations in augmented reality. However, previous research showed that subjective perception of distance varies between real and virtual surroundings. We wanted to determine whether objectively measured 3D gaze depth through eye tracking also exhibits differences between entirely real and augmented environments. To this end, we conducted an experiment (N = 25) in which we used Microsoft HoloLens with a binocular eye tracking add-on from Pupil Labs. Participants performed a task that required them to look at stationary real and virtual objects while wearing a HoloLens device. We were not able to find significant differences in the gaze depth measured by eye tracking. Finally, we discuss our findings and their implications for gaze interaction in immersive analytics, and the quality of the collected gaze data."
  },
  {
    "title": "Eye vs. Head: Comparing Gaze Methods for Interaction in Augmented Reality",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: Evaluation",
    "data": "June 2020",
    "authors": [
      "Nelusa Pathmanathan",
      "Michael Becher",
      "Nils Rodrigues",
      "Guido Reina",
      "Thomas Ertl",
      "Daniel Weiskopf",
      "Michael Sedlmair"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391829",
    "citation": "6",
    "abstract": "Visualization in virtual 3D environments can provide a natural way for users to explore data. Often, arm and short head movements are required for interaction in augmented reality, which can be tiring and strenuous though. In an effort toward more user-friendly interaction, we developed a prototype that allows users to manipulate virtual objects using a combination of eye gaze and an external clicker device. Using this prototype, we performed a user study comparing four different input methods of which head gaze plus clicker was preferred by most participants."
  },
  {
    "title": "Comparing Input Modalities for Shape Drawing Tasks",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: Evaluation",
    "data": "June 2020",
    "authors": [
      "Annalena Streichert",
      "Katrin Angerbauer",
      "Magdalena Schwarzl",
      "Michael Sedlmair"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391830",
    "citation": "1",
    "abstract": "With the growing interest in Immersive Analytics, there is also a need for novel and suitable input modalities for such applications. We explore eye tracking, head tracking, hand motion tracking, and data gloves as input methods for a 2D tracing task and compare them to touch input as a baseline in an exploratory user study (N=20). We compare these methods in terms of user experience, workload, accuracy, and time required for input. The results show that the input method has a significant influence on these measured variables. While touch input surpasses all other input methods in terms of user experience, workload, and accuracy, eye tracking shows promise in respect of the input time. The results form a starting point for future research investigating input methods."
  },
  {
    "title": "Introducing Eye Movement Modeling Examples for Programming Education and the Role of Teacher's Didactic Guidance",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1",
    "data": "June 2020",
    "authors": [
      "Selina Emhardt",
      "Halszka Jarodzka",
      "Saskia Brand-Gruwel",
      "Christian Drumm",
      "Tamara van Gog"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391978",
    "citation": "3",
    "abstract": "In this article, we introduce how eye-tracking technology might become a promising tool to teach programming skills, such as debugging with ‘Eye Movement Modeling Examples’ (EMME). EMME are tutorial videos that visualize an expert's (e.g., a programming teacher's) eye movements during task performance to guide students’ attention, e.g., as a moving dot or circle. We first introduce the general idea behind the EMME method and present studies that showed first promising results regarding the benefits of EMME to support programming education. However, we argue that the instructional design of EMME varies notably across them, as evidence-based guidelines on how to create effective EMME are often lacking. As an example, we present our ongoing research on the effects of different ways to instruct the EMME model prior to video creation. Finally, we highlight open questions for future investigations that could help improving the design of EMME for (programming) education."
  },
  {
    "title": "When you ignore what you see: How to study proof-readers’ error in pseudocode reading",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1",
    "data": "June 2020",
    "authors": [
      "Natalia Chitalkina",
      "Roman Bednarik",
      "Marjaana Puurtinen",
      "Hans Gruber"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391979",
    "citation": "1",
    "abstract": "When reading algorithms, expert programmers are often able to predict what the code should contain. On occasion, however, this ability may result in so-called proof-readers’ errors, where the visual input is ignored and programmers process the code based on their own predictions. The goal of this study is to gain initial understanding of how proof-readers’ errors are reflected in eye-movement parameters of an experienced programmer, and to search for parameters that may be indicative for proof-readers’ errors in pseudocode reading. We applied a case-study approach to test the hypothesis that cognitive processing of notation, when read both with and without proof-readers’ errors, results in similarities in terms of selected eye-movement measures. However, our experienced programmer turned out to become a critical case falsifying this hypothesis. In general, case studies with expert programmers are a rather novel approach for eye-tracking studies of programming, even though single cases of experts’ eye movements are actively applied for the development of eye movement modelling examples. This study therefore also points to the importance of regarding expert examples not just as representatives as ”expert reading” in general, but also as unique cases worth a closer investigation."
  },
  {
    "title": "Code Reviews in C++: Preliminary Results from an Eye Tracking Study",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1",
    "data": "June 2020",
    "authors": [
      "Florian Hauser",
      "Stefan Schreistter",
      "Rebecca Reuter",
      "Jurgen Horst Mottok",
      "Hans Gruber",
      "Kenneth Holmqvist",
      "Nick Schorr"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391980",
    "citation": "6",
    "abstract": "Code reviews are an essential part of quality assurance in modern software projects. But despite their great importance, they are still carried out in a way that relies on human skills and decisions. During the last decade, there have been several publications on code reviews using eye tracking as a method, but only a few studies have focused on the performance differences between experts and novices. To get a deeper understanding of these differences, the following experiment was developed: This study surveys expertise-related differences in experts’, advanced programmers’, and novices’ eye movements during the review of eight short C++ code examples, including correct and erroneous codes. A sample of 35 participants (21 novices, 14 advanced and expert programmers) were recruited. A Tobii Spectrum 600 was used for the data collection. Measures included participants’ eye movements during the code review, demographic background data, and cued retrospective verbal comments on replays of their own eye movement recordings. Preliminary results give proof for experience-related differences between participants. Advanced and expert programmers performed significantly better in case of error detection and the eye tracking data implies a more efficient reviewing strategy."
  },
  {
    "title": "Eye Movement Features in response to Comprehension Performance during the Reading of Programs",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1",
    "data": "June 2020",
    "authors": ["Minoru Nakayama", "Hiroto Harada"],
    "DOI": "https://doi.org/10.1145/3379156.3391981",
    "citation": "3",
    "abstract": "Some features of eye movement during the reading of program code were analysed in order to develop a procedure to assess viewer comprehension ability. A set of eye movement data which was created by the measurement of eye movement during the viewing of a code programming project was used. While backward eye movement is natural under normal reading circumstances, this paper focuses on intentional eye movement in the opposite direction of the usual pattern of reading while viewing blocks of code, and the impact of the frequency of this on the comprehension of a code, which was confirmed. In examining the frequency of this reading behaviour, there were significant differences in both overall fixation times and mean saccade lengths for the two levels of comprehension of the code."
  },
  {
    "title": "A Fine-grained Assessment on Novice Programmers’ Gaze Patterns on Pseudocode Problems",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1",
    "data": "June 2020",
    "authors": [
      "Unaizah Obaidellah",
      "Tanja Blascheck",
      "Drew T. Guarnera",
      "Jonathan Maletic"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391982",
    "citation": "3",
    "abstract": "To better understand code comprehension and problem solving strategies, we conducted an eye tracking study that includes 51 undergraduate computer science students solving six pseudocode program comprehension tasks. Each task required students to order a sequence of pseudocode statements necessary to correctly solve a programming problem. We compare the viewing patterns of computer science students to evaluate changes in behavior while participants solve problems of varying difficulty. The intent is to find out if gaze patterns are similar prior to solving the task and if this pattern changes as the problems get more difficult. The findings show that as the difficulty increases regressions between areas of interest also tend to increase. Furthermore, an analysis of clusters of participants’ common viewing patterns was performed to identify groups of participants’ sharing similar gaze patterns prior to selecting their first choice of answer. Future work suggests an investigation on the relationship of these patterns with other background information (such as gender, age, English language proficiency, course completion) as well as performance (score, duration of task completion, competency level)."
  },
  {
    "title": "Can the E-Z Reader Model Predict Eye Movements Over Code? Towards a Model of Eye Movements Over Source Code",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1",
    "data": "June 2020",
    "authors": [
      "Naser Al Madi",
      "Cole S. Peterson",
      "Bonita Sharif",
      "Jonathan Maletic"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391983",
    "citation": "2",
    "abstract": "Studies of eye movements during source code reading have supported the idea that reading source code differs fundamentally from reading natural text. The paper analyzed an existing data set of natural language and source code eye movement data using the E-Z reader model of eye movement control. The results show that the E-Z reader model can be used with natural text and with source code where it provides good predictions of eye movement duration. This result is confirmed by comparing model predictions to eye-movement data from this experiment and calculating the correlation score for each metric. Finally, it was found that gaze duration is influenced by token frequency in code and in natural text. The frequency effect is less pronounced on first fixation duration and single fixation duration. An eye movement control model for source code reading may open the door for tools in education and the industry to enhance program comprehension."
  },
  {
    "title": "Eye-Tracking in Educational Multi-Touch Games: Design-Based (interaction) research and great visions",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Considerations for Application of Eye-Tracking in Games",
    "data": "June 2020",
    "authors": [
      "Birte Heinemann",
      "Matthias Ehlenz",
      "Prof. Dr. Ulrik Schroeder"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391838",
    "citation": "3",
    "abstract": "Collaborative learning with educational games on multi-touch tabletop devices opens chances, challenges and questions which make contemporary approaches in learning analytics meet their boundaries. Multi-modality might help here, and eye-tracking is a promising data source in the effort to a better understanding of the learners’ behaviour. This article describes our previous work regarding serious games on large multi-touch tabletop displays, developed to teach computer science theory topics in a collaborative and entertaining way, our previous research efforts and challenges and obstacles we met on our way. Eye-tracking will improve our understanding of the learners’ behaviour while they are not interacting with the game, enhance the construction of coherent learner models and might even provide a subtle way of control on a medium of public interaction. The benefits of our work can be used to enhance the game mechanics and support shy or students with disabilities. We present our plan of action which follows a design-based research approach and includes the motivation for our work and our short- and long-term goals."
  },
  {
    "title": "Visual Analysis of Eye Movements During Game Play",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Considerations for Application of Eye-Tracking in Games",
    "data": "June 2020",
    "authors": ["Michael Burch", "Kuno Kurzhals"],
    "DOI": "https://doi.org/10.1145/3379156.3391839",
    "citation": "1",
    "abstract": "Eye movements indicate visual attention and strategies during game play, regardless of whether in board, sports, or computer games. Additional factors such as individual vs. group play and active playing vs. observing game play further differentiate application scenarios for eye movement analysis. Visual analysis has proven to be an effective means to investigate and interpret such highly dynamic spatio-temporal data. In this paper, we contribute a classification strategy for different scenarios for the visual analysis of gaze data during game play. Based on an initial sample of related work, we derive multiple aspects comprising data sources, game mode, player number, player state, analysis mode, and analysis goal. We apply this classification strategy to describe typical analysis scenarios and research questions as they can be found in related work. We further discuss open challenges and research directions for new application scenarios of eye movements in game play."
  },
  {
    "title": "Eye-tracking for Sense of Immersion and Linguistic Complexity in the Skyrim Game: Issues and Perspectives",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 1: Considerations for Application of Eye-Tracking in Games",
    "data": "June 2020",
    "authors": [
      "Alessandro Cierro",
      "Thibault Philippette",
      "Thomas Francois",
      "Sebastien Nahon",
      "Patrick Watrin"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391836",
    "citation": "1",
    "abstract": "As part of an experimental study aimed at evaluating the linguistic and paralinguistic factors that can influence the sense of immersion in an open-world video game, we have partially opted for an eye-tracking data collection protocol. In doing so, various problems emerged in the course of the research and we therefore propose to report and analyze them in this article in order to provide useful feedback for further research. The first set of problems is of a technical nature and relates to the difficulty of collecting reliable eye tracking data in an open and complex game environment. Our second concern is about the difficulties that may appear depending on the morphological characteristics of the players. The third issue is about player’s familiarity with the game and the experimental parameters. And lastly, we discuss some post-processing issues for the analysis. The reflections raised from these few difficulties allow us to discuss some challenges for future oculometric research in complex video game environments."
  },
  {
    "title": "Using the Uncalibrated Eye Tracker Signal in the Gaming Environment",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: Gaze-Based Studies",
    "data": "June 2020",
    "authors": ["Pawel Kasprowski", "Katarzyna Harezlak"],
    "DOI": "https://doi.org/10.1145/3379156.3391837",
    "citation": "0",
    "abstract": "It seems that controlling games with the eyes should be very intuitive and obvious. However, eye-controlled games have not become very popular yet. One of the reasons is – in our opinion – the necessity of eye tracker calibration before its every usage. This process is not very long, but it is inconvenient and requires focusing on the particular task. Moreover, sometimes the calibration fails and must be repeated."
  },
  {
    "title": "Gazing at Pac-Man: Lessons learned from a Eye-Tracking Study Focusing on Game Difficulty",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: Gaze-Based Studies",
    "data": "June 2020",
    "authors": ["Michael Lankes", "Andreas Stoeckl"],
    "DOI": "https://doi.org/10.1145/3379156.3391840",
    "citation": "2",
    "abstract": "This paper investigates the players’ gaze behavior in different game difficulty settings to explore potential use cases of gaze-informed design interventions for future research activities. A comparative study was set up where subjects played the game Pac-Man in three difficulty settings while their gaze behavior was recorded via an eye-tracking device. Several measures were employed, such as the current position of the players’ gaze, the current position of the Pac-Man character, and the currently attended game object. While some game aspects did not show any significant results (e.g., the distance between Pac-Man and the gaze point), the time spent looking at one of Pac-Man’s enemies revealed a highly significant effect for the difficulty level. With the findings, we aim at informing designers and researchers regarding the pitfalls of using gaze as an analysis tool in the field of challenge in games. Furthermore, the insights of our efforts provide the basis for our future research activities that will use the obtained data to provide player guidance and support players in challenging game situations through visually augmenting objects located in the peripheral visual field."
  },
  {
    "title": "Eye Caramba: Gaze-based Assistance for Virtual Reality Aiming and Throwing Tasks in Games",
    "conferenceTitle": "ETRA '20 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: SESSION 2: Gaze-Based Studies",
    "data": "June 2020",
    "authors": [
      "Martin Kocur",
      "Martin Johannes Dechant",
      "Michael Lankes",
      "Christian Wolff",
      "Regan Mandryk"
    ],
    "DOI": "https://doi.org/10.1145/3379156.3391841",
    "citation": "4",
    "abstract": "Gaze-based interaction in Virtual Reality (VR) has been attracting attention recently due to rapid advances in eye tracking technology in head-mounted displays. Since gazes are a natural and intuitive interaction modality for human beings, gaze-based interaction could enhance player experience in immersive VR games. Aiming assistance is a common feature in games to balance difficulty for different player skills. Previous work has investigated different aim assistance approaches and identified various shortcomings. We hypothesize that “bullet magnetism” is a promising technique for VR and could be enhanced by extending its functionality through players’ gazes. In this paper, we present a gaze-based aiming assistance approach and propose a study design to evaluate its performance and player experience in a “Mexican-style” VR first-person shooter game."
  }
]
