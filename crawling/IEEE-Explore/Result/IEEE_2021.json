[
    {
        "title": "Design and Optimization of Conforming Lattice Structures",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Jun Wu",
            "Weiming Wang",
            "Xifeng Gao"
        ],
        "DOI": "10.1109/TVCG.2019.2938946",
        "citation": 90,
        "abstract": "Inspired by natural cellular materials such as trabecular bone, lattice structures have been developed as a new type of lightweight material. In this paper we present a novel method to design lattice structures that conform with both the principal stress directions and the boundary of the optimized shape. Our method consists of two major steps: the first optimizes concurrently the shape (including its topology) and the distribution of orthotropic lattice materials inside the shape to maximize stiffness under application-specific external loads; the second takes the optimized configuration (i.e., locally-defined orientation, porosity, and anisotropy) of lattice materials from the previous step, and extracts a globally consistent lattice structure by field-aligned parameterization. Our approach is robust and works for both 2D planar and 3D volumetric domains. Numerical results and physical verifications demonstrate remarkable structural properties of conforming lattice structures generated by our method."
    },
    {
        "title": "ImaCytE: Visual Exploration of Cellular Micro-Environments for Imaging Mass Cytometry Data",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Antonios Somarakis",
            "Vincent Van Unen",
            "Frits Koning",
            "Boudewijn Lelieveldt",
            "Thomas Höllt"
        ],
        "DOI": "10.1109/TVCG.2019.2931299",
        "citation": 51,
        "abstract": "Tissue functionality is determined by the characteristics of tissue-resident cells and their interactions within their microenvironment. Imaging Mass Cytometry offers the opportunity to distinguish cell types with high precision and link them to their spatial location in intact tissues at sub-cellular resolution. This technology produces large amounts of spatially-resolved high-dimensional data, which constitutes a serious challenge for the data analysis. We present an interactive visual analysis workflow for the end-to-end analysis of Imaging Mass Cytometry data that was developed in close collaboration with domain expert partners. We implemented the presented workflow in an interactive visual analysis tool; ImaCytE. Our workflow is designed to allow the user to discriminate cell types according to their protein expression profiles and analyze their cellular microenvironments, aiding in the formulation or verification of hypotheses on tissue architecture and function. Finally, we show the effectiveness of our workflow and ImaCytE through a case study performed by a collaborating specialist."
    },
    {
        "title": "Locomotion in Place in Virtual Reality: A Comparative Evaluation of Joystick, Teleport, and Leaning",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Fabio Buttussi",
            "Luca Chittaro"
        ],
        "DOI": "10.1109/TVCG.2019.2928304",
        "citation": 49,
        "abstract": "Recent VR head-mounted displays for consumers feature 3-DOF or 6-DOF head tracking. However, position tracking (when available) is limited to a small area. Moreover, in small or cluttered physical spaces, users can safely experience VR only by staying in place, standing or seated. Different locomotion techniques have been proposed to allow users to explore virtual environments by staying in place. Two in-place locomotion techniques, frequently employed in the literature and in consumer applications, are based on joystick and teleport. Some authors explored leaning with the aim of proposing a more natural in-place locomotion technique. However, more research is needed to understand the effects of the three techniques, since no user study thoroughly compared them all together on a variety of fundamental aspects. Therefore, this paper presents a comparative evaluation with 75 users, assessing the effects of the three techniques on performance, sickness, presence, usability, and different aspects of comfort. Performance of teleport was better than the other techniques, and performance of leaning was better than joystick. Teleport also caused less nausea than the other techniques. Unexpectedly, no significant differences were found for presence. Teleport received a higher usability score than the other techniques. Finally, the techniques had different effects on comfort that we discuss in detail."
    },
    {
        "title": "NaviBoard and NaviChair: Limited Translation Combined with Full Rotation for Efficient Virtual Locomotion",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Thinh Nguyen-Vo",
            "Bernhard E. Riecke",
            "Wolfgang Stuerzlinger",
            "Duc-Minh Pham",
            "Ernst Kruijff"
        ],
        "DOI": "10.1109/TVCG.2019.2935730",
        "citation": 39,
        "abstract": "Walking has always been considered as the gold standard for navigation in Virtual Reality research. Though full rotation is no longer a technical challenge, physical translation is still restricted through limited tracked areas. While rotational information has been shown to be important, the benefit of the translational component is still unclear with mixed results in previous work. To address this gap, we conducted a mixed-method experiment to compare four levels of translational cues and control: none (using the trackpad of the HTC Vive controller to translate), upper-body leaning (sitting on a “NaviChair”, leaning the upper-body to locomote), whole-body leaning/stepping (standing on a platform called NaviBoard, leaning the whole body or stepping one foot off the center to navigate), and full translation (physically walking). Results showed that translational cues and control had significant effects on various measures including task performance, task load, and simulator sickness. While participants performed significantly worse when they used a controller with no embodied translational cues, there was no significant difference between the NaviChair, NaviBoard, and actual walking. These results suggest that translational body-based motion cues and control from a low-cost leaning/stepping interface might provide enough sensory information for supporting spatial updating, spatial awareness, and efficient locomotion in VR, although future work will need to investigate how these results might or might not generalize to other tasks and scenarios."
    },
    {
        "title": "Combining Recurrent Neural Networks and Adversarial Training for Human Motion Synthesis and Control",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Zhiyong Wang",
            "Jinxiang Chai",
            "Shihong Xia"
        ],
        "DOI": "10.1109/TVCG.2019.2938520",
        "citation": 37,
        "abstract": "This paper introduces a new generative deep learning network for human motion synthesis and control. Our key idea is to combine recurrent neural networks (RNNs) and adversarial training for human motion modeling. We first describe an efficient method for training an RNN model from prerecorded motion data. We implement RNNs with long short-term memory (LSTM) cells because they are capable of addressing the nonlinear dynamics and long term temporal dependencies present in human motions. Next, we train a refiner network using an adversarial loss, similar to generative adversarial networks (GANs), such that refined motion sequences are indistinguishable from real mocap data using a discriminative network. The resulting model is appealing for motion synthesis and control because it is compact, contact-aware, and can generate an infinite number of naturally looking motions with infinite lengths. Our experiments show that motions generated by our deep learning model are always highly realistic and comparable to high-quality motion capture data. We demonstrate the power and effectiveness of our models by exploring a variety of applications, ranging from random motion synthesis, online/offline motion control, and motion filtering. We show the superiority of our generative model by comparison against baseline models."
    },
    {
        "title": "Spatio-Temporal Manifold Learning for Human Motions via Long-Horizon Modeling",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "He Wang",
            "Edmond S. L. Ho",
            "Hubert P. H. Shum",
            "Zhanxing Zhu"
        ],
        "DOI": "10.1109/TVCG.2019.2936810",
        "citation": 36,
        "abstract": "Data-driven modeling of human motions is ubiquitous in computer graphics and computer vision applications, such as synthesizing realistic motions or recognizing actions. Recent research has shown that such problems can be approached by learning a natural motion manifold using deep learning on a large amount data, to address the shortcomings of traditional data-driven approaches. However, previous deep learning methods can be sub-optimal for two reasons. First, the skeletal information has not been fully utilized for feature extraction. Unlike images, it is difficult to define spatial proximity in skeletal motions in the way that deep networks can be applied for feature extraction. Second, motion is time-series data with strong multi-modal temporal correlations between frames. On the one hand, a frame could be followed by several candidate frames leading to different motions; on the other hand, long-range dependencies exist where a number of frames in the beginning are correlated with a number of frames later. Ineffective temporal modeling would either under-estimate the multi-modality and variance, resulting in featureless mean motion or over-estimate them resulting in jittery motions, which is a major source of visual artifacts. In this paper, we propose a new deep network to tackle these challenges by creating a natural motion manifold that is versatile for many applications. The network has a new spatial component for feature extraction. It is also equipped with a new batch prediction model that predicts a large number of frames at once, such that long-term temporally-based objective functions can be employed to correctly learn the motion multi-modality and variances. With our system, long-duration motions can be predicted/synthesized using an open-loop setup where the motion retains the dynamics accurately. It can also be used for denoising corrupted motions and synthesizing new motions with given control signals. We demonstrate that our system can create superior results comparing to existing work in multiple applications."
    },
    {
        "title": "Analyzing Dynamic Hypergraphs with Parallel Aggregated Ordered Hypergraph Visualization",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Paola Valdivia",
            "Paolo Buono",
            "Catherine Plaisant",
            "Nicole Dufournaud",
            "Jean-Daniel Fekete"
        ],
        "DOI": "10.1109/TVCG.2019.2933196",
        "citation": 33,
        "abstract": "Parallel Aggregated Ordered Hypergraph(PAOH) is a novel technique to visualize dynamic hypergraphs. Hypergraphs are a generalization of graphs where edges can connect several vertices. Hypergraphs can be used to model networks of business partners or co-authorship networks with multiple authors per article. A dynamic hypergraph evolves over discrete time slots. PAOH represents vertices as parallel horizontal bars and hyperedges as vertical lines, using dots to depict the connections to one or more vertices. We describe a prototype implementation of Parallel Aggregated Ordered Hypergraph, report on a usability study with 9 participants analyzing publication data, and summarize the improvements made. Two case studies and several examples are provided. We believe that PAOH is the first technique to provide a highly readable representation of dynamic hypergraphs. It is easy to learn and well suited for medium size dynamic hypergraphs (50-500 vertices) such as those commonly generated by digital humanities projects-our driving application domain."
    },
    {
        "title": "Perceptual-Aware Sketch Simplification Based on Integrated VGG Layers",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Xuemiao Xu",
            "Minshan Xie",
            "Peiqi Miao",
            "Wei Qu",
            "Wenpeng Xiao",
            "Huaidong Zhang",
            "Xueting Liu",
            "Tien-Tsin Wong"
        ],
        "DOI": "10.1109/TVCG.2019.2930512",
        "citation": 26,
        "abstract": "Deep learning has been recently demonstrated as an effective tool for raster-based sketch simplification. Nevertheless, it remains challenging to simplify extremely rough sketches. We found that a simplification network trained with a simple loss, such as pixel loss or discriminator loss, may fail to retain the semantically meaningful details when simplifying a very sketchy and complicated drawing. In this paper, we show that, with a well-designed multi-layer perceptual loss, we are able to obtain aesthetic and neat simplification results preserving semantically important global structures as well as fine details without blurriness and excessive emphasis on local structures. To do so, we design a multi-layer discriminator by fusing all VGG feature layers to differentiate sketches and clean lines. The weights used in layer fusing are automatically learned via an intelligent adjustment mechanism. Furthermore, to evaluate our method, we compare our method to state-of-the-art methods through multiple experiments, including visual comparison and intensive user study."
    },
    {
        "title": "FlyFusion: Realtime Dynamic Scene Reconstruction Using a Flying Depth Camera",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Lan Xu",
            "Wei Cheng",
            "Kaiwen Guo",
            "Lei Han",
            "Yebin Liu",
            "Lu Fang"
        ],
        "DOI": "10.1109/TVCG.2019.2930691",
        "citation": 21,
        "abstract": "While dynamic scene reconstruction has made revolutionary progress from the earliest setup using a mass of static cameras in studio environment to the latest egocentric or hand-held moving camera based schemes, it is still restricted by the recording volume, user comfortability, human labor and expertise. In this paper, a novel solution is proposed through a real-time and robust dynamic fusion scheme using a single flying depth camera, denoted as FlyFusion. By proposing a novel topology compactness strategy for effectively regularizing the complex topology changes, and the Geometry And Motion Energy (GAME) metric for guiding the viewpoint optimization in the volumetric space, FlyFusion succeeds to enable intelligent viewpoint selection based on the immediate dynamic reconstruction result. The merit of FlyFusion lies in its concurrent robustness, efficiency, and adaptation in producing fused and denoised 3D geometry and motions of a moving target interacting with different non-rigid objects in a large space."
    },
    {
        "title": "Realtime and Accurate 3D Eye Gaze Capture with DCNN-Based Iris and Pupil Segmentation",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Zhiyong Wang",
            "Jinxiang Chai",
            "Shihong Xia"
        ],
        "DOI": "10.1109/TVCG.2019.2938165",
        "citation": 20,
        "abstract": "This paper presents a realtime and accurate method for 3D eye gaze tracking with a monocular RGB camera. Our key idea is to train a deep convolutional neural network(DCNN) that automatically extracts the iris and pupil pixels of each eye from input images. To achieve this goal, we combine the power of Unet [1] and Squeezenet [2] to train an efficient convolutional neural network for pixel classification. In addition, we track the 3D eye gaze state in the Maximum A Posteriori (MAP) framework, which sequentially searches for the most likely state of the 3D eye gaze at each frame. When eye blinking occurs, the eye gaze tracker can obtain an inaccurate result. We further extend the convolutional neural network for eye close detection in order to improve the robustness and accuracy of the eye gaze tracker. Our system runs in realtime on desktop PCs and smart phones. We have evaluated our system on live videos and Internet videos, and our results demonstrate that the system is robust and accurate for various genders, races, lighting conditions, poses, shapes and facial expressions. A comparison against Wang et al. [3] shows that our method advances the state of the art in 3D eye tracking using a single RGB camera."
    },
    {
        "title": "High-Quality Textured 3D Shape Reconstruction with Cascaded Fully Convolutional Networks",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Zheng-Ning Liu",
            "Yan-Pei Cao",
            "Zheng-Fei Kuang",
            "Leif Kobbelt",
            "Shi-Min Hu"
        ],
        "DOI": "10.1109/TVCG.2019.2937300",
        "citation": 14,
        "abstract": "We present a learning-based approach to reconstructing high-resolution three-dimensional (3D) shapes with detailed geometry and high-fidelity textures. Albeit extensively studied, algorithms for 3D reconstruction from multi-view depth-and-color (RGB-D) scans are still prone to measurement noise and occlusions; limited scanning or capturing angles also often lead to incomplete reconstructions. Propelled by recent advances in 3D deep learning techniques, in this paper, we introduce a novel computation- and memory-efficient cascaded 3D convolutional network architecture, which learns to reconstruct implicit surface representations as well as the corresponding color information from noisy and imperfect RGB-D maps. The proposed 3D neural network performs reconstruction in a progressive and coarse-to-fine manner, achieving unprecedented output resolution and fidelity. Meanwhile, an algorithm for end-to-end training of the proposed cascaded structure is developed. We further introduce Human10, a newly created dataset containing both detailed and textured full-body reconstructions as well as corresponding raw RGB-D scans of 10 subjects. Qualitative and quantitative experimental results on both synthetic and real-world datasets demonstrate that the presented approach outperforms existing state-of-the-art work regarding visual quality and accuracy of reconstructed models."
    },
    {
        "title": "Mesh Saliency via Weakly Supervised Classification-for-Saliency CNN",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Ran Song",
            "Yonghuai Liu",
            "Paul L. Rosin"
        ],
        "DOI": "10.1109/TVCG.2019.2928794",
        "citation": 14,
        "abstract": "Recently, effort has been made to apply deep learning to the detection of mesh saliency. However, one major barrier is to collect a large amount of vertex-level annotation as saliency ground truth for training the neural networks. Quite a few pilot studies showed that this task is difficult. In this work, we solve this problem by developing a novel network trained in a weakly supervised manner. The training is end-to-end and does not require any saliency ground truth but only the class membership of meshes. Our Classification-for-Saliency CNN (CfS-CNN) employs a multi-view setup and contains a newly designed two-channel structure which integrates view-based features of both classification and saliency. It essentially transfers knowledge from 3D object classification to mesh saliency. Our approach significantly outperforms the existing state-of-the-art methods according to extensive experimental results. Also, the CfS-CNN can be directly used for scene saliency. We showcase two novel applications based on scene saliency to demonstrate its utility."
    },
    {
        "title": "Interactive Architectural Design with Diverse Solution Exploration",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Glen Berseth",
            "Brandon Haworth",
            "Muhammad Usman",
            "Davide Schaumann",
            "Mahyar Khayatkhoei",
            "Mubbasir Kapadia",
            "Petros Faloutsos"
        ],
        "DOI": "10.1109/TVCG.2019.2938961",
        "citation": 13,
        "abstract": "In architectural design, architects explore a vast amount of design options to maximize various performance criteria, while adhering to specific constraints. In an effort to assist architects in such a complex endeavour, we propose IDOME, an interactive system for computer-aided design optimization. Our approach balances automation and control by efficiently exploring, analyzing, and filtering space layouts to inform architects' decision-making better. At each design iteration, IDOME provides a set of alternative building layouts which satisfy user-defined constraints and optimality criteria concerning a user-defined space parametrization. When the user selects a design generated by IDOME, the system performs a similar optimization process with the same (or different) parameters and objectives. A user may iterate this exploration process as many times as needed. In this work, we focus on optimizing built environments using architectural metrics by improving the degree of visibility, accessibility, and information gaining for navigating a proposed space. This approach, however, can be extended to support other kinds of analysis as well. We demonstrate the capabilities of IDOME through a series of examples, performance analysis, user studies, and a usability test. The results indicate that IDOME successfully optimizes the proposed designs concerning the chosen metrics and offers a satisfactory experience for users with minimal training."
    },
    {
        "title": "Visual Analysis of Class Separations With Locally Linear Segments",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Yuxin Ma",
            "Ross Maciejewski"
        ],
        "DOI": "10.1109/TVCG.2020.3011155",
        "citation": 10,
        "abstract": "High-dimensional labeled data widely exists in many real-world applications such as classification and clustering. One main task in analyzing such datasets is to explore class separations and class boundaries derived from machine learning models. Dimension reduction techniques are commonly applied to support analysts in exploring the underlying decision boundary structures by depicting a low-dimensional representation of the data distributions from multiple classes. However, such projection-based analyses are limited due to their lack of ability to show separations in complex non-linear decision boundary structures and can suffer from heavy distortion and low interpretability. To overcome these issues of separability and interpretability, we propose a visual analysis approach that utilizes the power of explainability from linear projections to support analysts when exploring non-linear separation structures. Our approach is to extract a set of locally linear segments that approximate the original non-linear separations. Unlike traditional projection-based analysis where the data instances are mapped to a single scatterplot, our approach supports the exploration of complex class separations through multiple local projection results. We conduct case studies on two labeled datasets to demonstrate the effectiveness of our approach."
    },
    {
        "title": "Mechanics-Aware Modeling of Cloth Appearance",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "1 Jan. 2021",
        "authors": [
            "Zahra Montazeri",
            "Chang Xiao",
            "Yun Fei",
            "Changxi Zheng",
            "Shuang Zhao"
        ],
        "DOI": "10.1109/TVCG.2019.2937301",
        "citation": 10,
        "abstract": "Micro-appearance models have brought unprecedented fidelity and details to cloth rendering. Yet, these models neglect fabric mechanics: when a piece of cloth interacts with the environment, its yarn and fiber arrangement usually changes in response to external contact and tension forces. Since subtle changes of a fabric's microstructures can greatly affect its macroscopic appearance, mechanics-driven appearance variation of fabrics has been a phenomenon that remains to be captured. We introduce a mechanics-aware model that adapts the microstructures of cloth yarns in a physics-based manner. Our technique works on two distinct physical scales: using physics-based simulations of individual yarns, we capture the rearrangement of yarn-level structures in response to external forces. These yarn structures are further enriched to obtain appearance-driving fiber-level details. The cross-scale enrichment is made practical through a new parameter fitting algorithm for simulation, an augmented procedural yarn model coupled with a custom-design regression neural network. We train the network using a dataset generated by joint simulations at both the yarn and the fiber levels. Through several examples, we demonstrate that our model is capable of synthesizing photorealistic cloth appearance in a mechanically plausible way."
    }
]