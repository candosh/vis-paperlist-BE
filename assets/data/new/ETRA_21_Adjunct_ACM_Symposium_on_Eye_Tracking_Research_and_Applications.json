[
  {
    "title": "GazeHelp: Exploring Practical Gaze-assisted Interactions for Graphic Design Tools",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Demo & Video Track",
    "data": "May 2021",
    "authors": ["Ryan Lewien"],
    "DOI": "https://doi.org/10.1145/3450341.3458764",
    "citation": "2",
    "abstract": "This system development project introduces the Adobe Photoshop plugin GazeHelp, exploring the practical application of multimodal gaze-assisted interaction in assisting current graphic design activities. It implements three core features, including QuickTool: a gaze-triggered popup that allows the user to select their next tool with gaze; X-Ray: creating a small non-destructive window at the gaze point, cutting through an artboard’s layers to expose an element on a selected underlying layer; and Privacy Shield: dimming and blocking the current art board from view when looking away from the display. Each harness the speed, gaze-contingent observational nature and presence-implying strengths of gaze respectively, and are customisable to the user’s preferences. The accompanying GazeHelpServer, complete with intuitive GUI, can also be flexibly used by other programs and plugins for further development."
  },
  {
    "title": "Implementing Eye-Tracking for Persona Analytics",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Demo & Video Track",
    "data": "May 2021",
    "authors": ["Soon-Gyo Jung", "Joni Salminen", "Bernard Jansen"],
    "DOI": "https://doi.org/10.1145/3450341.3458765",
    "citation": "3",
    "abstract": "Investigating users’ engagement with interactive persona systems can yield crucial insights for the design of such systems. Using eye-tracking, researchers can address the scarcity of behavioral user studies, even during times when physical user studies are difficult or impossible to carry out. In this research, we implement a webcam-based eye-tracking module into an interactive persona system, facilitating remote user studies. Findings from the implementation can show what information users pay attention to in persona profiles."
  },
  {
    "title": "Automatic Recognition and Augmentation of Attended Objects in Real-time using Eye Tracking and a Head-mounted Display",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Demo & Video Track",
    "data": "May 2021",
    "authors": [
      "Michael Barz",
      "Sebastian Kapp",
      "Jochen Kuhn",
      "Daniel Sonntag"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3458766",
    "citation": "9",
    "abstract": "Scanning and processing visual stimuli in a scene is essential for the human brain to make situation-aware decisions. Adding the ability to observe the scanning behavior and scene processing to intelligent mobile user interfaces can facilitate a new class of cognition-aware user interfaces. As a first step in this direction, we implement an augmented reality (AR) system that classifies objects at the user’s point of regard, detects visual attention to them, and augments the real objects with virtual labels that stick to the objects in real-time. We use a head-mounted AR device (Microsoft HoloLens 2) with integrated eye tracking capabilities and a front-facing camera for implementing our prototype."
  },
  {
    "title": "Eye Tracking Calibration on Mobile Devices",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "May 2021",
    "authors": ["Yaxiong Lei"],
    "DOI": "https://doi.org/10.1145/3450341.3457989",
    "citation": "1",
    "abstract": "Eye tracking has been widely used in psychology, human-computer interaction and many other fields. Recently, eye tracking based on off-the-shelf cameras has produced promising results, compared to the traditional eye tracking devices. This presents an opportunity to introduce eye tracking on mobile devices. However, eye tracking on mobile devices face many challenges, including occlusion of faces and unstable and changing distance between face and camera. This research project aims to obtain stable and accurate calibration of front-camera based eye tracking in dynamic contexts through the construction of real-world eye-movement datasets, the introduction of novel context-awareness models and improved gaze estimation methods that can be adapted to partial faces."
  },
  {
    "title": "The influence of clutter on search-based learning, long-term memory, and memory-guided attention in real-world scenes: an eye-movement research protocol",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "May 2021",
    "authors": ["Christos Gkoumas", "Andria Shimi"],
    "DOI": "https://doi.org/10.1145/3450341.3457990",
    "citation": "0",
    "abstract": "Previous studies have shown that visual clutter degrades visual search performance. This performance decrement is also reflected in several eye movement metrics, such as mean fixation duration, scan path length, and first saccade latency. However, whether and if so, how visual clutter might impact other cognitive processes that are important for adaptive functioning, like learning, long-term memory, and attention, remains poorly understood. Here, we present the rationale for the use of a three-stage experimental paradigm combined with eye-tracking to better understand the effects of visual clutter observed in real-world scenes on cognition and eye movement behavior. We also present preliminary behavioral findings on this topic from our lab and discuss areas with significant potential for future research."
  },
  {
    "title": "Climate change overlooked. The role of attitudes and mood regulation in visual attention to global warming",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "May 2021",
    "authors": ["Anna Mazurowska"],
    "DOI": "https://doi.org/10.1145/3450341.3457991",
    "citation": "0",
    "abstract": "Why, in the face of climate catastrophe, do people still seem to underestimate the weight of the threat without taking adequate action to fight global warming? Among many reasons for this, the current study aims to dive into people’s cognitive abilities and explore the barriers located at the individual level, using an eye-tracking methodology. Previous findings indicate that a pro-environmental attitude does not necessarily lead to pro-environmental behavior. What may stand in the way is ignorance that can be mediated by other factors. This study will examine whether visual distraction from images depicting the impacts of climate change is mediated by mood regulation and environmental concern. This will help to fit educational and information materials to specific viewers, which may result in more pro-environmental behaviors in the future."
  },
  {
    "title": "Gaze and Heart Rate Synchronization in Computer-Mediated Collaboration",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ETRA Doctoral Symposium",
    "data": "May 2021",
    "authors": ["Katarzyna Wisiecka"],
    "DOI": "https://doi.org/10.1145/3450341.3457992",
    "citation": "0",
    "abstract": "Computer-mediated collaboration has become an integral part of our every day functioning. Despite decreased non-verbal communication and face-to-face contact with partners of collaboration, people learned how to remotely work together. The consequences of decreased non-verbal signals such as gaze communication in remote collaboration are however not fully investigated. In a series of three experiments, we propose solutions to enhance quality of remote collaboration. The present paper is focused on examining the relation between gaze and heart reaction during face-to-face and remote collaboration."
  },
  {
    "title": "Solving Parallax Error for 3D Eye Tracking",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": ["Agostino Gibaldi", "Vasha DuTell", "Martin S Banks"],
    "DOI": "https://doi.org/10.1145/3450341.3458494",
    "citation": "2",
    "abstract": "Head-mounted eye-trackers allow for unrestricted behavior in the natural environment, but have calibration issues that compromise accuracy and usability. A well-known problem arises from the fact that gaze measurements suffer from parallax error due to the offset between the scene camera origin and eye position. To compensate for this error two pieces of data are required: the pose of the scene camera in head coordinates, and the three-dimensional coordinates of the fixation point in head coordinates. We implemented a method that allows for effective and accurate eye-tracking in the three-dimensional environment. Our approach consists of a calibration procedure that allows to contextually calibrate the eye-tracker and compute the eyes pose in the reference frame of the scene camera, and a custom stereoscopic scene camera that provides the three-dimensional coordinates of the fixation point. The resulting gaze data are free from parallax error, allowing accurate and effective use of the eye-tracker in the natural environment."
  },
  {
    "title": "Integrating High Fidelity Eye, Head and World Tracking in a Wearable Device",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": [
      "Vasha DuTell",
      "Agostino Gibaldi",
      "Giulia Focarelli",
      "Bruno Olshausen",
      "Martin S Banks"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3458488",
    "citation": "0",
    "abstract": "A challenge in mobile eye tracking is balancing the quality of data collected with the ability for a subject to move freely and naturally through their environment. This challenge is exacerbated when an experiment necessitates multiple data streams recorded simultaneously and in high fidelity. Given these constraints, previous devices have had limited spatial and temporal resolution, as well as compression artifacts. To address this, we have designed a wearable device capable of recording a subject’s body, head, and eye positions, simultaneously with RGB and depth data from the subject’s visual environment, measured in high spatial and temporal resolution. The sensors include a binocular eye tracker, an RGB-D scene camera, a high-frame-rate scene camera, and two visual odometry sensors, which we synchronize and record from, with a total incoming data rate of over 700 MB/s. All sensors are operated by a mini-PC optimized for fast data collection, and powered by a small battery pack. The headset weighs only 1.4 kg, the remainder just 3.9kg, and can be comfortably worn by the subject in a small backpack, allowing full mobility."
  },
  {
    "title": "Fixational stability as a measure for the recovery of visual function in amblyopia",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": ["Avi M. Aizenman", "Dennis M. Levi"],
    "DOI": "https://doi.org/10.1145/3450341.3458493",
    "citation": "0",
    "abstract": "People with amblyopia have been shown to have decreased fixational stability, particularly those with strabismic amblyopia. Fixational stability and visual acuity have been shown to be tightly correlated across multiple studies, suggesting a relationship between acuity and oculomotor stability. Reduced visual acuity is the sine qua non of amblyopia, and recovery is measured by the improvement in visual acuity. Here we ask whether fixational stability can be used as an objective marker for the recovery of visual function in amblyopia. We tracked children’s fixational stability during patching treatment over time and found fixational stability changes alongside improvements in visual acuity. This suggests fixational stability can be used as an objective measure for monitoring treatment in amblyopia and other disorders."
  },
  {
    "title": "Tracking Active Observers in 3D Visuo-Cognitive Tasks",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": ["Markus D. Solbach", "John K. Tsotsos"],
    "DOI": "https://doi.org/10.1145/3450341.3458496",
    "citation": "1",
    "abstract": "Most past and present research in computer vision involves passively observed data. Humans, however, are active observers in real life; they explore, search, select what and how to look. In this work, we present a psychophysical experimental setup for active, visual observation in a 3D world dubbed PESAO. The goal was to design PESAO for various active perception tasks with human subjects (active observers) capable of tracking the head and gaze."
  },
  {
    "title": "Algorithmic gaze classification for mobile eye-tracking",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": ["Daniel Müller", "David Mann"],
    "DOI": "https://doi.org/10.1145/3450341.3458886",
    "citation": "0",
    "abstract": "Mobile eye tracking traditionally requires gaze to be coded manually. We introduce an open-source Python package (GazeClassify) that algorithmically annotates mobile eye tracking data for the study of human interactions. Instead of manually identifying objects and identifying if gaze is directed towards an area of interest, computer vision algorithms are used for the identification and segmentation of human bodies. To validate the algorithm, mobile eye tracking data from short combat sport sequences were analyzed. The performance of the algorithm was compared against three manual raters. The algorithm performed with substantial reliability in comparison to the manual raters when it came to annotating which area of interest gaze was closest to. However, the algorithm was more conservative than the manual raters for classifying if gaze was directed towards an object of interest. The algorithmic approach represents a viable and promising means for automating gaze classification for mobile eye tracking."
  },
  {
    "title": "Sub-centimeter 3D gaze vector accuracy on real-world tasks: an investigation of eye and motion capture calibration routines",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": [
      "Scott A. Stone",
      "Quinn A. Boser",
      "T. Riley Dawson",
      "Albert H. Vette",
      "Jacqueline S. Hebert",
      "Patrick M. Pilarski",
      "Craig S. Chapman"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3458880",
    "citation": "0",
    "abstract": "Measuring where people look in real-world tasks has never been easier but analyzing the resulting data remains laborious. One solution integrates head-mounted eye tracking with motion capture but no best practice exists regarding what calibration data to collect. Here, we compared four ~1 min calibration routines used to train linear regression gaze vector models and examined how the coordinate system, eye data used and location of fixation changed gaze vector accuracy on three trial types: calibration, validation (static fixation to task relevant locations), and task (naturally occurring fixations during object interaction). Impressively, predicted gaze vectors show ~1 cm of error when looking straight ahead toward objects during natural arms-length interaction. This result was achieved predicting fixations in a Spherical coordinate frame, from the best monocular data, and, surprisingly, depends little on the calibration routine."
  },
  {
    "title": "Ergonomic Design Development of the Visual Experience Database Headset",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": [
      "Bharath Shankar",
      "Christian Sinnott",
      "Kamran Binaee",
      "Mark D. Lescroart",
      "Paul MacNeilage"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3458487",
    "citation": "1",
    "abstract": "Head-mounted devices allow recording of eye movements, head movements, and scene video outside of the traditional laboratory setting. A key challenge for recording comprehensive first-person stimuli and behavior outside the lab is the form factor of the head-mounted assembly. It should be mounted stably on the head to minimize slippage and maximize accuracy of the data; it should be as unobtrusive and comfortable as possible to allow for natural behaviors and enable longer duration recordings; and it should be able to fit a diverse user population. Here, we survey preliminary design iterations of the Visual Experience Database headset, an assembly consisting of the Pupil Core eye tracker, the Intel RealSense T265 ™ (T265) tracking camera, and the FLIR Chameleon™3 (FLIR) world camera. Strengths and weaknesses of each iteration are explored and documented with the goal of informing future ergonomic design efforts for similar head-mounted systems."
  },
  {
    "title": "VEDBViz: The Visual Experience Database Visualization and Interaction Tool",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": [
      "Sanjana Ramanujam",
      "Christian Sinnott",
      "Bharath Shankar",
      "Savannah Jo Halow",
      "Brian Szekely",
      "Paul MacNeilage",
      "Kamran Binaee"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3458486",
    "citation": "1",
    "abstract": "Mobile, simultaneous tracking of both the head and eyes is typically achieved through integration of separate head and eye tracking systems because off-the-shelf solutions do not yet exist. Similarly, joint visualization and analysis of head and eye movement data is not possible with standard software packages because these were designed to support either head or eye tracking in isolation. Thus, there is a need for software that supports joint analysis of head and eye data to characterize and investigate topics including head-eye coordination and reconstruction of how the eye is moving in space. To address this need, we have begun developing VEDBViz which supports simultaneous graphing and animation of head and eye movement data recorded with the Intel RealSense T265 and Pupil Core, respectively. We describe current functionality as well as features and applications that are still in development."
  },
  {
    "title": "Eye, Robot: Calibration Challenges and Potential Solutions for Wearable Eye Tracking in Individuals with Eccentric Fixation",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": ["Kassia Love", "Anca Velisar", "Natela Shanidze"],
    "DOI": "https://doi.org/10.1145/3450341.3458489",
    "citation": "3",
    "abstract": "Loss of the central retina, including the fovea, can lead to a loss of visual acuity and oculomotor deficits, and thus have profound effects on day-to-day tasks. Recent advances in head-mounted, 3D eye tracking have allowed researchers to extend studies in this population to a broader set of daily tasks and more naturalistic behaviors and settings. However, decreases in fixational stability, multiple fixational loci and their uncertain role as oculomotor references, as well as eccentric fixation all provide additional challenges for calibration and collection of eye movement data. Here we quantify reductions in calibration accuracy relative to fixation eccentricity, and suggest a robotic calibration and validation tool that will allow for future developments of calibration and tracking algorithms designed with this population in mind."
  },
  {
    "title": "Post-processing integration and semi-automated analysis of eye-tracking and motion-capture data obtained in immersive virtual reality environments to measure visuomotor integration",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": [
      "Haylie L. Miller",
      "Ian Raphael Zurutuza",
      "Nicholas Fears",
      "Suleyman Polat",
      "Rodney Nielsen"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3458881",
    "citation": "0",
    "abstract": "Mobile eye-tracking and motion-capture techniques yield rich, precisely quantifiable data that can inform our understanding of the relationship between visual and motor processes during task performance. However, these systems are rarely used in combination, in part because of the significant time and human resources required for post-processing and analysis. Recent advances in computer vision have opened the door for more efficient processing and analysis solutions. We developed a post-processing pipeline to integrate mobile eye-tracking and full-body motion-capture data. These systems were used simultaneously to measure visuomotor integration in an immersive virtual environment. Our approach enables calculation of a 3D gaze vector that can be mapped to the participant's body position and objects in the virtual environment using a uniform coordinate system. This approach is generalizable to other configurations, and enables more efficient analysis of eye, head, and body movements together during visuomotor tasks administered in controlled, repeatable environments."
  },
  {
    "title": "Pupil Tracking Under Direct Sunlight",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": [
      "Kamran Binaee",
      "Christian Sinnott",
      "Kaylie Jacleen Capurro",
      "Paul MacNeilage",
      "Mark D Lescroart"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3458490",
    "citation": "3",
    "abstract": "Pupil tracking in a bright outdoor environment is challenging due to low eye image quality and reduced pupil size in response to bright light. In this study we present research to develop robust outdoor pupil tracking without the need for shading the eyes. We first investigate the effect of camera post-processing settings in order to find values that enhance image quality for the purpose of pupil tracking under direct, oblique and overcast sunlight illuminations. We then tested the performance of the state-of-the-art pupil tracking techniques under these extreme real-world outdoor lighting conditions. Our results suggest that a key goal should be maintaining the contrast between iris and pupil to support accurate estimation of pupil position regardless of the overall eye image quality."
  },
  {
    "title": "Characterizing the Performance of Deep Neural Networks for Eye-Tracking",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": [
      "Arnab Biswas",
      "Kamran Binaee",
      "Kaylie Jacleen Capurro",
      "Mark D. Lescroart"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3458491",
    "citation": "0",
    "abstract": "Deep neural networks (DNNs) provide powerful tools to identify and track features of interest, and have recently come into use for eye-tracking. Here, we test the ability of a DNN to predict keypoints localizing the eyelid and pupil under the types of challenging image variability that occur in mobile eye-tracking. We simulate varying degrees of perturbation for five common sources of image variation in mobile eye-tracking: rotations, blur, exposure, reflection, and compression artifacts. To compare the relative performance decrease across domains in a common space of image variation, we used features derived from a DNN (ResNet50) to compute the distance of each perturbed video from the videos used to train our DNN. We found that increasing cosine distance from the training distribution was associated with monotonic decreases in model performance in all domains. These results suggest ways to optimize the selection of diverse images for model training."
  },
  {
    "title": "Noise in the Machine: Sources of Physical and Computation Error in Eye Tracking with Pupil Core Wearable Eye Tracker: Wearable Eye Tracker Noise in Natural Motion Experiments",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: ActivEye: Challenges in large scale eye-tracking for active participants",
    "data": "May 2021",
    "authors": ["Anca Velisar", "Natela Shanidze"],
    "DOI": "https://doi.org/10.1145/3450341.3458495",
    "citation": "1",
    "abstract": "Developments in wearable eye tracking devices make them an attractive solution for studies of eye movements during naturalistic head/body motion. However, before these systems’ potential can be fully realized, a thorough assessment of potential sources of error is needed. In this study, we examine three possible sources for the Pupil Core eye tracking goggles: camera motion during head/body motion, choice of calibration marker configuration, and eye movement estimation. In our data, we find that up to 36% of reported eye motion may be attributable to camera movement; choice of appropriate calibration routine is essential for minimizing error; and the use of a secondary calibration for eye position remapping can improve eye position errors estimated from the eye tracker."
  },
  {
    "title": "Gaze Interactive and Attention Aware Low Vision Aids as Future Smart Glasses",
    "conferenceTitle": "ETRA '21 Adjunct: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN Symposium",
    "data": "May 2021",
    "authors": [
      "Fiona Bríd Mulvey",
      "Marek Mikitovic",
      "Mateusz Sadowski",
      "Baosheng Hou",
      "Nils David Rasamoel",
      "John Paulin Paulin Hansen",
      "Per Bækgaard"
    ],
    "DOI": "https://doi.org/10.1145/3450341.3460769",
    "citation": "2",
    "abstract": "We present a working paper on integrating eye tracking with mixed and augmented reality for the benefit of low vision aids. We outline the current state of the art and relevant research and point to further research and development required in order to adapt to individual user, environment, and current task. We outline key technical challenges and possible solutions including calibration, dealing with variant eye data quality, measuring and adapting image processing to low vision within current technical limitations, and outline an experimental approach to designing data-driven solutions using machine learning and artificial intelligence."
  },
  {
    "title": "Important Considerations of Data Collection and Curation for Reliable Benchmarking of End-User Eye-Tracking Systems",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": [
      "Iakov Chernyak",
      "Grigory Chernyak",
      "Jeffrey K. S. Bland",
      "Pierre D. P. Rahier"
    ],
    "DOI": "https://doi.org/10.1145/3448017.3457383",
    "citation": "2",
    "abstract": "In this article we discuss how to build a reliable system to estimate the quality of a VR eye-tracker from an accuracy and robustness point of view. We list up and discuss problems that occur at the data collection, data curation and data processing stages. We address this article to academic eye-tracking researchers and commercial eye-tracker developers with the purpose of raising the problem of standardization of eye-tracking benchmarks, and to make a step towards repeatability of benchmarking results. The main scope of this article is consumer-focused eye-tracking VR headsets, however some parts also apply to AR and remote eye-trackers, and to research environments. As an example, we demonstrate how to use the proposed methodology to build, benchmark and estimate the accuracy of the FOVE0 eye-tracking headset."
  },
  {
    "title": "Analysis of iris obfuscation: Generalising eye information processes for privacy studies in eye tracking.",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": ["Anton Mølbjerg Eskildsen", "Dan Witzner Hansen"],
    "DOI": "https://doi.org/10.1145/3448017.3457385",
    "citation": "0",
    "abstract": "We present a framework to model and evaluate obfuscation methods for removing sensitive information in eye-tracking. The focus is on preventing iris-pattern identification. Candidate methods have to be effective at removing information while retaining high utility for gaze estimation. We propose several obfuscation methods that drastically outperform existing ones. A stochastic grid-search is used to determine optimal method parameters and evaluate the model framework. Precise obfuscation and gaze effects are measured for selected parameters. Two attack scenarios are considered and evaluated. We show that large datasets are susceptible to probabilistic attacks, even with seemingly effective obfuscation methods. However, additional data is needed to more accurately access the probabilistic security."
  },
  {
    "title": "The Power of Linked Eye Movement Data Visualizations",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Visualization and Annotation",
    "data": "May 2021",
    "authors": [
      "Michael Burch",
      "Günter Wallner",
      "Nick Broeks",
      "Lulof Piree",
      "Nynke Boonstra",
      "Paul Vlaswinkel",
      "Silke Franken",
      "Vince van Wijk"
    ],
    "DOI": "https://doi.org/10.1145/3448017.3457377",
    "citation": "6",
    "abstract": "In this paper we showcase several eye movement data visualizations and how they can be interactively linked to design a flexible visualization tool for eye movement data. The aim of this project is to create a user-friendly and easy accessible tool to interpret visual attention patterns and to facilitate data analysis for eye movement data. Hence, to increase accessibility and usability we provide a web-based solution. Users can upload their own eye movement data set and inspect it from several perspectives simultaneously. Insights can be shared and collaboratively be discussed with others. The currently available visualization techniques are a 2D density plot, a scanpath representation, a bee swarm, and a scarf plot, all supporting several standard interaction techniques. Moreover, due to the linking feature, users can select data in one visualization, and the same data points will be highlighted in all active visualizations for solving comparison tasks. The tool also provides functions that make it possible to upload both, private or public data sets, and can generate URLs to share the data and settings of customized visualizations. A user study showed that the tool is understandable and that providing linked customizable views is beneficial for analyzing eye movement data."
  },
  {
    "title": "Image-Based Projection Labeling for Mobile Eye Tracking",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Visualization and Annotation",
    "data": "May 2021",
    "authors": ["Kuno Kurzhals"],
    "DOI": "https://doi.org/10.1145/3448017.3457382",
    "citation": "6",
    "abstract": "The annotation of gaze data concerning investigated areas of interest (AOIs) poses a time-consuming step in the analysis procedure of eye tracking experiments. For data from mobile eye tracking glasses, the annotation effort is further increased because each recording has to be investigated individually. Automated approaches based on supervised machine learning require pre-trained categories which are hard to obtain without human interpretation, i.e., labeling ground truth data. We present an interactive visualization approach that supports efficient annotation of gaze data based on image content participants with eye tracking glasses focused on. Recordings can be segmented individually to reduce the annotation effort. Thumbnails represent segments visually and are projected on a 2D plane for a fast comparison of AOIs. Annotated scanpaths can then be interpreted directly with the timeline visualization. We showcase our approach with three different scenarios."
  },
  {
    "title": "Neural Networks for Semantic Gaze Analysis in XR Settings",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Visualization and Annotation",
    "data": "May 2021",
    "authors": [
      "Lena Stubbemann",
      "Dominik Dürrschnabel",
      "Robert Refflinghaus"
    ],
    "DOI": "https://doi.org/10.1145/3448017.3457380",
    "citation": "2",
    "abstract": "Virtual-reality (VR) and augmented-reality (AR) technology is increasingly combined with eye-tracking. This combination broadens both fields and opens up new areas of application, in which visual perception and related cognitive processes can be studied in interactive but still well controlled settings. However, performing a semantic gaze analysis of eye-tracking data from interactive three-dimensional scenes is a resource-intense task, which so far has been an obstacle to economic use. In this paper we present a novel approach which minimizes time and information necessary to annotate volumes of interest (VOIs) by using techniques from object recognition. To do so, we train convolutional neural networks (CNNs) on synthetic data sets derived from virtual models using image augmentation techniques. We evaluate our method in real and virtual environments, showing that the method can compete with state-of-the-art approaches, while not relying on additional markers or preexisting databases but instead offering cross-platform use."
  },
  {
    "title": "Where Do Deep Fakes Look? Synthetic Face Detection via Gaze Tracking",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": ["Ilke Demir", "Umur Aybars Ciftci"],
    "DOI": "https://doi.org/10.1145/3448017.3457387",
    "citation": "10",
    "abstract": "Following the recent initiatives for the democratization of AI, deep fake generators have become increasingly popular and accessible, causing dystopian scenarios towards social erosion of trust. A particular domain, such as biological signals, attracted attention towards detection methods that are capable of exploiting authenticity signatures in real videos that are not yet faked by generative approaches. In this paper, we first propose several prominent eye and gaze features that deep fakes exhibit differently. Second, we compile those features into signatures and analyze and compare those of real and fake videos, formulating geometric, visual, metric, temporal, and spectral variations. Third, we generalize this formulation to the deep fake detection problem by a deep neural network, to classify any video in the wild as fake or real. We evaluate our approach on several deep fake datasets, achieving 92.48% accuracy on FaceForensics++, 80.0% on Deep Fakes (in the wild), 88.35% on CelebDF, and 99.27% on DeeperForensics datasets. Our approach outperforms most deep and biological fake detectors with complex network architectures without the proposed gaze signatures. We conduct ablation studies involving different features, architectures, sequence durations, and post-processing artifacts."
  },
  {
    "title": "Toward Eye-Tracked Sideline Concussion Assessment in eXtended Reality",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": [
      "Anderson Schrader",
      "Isabella Gebhart",
      "Drew Garrison",
      "Andrew Duchowski",
      "Martian Lapadatescu",
      "Weiyu Feng",
      "Mahmoud Thabit",
      "Fang Wang",
      "Krzysztof Krejtz",
      "Daniel D. Petty"
    ],
    "DOI": "https://doi.org/10.1145/3448017.3457378",
    "citation": "4",
    "abstract": "As there is no currently available portable, visuomotor assessment of concussion at the sidelines, we present preliminary development of an approach based on Predictive Visual Tracking (PVT) suitable for the sidelines. Previous work has shown PVT sensitivity and specificity of 0.85 and 0.73, respectively, for standard deviation of radial error for normal and acute concussion (mild Traumatic Brain Injury, or mTBI), using a simple orbiting target stimulus. We propose new variants of the radial and tangential error metrics and conduct preliminary evaluation in Virtual Reality when applied to two different target motions (orbit and pendulum). Our new local visualization is intuitive, especially when considering evaluation of the pendulum target. Initial results indicate promise for baseline-related, personalized concussion testing in extended reality."
  },
  {
    "title": "Crossed Eyes: Domain Adaptation for Gaze-Based Mind Wandering Models",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": ["Robert E. Bixler", "Sidney K. D'Mello"],
    "DOI": "https://doi.org/10.1145/3448017.3457386",
    "citation": "4",
    "abstract": "The effectiveness of user interfaces are limited by the tendency for the human mind to wander. Intelligent user interfaces can combat this by detecting when mind wandering occurs and attempting to regain user attention through a variety of intervention strategies. However, collecting data to build mind wandering detection models can be expensive, especially considering the variety of media available and potential differences in mind wandering across them. We explored the possibility of using eye gaze to build cross-domain models of mind wandering where models trained on data from users in one domain are used for different users in another domain. We built supervised classification models using a dataset of 132 users whose mind wandering reports were collected in response to thought-probes while they completed tasks from seven different domains for six minutes each (five domains are investigated here: Illustrated Text, Narrative Film, Video Lecture, Naturalistic Scene, and Reading Text). We used global eye gaze features to build within- and cross- domain models using 5-fold user-independent cross validation. The best performing within-domain models yielded AUROCs ranging from .57 to .72, which were comparable for the cross-domain models (AUROCs of .56 to .68). Models built from coarse-grained locality features capturing the spatial distribution of gaze resulted in slightly better transfer on average (transfer ratios of .61 vs .54 for global models) due to improved performance in certain domains. Instance-based and feature-level domain adaptation did not result in any improvements in transfer. We found that seven gaze features likely contributed to transfer as they were among the top ten features for at least four domains. Our results indicate that gaze features are suitable for domain adaptation from similar domains, but more research is needed to improve domain adaptation between more dissimilar domains."
  },
  {
    "title": "GazeMeter: Exploring the Usage of Gaze Behaviour to Enhance Password Assessments",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": [
      "Yasmeen Abdrabou",
      "Ahmed Shams",
      "Mohamed Omar Mantawy",
      "Anam Ahmad Khan",
      "Mohamed Khamis",
      "Florian Alt",
      "Yomna Abdelrahman"
    ],
    "DOI": "https://doi.org/10.1145/3448017.3457384",
    "citation": "3",
    "abstract": "We investigate the use of gaze behaviour as a means to assess password strength as perceived by users. We contribute to the effort of making users choose passwords that are robust against guessing-attacks. Our particular idea is to consider also the users’ understanding of password strength in security mechanisms. We demonstrate how eye tracking can enable this: by analysing people’s gaze behaviour during password creation, its strength can be determined. To demonstrate the feasibility of this approach, we present a proof of concept study (N = 15) in which we asked participants to create weak and strong passwords. Our findings reveal that it is possible to estimate password strength from gaze behaviour with an accuracy of 86% using Machine Learning. Thus, we enable research on novel interfaces that consider users’ understanding with the ultimate goal of making users choose stronger passwords."
  },
  {
    "title": "Gaze+Hold: Eyes-only Direct Manipulation with Continuous Gaze Modulated by Closure of One Eye",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Gaze Input",
    "data": "May 2021",
    "authors": [
      "Argenis Ramirez Ramirez Gomez",
      "Christopher Clarke",
      "Ludwig Sidenmark",
      "Hans Gellersen"
    ],
    "DOI": "https://doi.org/10.1145/3448017.3457381",
    "citation": "10",
    "abstract": "The eyes are coupled in their gaze function and therefore usually treated as a single input channel, limiting the range of interactions. However, people are able to open and close one eye while still gazing with the other. We introduce Gaze+Hold as an eyes-only technique that builds on this ability to leverage the eyes as separate input channels, with one eye modulating the state of interaction while the other provides continuous input. Gaze+Hold enables direct manipulation beyond pointing which we explore through the design of Gaze+Hold techniques for a range of user interface tasks. In a user study, we evaluated performance, usability and user’s spontaneous choice of eye for modulation of input. The results show that users are effective with Gaze+Hold. The choice of dominant versus non-dominant eye had no effect on performance, perceived usability and workload. This is significant for the utility of Gaze+Hold as it affords flexibility for mapping of either eye in different configurations."
  },
  {
    "title": "HGaze Typing: Head-Gesture Assisted Gaze Typing",
    "conferenceTitle": "ETRA '21 Full Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Gaze Input",
    "data": "May 2021",
    "authors": [
      "Wenxin Feng",
      "Jiangnan Zou",
      "Andrew Kurauchi",
      "Carlos H Morimoto",
      "Margrit Betke"
    ],
    "DOI": "https://doi.org/10.1145/3448017.3457379",
    "citation": "10",
    "abstract": "This paper introduces a bi-modal typing interface, HGaze Typing, which combines the simplicity of head gestures with the speed of gaze inputs to provide efficient and comfortable dwell-free text entry. HGaze Typing uses gaze path information to compute candidate words and allows explicit activation of common text entry commands, such as selection, deletion, and revision, by using head gestures (nodding, shaking, and tilting). By adding a head-based input channel, HGaze Typing reduces the size of the screen regions for cancel/deletion buttons and the word candidate list, which are required by most eye-typing interfaces. A user study finds HGaze Typing outperforms a dwell-time-based keyboard in efficacy and user satisfaction. The results demonstrate that the proposed method of integrating gaze and head-movement inputs can serve as an effective interface for text entry and is robust to unintended selections."
  },
  {
    "title": "Synchronization of Spontaneous Eyeblink during Formula Car Driving",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": ["Ryota Nishizono", "Naoki Saijo", "Makio Kashino"],
    "DOI": "https://doi.org/10.1145/3448018.3458002",
    "citation": "1",
    "abstract": "Formula car racing is a highly competitive sport. Previous studies have investigated the physiological characteristics and motor behaviors of drivers; however, little is known about how they modulate their cognitive states to improve their skills. Spontaneous eyeblink is a noteworthy factor because it reflects attentional states and is important for drivers to minimize the chance of losing critical visual information. In this study, we investigated whether the blink rate, blink synchronization among laps in each driver, and synchronization across drivers were related to their performance. Toward this end, we recorded the blinks and car behavior data of two professional drivers in quasi-racing environments. The results showed higher synchronization in higher-performance laps of each driver and across drivers but no significant change in blink rate. These results suggest that blink synchronization could reflect the changes in performance mode during formula car driving."
  },
  {
    "title": "Towards gaze-based prediction of the intent to interact in virtual reality",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": [
      "Brendan David-John",
      "Candace Peacock",
      "Ting Zhang",
      "T. Scott Murdison",
      "Hrvoje Benko",
      "Tanya R. Jonker"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3458008",
    "citation": "27",
    "abstract": "With the increasing frequency of eye tracking in consumer products, including head-mounted augmented and virtual reality displays, gaze-based models have the potential to predict user intent and unlock intuitive new interaction schemes. In the present work, we explored whether gaze dynamics can predict when a user intends to interact with the real or digital world, which could be used to develop predictive interfaces for low-effort input. Eye-tracking data were collected from 15 participants performing an item-selection task in virtual reality. Using logistic regression, we demonstrated successful prediction of the onset of item selection. The most prevalent predictive features in the model were gaze velocity, ambient/focal attention, and saccade dynamics, demonstrating that gaze features typically used to characterize visual attention can be applied to model interaction intent. In the future, these types of models can be used to infer user’s near-term interaction goals and drive ultra-low-friction predictive interfaces."
  },
  {
    "title": "Pupillary response reflects vocabulary comprehension",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": ["Takashi Hirata", "Yutaka Hirata"],
    "DOI": "https://doi.org/10.1145/3448018.3457999",
    "citation": "0",
    "abstract": "It has been known that the pupil of the human eye responds not only to changes in brightness, but also those in cognitive activities. A recent study reported that pupillary dilation reflects one's ability to discriminate English sounds /l/ and /r/, suggesting that pupillary responses may be used to evaluate learner's listening comprehension. Presently, we recorded English learners’ pupillary responses in three situations: pre- (Control), in- (Study), and post-study (Test). We then classified our participants into two groups based upon their test scores (high and low), and compared their pupillary responses in the three situations. As the result, significantly different pupillary responses (dilatation) were observed between the two groups in Study. This result suggests that quantitative observation of pupillary responses may replace or be employed in parallel with traditional vocabulary tests to make vocabulary learning more efficient."
  },
  {
    "title": "Understanding Game Roles and Strategy Using a Mixed Methods Approach",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": ["Kaitlyn M Roose", "Elizabeth S. Veinott"],
    "DOI": "https://doi.org/10.1145/3448018.3458006",
    "citation": "0",
    "abstract": "In this paper, we use the Tracer Method to examine a complex and team-oriented, first-person shooter game to determine how the output can better inform Esports training. The Tracer Method combines eye tracking with Critical Decision Method to focus the analyses on the critical aspects of gameplay, while providing insight into the most frequent visual search transitions across game areas of interest. We examined the differences across three in-game roles and three decision types (strategic, operational, and tactical) using network centrality diagrams and entropy measures. No differences in overall stationary entropy were found for either role or decision type. However, each game role and decision type produced a different network centrality diagram, indicating different visual search transitions, which could support training of Esport players."
  },
  {
    "title": "Understanding Urban Devotion through the Eyes of an Observer",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Applications",
    "data": "May 2021",
    "authors": [
      "Margarita Vinnikov",
      "Kian Motahari",
      "Louis I. Hamilton",
      "Burcak Ozludil Altin"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3458003",
    "citation": "0",
    "abstract": "If and how an individual’s social, economic, and cultural backgrounds affect their perception of the built environment, is a fundamental problem for architects, anthropologists, historians, and urban planners alike. Similar factors affect an individual’s religious beliefs and tendencies. Our research addresses the intersection of personal background and perception of sacred space by examining people’s responses to a virtual replica of a “madonella,” a street shrine in Rome. The shrine was virtually recreated using photogrammetry. It was optimized for user studies employing VIVE Pro Eye. The study looked at the gaze behavior of 24 participants and compared their gaze patterns with demographic background and social-communal responses. The study finds that certain religious habits of an individual could predict their fixational features, including the number and total duration of fixations, on pivotal areas of interest in the shrine environment (even though these areas were placed outside of immediate sight). These results are a promising start to our ongoing study of the perception and received meaning of sacred space."
  },
  {
    "title": "Repetition effects in task-driven eye movement analyses after longer time-spans",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": ["Thomas Berger", "Michael Raschke"],
    "DOI": "https://doi.org/10.1145/3448018.3458005",
    "citation": "1",
    "abstract": "Visualizations are an important tool in risk management to support decision-making of recipients of risk reports. Many trainings aim at helping managers to better understand how to read such visualizations. In this paper we present first results of an ongoing large study on the effect of repeated presentation of risk visualizations from annual reports. This is of importance to find out if such repetitions have an effect on accuracy and the behavior of readers. Contrary to other studies we had longer time-spans of months and weeks between two trials. We found that fixation durations are different after second presentation and that the number of fixations generally are lower. We also analyzed scan paths, indicating that regions that are more semantically meaningful are more often in the center of attention. We call for more studies with longer time spans between two trials as we found some interesting patterns."
  },
  {
    "title": "Eye Gaze Estimation using Imperceptible Marker Presented on High-Speed Display",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": ["Kento Seida", "Kentaro Takemura"],
    "DOI": "https://doi.org/10.1145/3448018.3458000",
    "citation": "1",
    "abstract": "Advanced eye-tracking methods require a dedicated display equipped with near-infrared LEDs (light-emitting diodes). However, this requirement hinders the widespread adoption of such methods. Additionally, some glints may pass undetected when a large display is employed. To avoid these problems, we propose eye gaze estimation using imperceptible markers presented on a commercially available high-speed display. The marker reference points reflected on the cornea are extracted instead of glints, and the point-of-gaze can be estimated using the cross-ratio method. The accuracy of the estimated point-of-gaze was approximately 1.64 degrees, as verified from experimental evaluations of the estimation using a high-speed display."
  },
  {
    "title": "Semi-Supervised Learning for Eye Image Segmentation",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": [
      "Aayush Kumar Chaudhary",
      "Prashnna K Gyawali",
      "Linwei Wang",
      "Jeff B Pelz"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3458009",
    "citation": "1",
    "abstract": "Recent advances in appearance-based models have shown improved eye tracking performance in difficult scenarios like occlusion due to eyelashes, eyelids or camera placement, and environmental reflections on the cornea and glasses. The key reason for the improvement is the accurate and robust identification of eye parts (pupil, iris, and sclera regions). The improved accuracy often comes at the cost of labeling an enormous dataset, which is complex and time-consuming. This work presents two semi-supervised learning frameworks to identify eye-parts by taking advantage of unlabeled images where labeled datasets are scarce. With these frameworks, leveraging the domain-specific augmentation and novel spatially varying transformations for image segmentation, we show improved performance on various test cases with limited labeled samples. For instance, for a model trained on just 4 and 48 labeled images, these frameworks improved by at least 4.7% and 0.4% respectively, in segmentation performance over the baseline model, which is trained only with the labeled dataset."
  },
  {
    "title": "Modelling of Blink-Related Eyelid-Induced Shunting on the Electrooculogram",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": [
      "Nathaniel Barbara",
      "Tracey A. Camilleri",
      "Kenneth P. Camilleri"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3457994",
    "citation": "1",
    "abstract": "Besides the traditional regression model-based techniques to estimate the gaze angles (GAs) from electrooculography (EOG) signals, more recent works have investigated the use of a battery model for GA estimation. This is a white-box, explicit and physically-driven model which relates the monopolar EOG potential to the electrode-cornea and electrode-retina distances. In this work, this model is augmented to cater for the blink-induced EOG signal characteristics, by modelling the eyelid-induced shunting effect during blinks. Specifically, a channel-dependent parameter representing the extent to which the amount of eyelid opening affects the particular EOG channel is introduced. A method to estimate these parameters is also proposed and the proposed model is validated by incorporating it in a Kalman filter to estimate the eyelid opening during blinks. The results obtained have demonstrated that the proposed model can accurately represent the blink-related eyelid-induced shunting."
  },
  {
    "title": "Visualizing Prediction Correctness of Eye Tracking Classifiers",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": [
      "Martin H.U. Prinzler",
      "Christoph Schröder",
      "Sahar Mahdie Klim Al Zaidawi",
      "Gabriel Zachmann",
      "Sebastian Maneth"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3457997",
    "citation": "0",
    "abstract": "Eye tracking data is often used to train machine learning algorithms for classification tasks. The main indicator of performance for such classifiers is typically their prediction accuracy. However, this number does not reveal any information about the specific intrinsic workings of the classifier. In this paper we introduce novel visualization methods which are able to provide such information. We introduce the Prediction Correctness Value (PCV). It is the difference between the calculated probability for the correct class and the maximum calculated probability for any other class. Based on the PCV we present two visualizations:  (1) coloring segments of eye tracking trajectories according to their PCV, thus indicating how beneficial certain parts are towards correct classification, and (2) overlaying similar information for all participants to produce a heatmap that indicates at which places fixations are particularly beneficial towards correct classification. Using these new visualizations we compare the performance of two classifiers (RF and RBFN)."
  },
  {
    "title": "OpenNEEDS: A Dataset of Gaze, Head, Hand, and Scene Signals During Exploration in Open-Ended VR Environments",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": [
      "Kara J Emery",
      "Marina Zannoli",
      "James Warren",
      "Lei Xiao",
      "Sachin S Talathi"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3457996",
    "citation": "7",
    "abstract": "We present OpenNEEDS, the first large-scale, high frame rate, comprehensive, and open-source dataset of Non-Eye (head, hand, and scene) and Eye (3D gaze vectors) data captured for 44 participants as they freely explored two virtual environments with many potential tasks (i.e., reading, drawing, shooting, object manipulation, etc.). With this dataset, we aim to enable research on the relationship between head, hand, scene, and gaze spatiotemporal statistics and its applications to gaze estimation. To demonstrate the power of OpenNEEDS, we show that gaze estimation models using individual non-eye sensors and an early fusion model combining all non-eye sensors outperform all baseline gaze estimation models considered, suggesting the possibility of considering non-eye sensors in the design of robust eye trackers. We anticipate that this dataset will support research progress in many areas and applications such as gaze estimation and prediction, sensor fusion, human-computer interaction, intent prediction, perceptuo-motor control, and machine learning."
  },
  {
    "title": "Enhancing the precision of remote eye-tracking using iris velocity estimation",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Methods",
    "data": "May 2021",
    "authors": ["Aayush Kumar Chaudhary", "Jeff B Pelz"],
    "DOI": "https://doi.org/10.1145/3448018.3458010",
    "citation": "1",
    "abstract": "Most of the previous work on eye-tracking has focused on positional information of the eye features. Recent advances in camera technology such as high-resolution and event cameras allow consideration of the velocity estimate for eye tracking. Some previous work on velocity-based estimates has demonstrated high-precision gaze estimation by tracking the motion of iris features on high-resolution images rather than by exploiting pupil edges. While these methods provide high precision, the bottleneck for velocity-based methods are temporal drift and the inability to track across blinks. In this work, we present a new theoretical methodology (πt) to address these issues by optimally combining low-temporal frequency components of the pupil edges with the high-temporal frequency components from the iris textures. We show improved precision with this method while fixating a series of small targets and following a smoothly moving target. Further, we demonstrate the capability to reliably identify microsaccades between targets separated by 0.2°."
  },
  {
    "title": "Gaze+Lip: Rapid, Precise and Expressive Interactions Combining Gaze Input and Silent Speech Commands for Hands-free Smart TV Control",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: User Interfaces and Interaction",
    "data": "May 2021",
    "authors": ["Zixiong Su", "Xinlei Zhang", "Naoki Kimura", "Jun Rekimoto"],
    "DOI": "https://doi.org/10.1145/3448018.3458011",
    "citation": "5",
    "abstract": "As eye-tracking technologies develop, gaze becomes more and more popular as an input modality. However, in situations that require fast and precise object selection, gaze is hard to use because of limited accuracy. We present Gaze+Lip, a hands-free interface that combines gaze and lip reading to enable rapid and precise remote controls when interacting with big displays. Gaze+Lip takes advantage of gaze for target selection and leverages silent speech to ensure accurate and reliable command execution in noisy scenarios such as watching TV or playing videos on a computer. For evaluation, we implemented a system on a TV, and conducted an experiment to compare our method with the dwell-based gaze-only input method. Results showed that Gaze+Lip outperformed the gaze-only approach in accuracy and input speed. Furthermore, subjective evaluations indicated that Gaze+Lip is easy to understand, easy to use, and has higher perceived speed than the gaze-only approach."
  },
  {
    "title": "EyeLogin - Calibration-free Authentication Method for Public Displays Using Eye Gaze",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: User Interfaces and Interaction",
    "data": "May 2021",
    "authors": ["Omair Shahzad Bhatti", "Michael Barz", "Daniel Sonntag"],
    "DOI": "https://doi.org/10.1145/3448018.3458001",
    "citation": "3",
    "abstract": "The usage of interactive public displays has increased including the number of sensitive applications and, hence, the demand for user authentication methods. In this context, gaze-based authentication was shown to be effective and more secure, but significantly slower than touch- or gesture-based methods. We implement a calibration-free and fast authentication method for situated displays based on saccadic eye movements. In a user study (n = 10), we compare our new method with CueAuth from Khamis et al. (IMWUT’18), an authentication method based on smooth pursuit eye movements. The results show a significant improvement in accuracy from 82.94% to 95.88%. At the same time, we found that the entry speed can be increased enormously with our method, on average, 18.28s down to 5.12s, which is comparable to touch-based input."
  },
  {
    "title": "Pinch, Click, or Dwell: Comparing Different Selection Techniques for Eye-Gaze-Based Pointing in Virtual Reality",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: User Interfaces and Interaction",
    "data": "May 2021",
    "authors": [
      "Aunnoy K Mutasim",
      "Anil Ufuk Batmaz",
      "Wolfgang Stuerzlinger"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3457998",
    "citation": "18",
    "abstract": "While a pinch action is gaining popularity for selection of virtual objects in eye-gaze-based systems, it is still unknown how well this method performs compared to other popular alternatives, e.g., a button click or a dwell action. To determine pinch’s performance in terms of execution time, error rate, and throughput, we implemented a Fitts’ law task in Virtual Reality (VR) where the subjects pointed with their (eye-)gaze and selected / activated the targets by pinch, clicking a button, or dwell. Results revealed that although pinch was slower, made more errors, and had less throughput compared to button clicks, none of these differences were significant. Dwell exhibited the least errors but was significantly slower and achieved less throughput compared to the other conditions. Based on these findings, we conclude that the pinch gesture is a reasonable alternative to button clicks for eye-gaze-based VR systems."
  },
  {
    "title": "A Multimodal Eye Movement Dataset and a Multimodal Eye Movement Segmentation Analysis",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Eye Movements and Attention",
    "data": "May 2021",
    "authors": ["Wolfgang Fuhl", "Enkelejda Kasneci"],
    "DOI": "https://doi.org/10.1145/3448018.3458004",
    "citation": "6",
    "abstract": "We present a new dataset with annotated eye movements. The dataset consists of over 800,000 gaze points recorded during a car ride in the real world and in the simulator. In total, the eye movements of 19 subjects were annotated. In this dataset, there are several data sources including the eyelid closure, the pupil center, the optical vector, and a vector into the pupil center starting from the center of the eye corners. These different data sources are analyzed and evaluated individually as well as in combination with respect to their suitability for eye movement classification. These results will help developers of real-time systems and algorithms to find the best data sources for their application. Also, new algorithms can be trained and evaluated on this data set. Link to code and dataset https://atreus.informatik.uni-tuebingen.de/seafile/d/8e2ab8c3fdd444e1a135/?p=%2FA%20Multimodal%20Eye%20Movement%20Dataset%20and%20...&mode=list"
  },
  {
    "title": "55 Rides: attention annotated head and gaze data during naturalistic driving",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Eye Movements and Attention",
    "data": "May 2021",
    "authors": [
      "Thomas C Kübler",
      "Wolfgang Fuhl",
      "Elena Wagner",
      "Enkelejda Kasneci"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3457993",
    "citation": "2",
    "abstract": "Trained eye patterns are essential for safe driving. Whether for exploration of the surrounding traffic or to make sure that a lane is clear through a shoulder check - quick and effective perception is the key to driving safety. Surprisingly though, free and open access data on gaze behavior during driving are yet extremely sparse. The environment inside a vehicle is challenging for eye-tracking technology due to rapidly changing illumination conditions, such as exiting a tunnel to brightest sunlight, proper calibration and safety. So far, available data exhibits environments that likely influence the viewing behavior, sometimes dramatically (e.g., driving simulators without mirrors, limited field of view)."
  },
  {
    "title": "Fixation: A universal framework for experimental eye movement research✱",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Eye Movements and Attention",
    "data": "May 2021",
    "authors": ["Mostafa Elshamy", "Peter Khooshabeh"],
    "DOI": "https://doi.org/10.1145/3448018.3458007",
    "citation": "0",
    "abstract": "We propose a free and open-source framework for experimental eye movement research, whereby a researcher can record and visualize gaze data and export it for analysis using an offline web application without proprietary components or licensing restrictions. The framework is agnostic to the source of the raw gaze stream and can be used with any eye tracking platform by mapping data to a standard json-based format and streaming it in real time. We leverage web technologies to address data privacy concerns and demonstrate support for recording at 300 Hz and real-time visualization."
  },
  {
    "title": "Estimation of Visual Attention using Microsaccades in response to Vibrations in the Peripheral Field of Vision",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Eye Movements and Attention",
    "data": "May 2021",
    "authors": ["Takahiro Ueno", "Minoru Nakayama"],
    "DOI": "https://doi.org/10.1145/3448018.3458012",
    "citation": "2",
    "abstract": "Viewer’s eye movements and behavioural responses were analysed in order to determine the relationship between selective perception and visual attention during a dual detection task in the central and peripheral fields of vision, in order to design a better functioning information display. Changes in visual attention levels were evaluated using the temporal frequency of microsaccades. Accurate rates of stimulus detection response and microsaccade frequency were estimated using a hierarchical Bayesian model. In the results, the dominance of the response in the peripheral field of vision is confirmed. Also, chronological changes in levels of attention and the contribution of these changes to behavioural responses were examined. The relationship between behavioural responses, micorsaccade frequency, and the directional dominance of certain viewing areas in the peripheral field of vision were discussed, in order to evaluate the level of visual attention of viewers."
  },
  {
    "title": "Saccades, attentional orienting and disengagement: the effects of anodal tDCS over right posterior parietal cortex (PPC) and frontal eye field (FEF)",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: Eye Movements and Attention",
    "data": "May 2021",
    "authors": [
      "Lorenzo Diana",
      "Patrick Pilastro",
      "Edoardo N. Aiello",
      "Aleksandra K. Eberhard-Moscicka",
      "René M. Müri",
      "Nadia Bolognini"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3457995",
    "citation": "0",
    "abstract": "In the present work, we applied anodal transcranial direct current stimulation (tDCS) over the posterior parietal cortex (PPC) and frontal eye field (FEF) of the right hemisphere in healthy subjects to modulate attentional orienting and disengagement in a gap-overlap task. Both stimulations led to bilateral improvements in saccadic reaction times (SRTs), with larger effects for gap trials. However, analyses showed that the gap effect was not affected by tDCS. Importantly, we observed significant effects of baseline performance that may mediate side- and task-specific effects of brain stimulation."
  },
  {
    "title": "Minimizing Cognitive Load in Cyber Learning Materials – An Eye Tracking Study",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EYESEC: Workshop on Eye-Gaze for Security Applications",
    "data": "May 2021",
    "authors": [
      "Leon Bernard",
      "Sagar Raina",
      "Blair Taylor",
      "Siddharth Kaza"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3458617",
    "citation": "0",
    "abstract": "Cybersecurity education is critical in addressing the global cyber crisis. However, cybersecurity is inherently complex and teaching cyber can lead to cognitive overload among students. Cognitive load includes: 1) intrinsic load (IL- due to inherent difficulty of the topic), 2) extraneous (EL- due to presentation of material), and 3) germane (GL- due to extra effort put in for learning). The challenge is to minimize IL and EL and maximize GL. We propose a model to develop cybersecurity learning materials that incorporate both the Bloom's taxonomy cognitive framework and the design principles of content segmentation and interactivity. We conducted a randomized control/treatment group study to test the proposed model by measuring cognitive load using two eye-tracking metrics (fixation duration and pupil size) between two cybersecurity learning modalities – 1) segmented and interactive modules, and 2) traditional-without segmentation and interactivity (control). Nineteen computer science majors in a large comprehensive university participated in the study and completed a learning module focused on integer overflow in a popular programming language."
  },
  {
    "title": "Eye-GUAna: Higher Gaze-Based Entropy and Increased Password Space in Graphical User Authentication Through Gamification",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EYESEC: Workshop on Eye-Gaze for Security Applications",
    "data": "May 2021",
    "authors": [
      "Christina Katsini",
      "George E. Raptis",
      "Andrew Jian-lan Cen",
      "Nalin Asanka Gamagedara Arachchilage",
      "Lennart E. Nacke"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3458615",
    "citation": "0",
    "abstract": "Graphical user authentication (GUA) is a common alternative to text-based user authentication, where people are required to draw graphical passwords on background images. Recent research provides evidence that gamification of the graphical password creation process influences people to make less predictable choices. Aiming to understand the underlying reasons from a visual behavior perspective, in this paper, we report a small-scale eye-tracking study that compares the visual behavior developed by people who follow a gamified approach and people who follow a non-gamified approach to make their graphical password choices. The results show that people who follow a gamified approach have higher gaze-based entropy, as they fixate on more image areas and for longer periods, and thus, they have an increased effective password space, which could lead to better and less predictable password choices."
  },
  {
    "title": "Effects of measurement time and presentation size conditions on biometric identification using eye movements",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EYESEC: Workshop on Eye-Gaze for Security Applications",
    "data": "May 2021",
    "authors": ["Yudai Niitsu", "Minoru Nakayama"],
    "DOI": "https://doi.org/10.1145/3448018.3458616",
    "citation": "0",
    "abstract": "Biometric identification using eye movements is an identification method with low risk of spoofing, however the problem with it is that the eye movement measurement time is long. In this paper, we studied pattern lock authentication using eye movement features. As a result of 1-to-N identification using the data of six subjects, it was found that the identification rate was maximized at a measurement time of 3 seconds, indicating that it was possible to identify individuals in a short measurement time. In addition, we examined the effects of the data measurement time conditions and the presentation size on the rate of identification. The condition which maximized the identification rate was a measurement time limit of 3 seconds or the presentation of a stimulus pattern using a visual angle of 27.20°. Furthermore, the Mel-Frequency Cepstral Coefficient (MFCC) of the viewpoint coordinates and the diameter of the pupil were the features that contributed most to identification."
  },
  {
    "title": "EyeTell: Tablet-based Calibration-free Eye-typing using Smooth-pursuit movements",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN Symposium 2021",
    "data": "May 2021",
    "authors": ["Tanya Bafna", "Per Bækgaard", "John Paulin Paulin Hansen"],
    "DOI": "https://doi.org/10.1145/3448018.3458015",
    "citation": "2",
    "abstract": "Gaze tracking technology, with the increasingly robust and lightweight equipment, can have tremendous applications. To use the technology during short interactions, such as in public displays or hospitals to communicate non-verbally after a surgery, the application needs to be intuitive without requiring a calibration. Gaze gestures such as smooth-pursuit eye movements can be detected without calibration. We report the working performance of a calibration-free eye-typing application using only the front-facing camera of a tablet. In a user study with 29 participants, we obtained an average typing speed of 1.27 WPM after four trials and a maximum typing speed of 1.95 WPM."
  },
  {
    "title": "Did you Understand this?: Leveraging Gaze Behavior to Assess Questionnaire Comprehension",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN Symposium 2021",
    "data": "May 2021",
    "authors": [
      "Radiah Rivu",
      "Yasmeen Abdrabou",
      "Yomna Abdelrahman",
      "Ken Pfeuffer",
      "Dagmar Kern",
      "Cornelia Neuert",
      "Daniel Buschek",
      "Florian Alt"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3458018",
    "citation": "1",
    "abstract": "We investigate how problems in understanding text – specifically a word or a sentence – while filling in questionnaires are reflected in gaze behaviour. To identify text comprehension problems, while filling a questionnaire, and their correlation with the gaze features, we collected data from 42 participant. In a follow-up study (N=30), we evoked comprehension problems and features they affect and quantified users’ gaze behaviour. Our findings implies that comprehension problems could be reflected in a set of gaze features, namely, in the number of fixations, duration of fixations, and number of regressions. Our findings not only demonstrate the potential of eye tracking for assessing reading comprehension but also pave the way for researchers and designers to build novel questionnaire tools that instantly mitigate problems in reading comprehension."
  },
  {
    "title": "Multi-user Gaze-based Interaction Techniques on Collaborative Touchscreens",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN Symposium 2021",
    "data": "May 2021",
    "authors": ["Ken Pfeuffer", "Jason Alexander", "Hans Gellersen"],
    "DOI": "https://doi.org/10.1145/3448018.3458016",
    "citation": "1",
    "abstract": "Eye-gaze is a technology for implicit, fast, and hands-free input for a variety of use cases, with the majority of techniques focusing on single-user contexts. In this work, we present an exploration into gaze techniques of users interacting together on the same surface. We explore interaction concepts that exploit two states in an interactive system: 1) users visually attending to the same object in the UI, or 2) users focusing on separate targets. Interfaces can exploit these states with increasing availability of eye-tracking. For example, to dynamically personalise content on the UI to each user, and to provide a merged or compromised view on an object when both users’ gaze are falling upon it. These concepts are explored with a prototype horizontal interface that tracks gaze of two users facing each other. We build three applications that illustrate different mappings of gaze to multi-user support: an indoor map with gaze-highlighted information, an interactive tree-of-life visualisation that dynamically expands on users’ gaze, and a worldmap application with gaze-aware fisheye zooming. We conclude with insights from a public deployment of this system, pointing toward the engaging and seamless ways how eye based input integrates into collaborative interaction."
  },
  {
    "title": "Combining Oculo-motor Indices to Measure Cognitive Load of Synthetic Speech in Noisy Listening Conditions.",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN Symposium 2021",
    "data": "May 2021",
    "authors": ["Mateusz Dubiel", "Minoru Nakayama", "Xin Wang"],
    "DOI": "https://doi.org/10.1145/3448018.3458013",
    "citation": "1",
    "abstract": "Gaze-based assistive technologies (ATs) that feature speech have the potential to improve the life of people with communication disorders. However, due to a limited understanding of how different speech types affect the cognitive load of users, an evaluation of ATs remains a challenge. Expanding on previous work, we combined temporal changes in pupil size and ocular movements (saccades and fixation differentials) to evaluate cognitive workload of two types of speech (natural and synthetic) mixed with noise, through a listening test. While observed pupil sizes were significantly larger at lower signal-to-noise levels, as participants listened and memorised speech stimuli; saccadic eye-movements were significantly more frequent for synthetic speech. In the synthetic condition, there was a strong negative correlation between pupil dilation and fixation differentials, indicating a higher strain on participants’ cognitive resources. These results suggest that combining oculo-motor indices can aid our understanding of the cognitive implications of different speech types."
  },
  {
    "title": "Using Deep Learning to Classify Saccade Direction from Brain Activity",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN Symposium 2021",
    "data": "May 2021",
    "authors": [
      "Ard Kastrati",
      "Martyna Beata Plomecka",
      "Roger Wattenhofer",
      "Nicolas Langer"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3458014",
    "citation": "1",
    "abstract": "We present first insights into our project that aims to develop an Electroencephalography (EEG) based Eye-Tracker. Our approach is tested and validated on a large dataset of simultaneously recorded EEG and infrared video-based Eye-Tracking, serving as ground truth. We compared several state-of-the-art neural network architectures for time series classification: InceptionTime, EEGNet, and investigated other architectures such as convolutional neural networks (CNN) with Xception modules and Pyramidal CNN. We prepared and tested these architectures with our rich dataset and obtained a remarkable accuracy of the left/right saccades direction classification (94.8 %) for the InceptionTime network, after hyperparameter tuning."
  },
  {
    "title": "Feasibility of evaluating temporal changes in cognitive load factors using ocular features",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: COGAIN Symposium 2021",
    "data": "May 2021",
    "authors": ["Tomomi Okano", "Minoru Nakayama"],
    "DOI": "https://doi.org/10.1145/3448018.3458019",
    "citation": "1",
    "abstract": "In this paper, we have focused on microsaccade, pupil diameter and eye movements to discover the relationship between cognitive load. In detail, we have confirmed the relevance of factors of mental workload and oculomotor reactions. Utilizing a visual search task, we measured how eye features change by combining subjective evaluation assessment data. To evaluate the amount of cognitive load gained, we used a systematic evaluation index, NASA-TLX and analyzed with correct rate and reaction time. As a result, we have discovered that oculomotor indices brings correlation relationship on specific cognitive load items."
  },
  {
    "title": "Determining Differences in Reading Behavior Between Experts and Novices by Investigating Eye Movement on Source Code Constructs During a Bug Fixing Task",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EMIP: Eye Movements in Programming",
    "data": "May 2021",
    "authors": ["Salwa Aljehane", "Bonita Sharif", "Jonathan Maletic"],
    "DOI": "https://doi.org/10.1145/3448018.3457424",
    "citation": "3",
    "abstract": "This research compares the eye movement of expert and novice programmers working on a bug fixing task. This comparison aims at investigating which source code elements programmers focus on when they review Java source code. Programmer code reading behaviors at the line and term levels are used to characterize the differences between experts and novices. The study analyzes programmers’ eye movements over identified source code areas using an existing eye tracking dataset of 12 experts and 10 novices. The results show that the difference between experts and novices is significant in source code element coverage. Specifically, novices read more method signatures, variable declarations, identifiers, and keywords compared to experts. However, experts are better at finishing the task using fewer source code elements when compared to novices. Moreover, programmers tend to focus on the method signatures the most while reading the code."
  },
  {
    "title": "A Deeper Analysis of AOI Coverage in Code Reading",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EMIP: Eye Movements in Programming",
    "data": "May 2021",
    "authors": ["Teresa Busjahn", "Sascha Tamm"],
    "DOI": "https://doi.org/10.1145/3448018.3457422",
    "citation": "2",
    "abstract": "The proportion of areas of interest that are covered with gaze is employed as metric to compare natural-language text and source code reading, as well as novice and expert programmers’ code reading behavior. Two levels of abstraction are considered for AOIs: lines and elements. AOI coverage is significantly higher on natural-language text than on code, so a detailed account is provided on the areas that are skipped. Between novice and expert programmers, the overall AOI coverage is comparable. However, segmenting the stimuli into meaningful components revealed that they distribute their gaze differently and partly look at different AOIs. Thus, while programming expertise does not strongly influence AOI coverage quantitatively, it does so qualitatively."
  },
  {
    "title": "Estimation of reading ability of program codes using features of eye movements",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EMIP: Eye Movements in Programming",
    "data": "May 2021",
    "authors": ["Hiroto Harada", "Minoru Nakayama"],
    "DOI": "https://doi.org/10.1145/3448018.3457421",
    "citation": "3",
    "abstract": "A prediction model for code reading ability using eye movement features was developed, and analysed in order to evaluate reader’s level of mastery and provide appropriate support. Sixty-nine features were extracted from eye movements during the reading of two program codes. These codes consisted of three areas of interest (AOIs) that were modules of code which performed 3 functions. Also, code reader’s performance ability was estimated using responses to question surveys and item response theory. The relationships between estimated ability and the metrics of eye movements were generated using a support vector regression technique. Factors of the extracted metrics were analysed. These results confirm the relationship between code comprehension reading behaviour and reading comprehension performance."
  },
  {
    "title": "Eye Tracking Analysis of Code Layout, Crowding and Dyslexia - An Open Data Set",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EMIP: Eye Movements in Programming",
    "data": "May 2021",
    "authors": ["Ian McChesney", "Raymond Bond"],
    "DOI": "https://doi.org/10.1145/3448018.3457420",
    "citation": "2",
    "abstract": "Within computer science there is increasing recognition of the need for research data sets to be openly available to facilitate transparency and reproducibility of studies. In this short paper an open data set is described which contains the eye tracking recordings from an experiment in which programmers with and without dyslexia reviewed and described Java code. The aim of the experiment was to investigate if crowding in code layout affected the gaze behaviour and program comprehension of programmers with dyslexia. The data set provides data from 30 participants (14 dyslexia, 16 control) and their eye gaze behaviour in reviewing three small Java programs in various combinations of crowded and spaced configurations. The key features of the data set are described and observations made on the effect of alternative area of interest configurations. The paper concludes with some observations on enhancing access to data sets through metadata, data provenance and visualizations."
  },
  {
    "title": "EMIP Toolkit: A Python Library for Customized Post-processing of the Eye Movements in Programming Dataset",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EMIP: Eye Movements in Programming",
    "data": "May 2021",
    "authors": [
      "Naser Al Madi",
      "Drew Guarnera",
      "Bonita Sharif",
      "Jonathan Maletic"
    ],
    "DOI": "https://doi.org/10.1145/3448018.3457425",
    "citation": "2",
    "abstract": "The use of eye tracking in the study of program comprehension in software engineering allows researchers to gain a better understanding of the strategies and processes applied by programmers. Despite the large number of eye tracking studies in software engineering, very few datasets are publicly available. The existence of the large Eye Movements in Programming Dataset (EMIP) opens the door for new studies and makes reproducibility of existing research easier. In this paper, a Python library (the EMIP Toolkit) for customized post-processing of the EMIP dataset is presented. The toolkit is specifically designed to make using the EMIP dataset easier and more accessible. It implements features for fixation detection and correction, trial visualization, source code lexical data enrichment, and mapping fixation data over areas of interest. In addition to the toolkit, a filtered token-level dataset with scored recording quality is presented for all Java trials (accounting for 95.8% of the data) in the EMIP dataset."
  },
  {
    "title": "REyeker: Remote Eye Tracker",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: EMIP: Eye Movements in Programming",
    "data": "May 2021",
    "authors": ["Jonas Mucke", "Marc Schwarzkopf", "Janet Siegmund"],
    "DOI": "https://doi.org/10.1145/3448018.3457423",
    "citation": "0",
    "abstract": "Eye tracking allows us to shed light on how developers read and understand source code and how that is linked to cognitive processes. However, studies with eye trackers are usually tied to a laboratory, requiring to observe participants one at a time, which is especially challenging in the current pandemic. To allow for safe and parallel observation, we present our tool REyeker, which allows researchers to observe developers remotely while they understand source code from their own computer without having to directly interact with the experimenter. The original image is blurred to distort text regions and disable legibility, requiring participants to click on areas of interest to deblur them to make them readable. While REyeker naturally can only track eye movements to a limited degree, it allows researchers to get a basic understanding of developers’ reading behavior."
  },
  {
    "title": "What the Eyes Can Tell: Analyzing Visual Attention with an Educational Video Game",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: PLEY: 2nd Eye Tracking in Games and Play Workshop",
    "data": "May 2021",
    "authors": ["Wenyi Lu", "Hao He", "Alex Urban", "Joe Griffin"],
    "DOI": "https://doi.org/10.1145/3448018.3459654",
    "citation": "0",
    "abstract": "3D video games show potential as educational tools that improve learner engagement. Integrating 3D games into school curricula, however, faces various challenges. One challenge is providing visualizations on learning dashboards for instructors. Such dashboards provide needed information so that instructors may conduct timely and appropriate interventions when students need it. Another challenge is identifying contributive learning predictors for a computational model, which can be the core algorithm used to make games more intelligent for tutoring and assessment purposes. Previous studies have found that students' visual-attention is a vital aspect of engagement during gameplay. However, few studies have examined whether attention visualization patterns can distinguish students from different performance groups. Complicating this research is the relatively nascent investigation into gaze metrics for learning-prediction models. In this exploratory study, we used eye-tracking data from an educational game, Mission HydroSci, to examine visual-attention pattern differences between low and high performers and how their self-reported demographics affect such patterns. Results showed different visual-attention patterns between low and high performers. Additionally, self-reported science, gaming, and navigational expertise levels were significantly correlated to several gaze metric features."
  },
  {
    "title": "Analyzing Scanpaths From A Field Dependence-Independence Perspective When Playing A Visual Search Game",
    "conferenceTitle": "ETRA '21 Short Papers: ACM Symposium on Eye Tracking Research and Applications",
    "Session": "SESSION: PLEY: 2nd Eye Tracking in Games and Play Workshop",
    "data": "May 2021",
    "authors": ["George E. Raptis", "Christina Katsini"],
    "DOI": "https://doi.org/10.1145/3448018.3459655",
    "citation": "1",
    "abstract": "People tend to develop different cognitive styles, which influence how we process information when interacting with computer systems. Field Dependence-Independence is one of the most well-known cognitive styles that influences how we process information in visual search tasks. Considering that such tasks are common in video games, this paper investigates whether information processing differences, derived from Field Dependence-Independence cognitive style, reflect on different eye trajectories when playing a visual search game. We performed a small-scale eye-tracking study to investigate it. The results of the scanpath analysis indicated that such differences exist. The study results provide a first step towards understanding how people who share different cognitive styles differ in the scanpaths they develop when playing a visual search game."
  }
]
