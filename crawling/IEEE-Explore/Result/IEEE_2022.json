[
    {
        "title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Haotian Li",
            "Yong Wang",
            "Songheng Zhang",
            "Yangqiu Song",
            "Huamin Qu"
        ],
        "DOI": "10.1109/TVCG.2021.3114863",
        "citation": 36,
        "abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach."
    },
    {
        "title": "Natural Language to Visualization by Neural Machine Translation",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Yuyu Luo",
            "Nan Tang",
            "Guoliang Li",
            "Jiawei Tang",
            "Chengliang Chai",
            "Xuedi Qin"
        ],
        "DOI": "10.1109/TVCG.2021.3114848",
        "citation": 34,
        "abstract": "Supporting the translation from natural language (NL) query to visualization (NL2VIS) can simplify the creation of data visualizations because if successful, anyone can generate visualizations by their natural language from the tabular data. The state-of-the-art NL2VIS approaches (e.g., NL4DV and FlowSense) are based on semantic parsers and heuristic algorithms, which are not end-to-end and are not designed for supporting (possibly) complex data transformations. Deep neural network powered neural machine translation models have made great strides in many machine translation tasks, which suggests that they might be viable for NL2VIS as well. In this paper, we present ncNet, a Transformer-based sequence-to-sequence model for supporting NL2VIS, with several novel visualization-aware optimizations, including using attention-forcing to optimize the learning process, and visualization-aware rendering to produce better visualization results. To enhance the capability of machine to comprehend natural language queries, ncNet is also designed to take an optional chart template (e.g., a pie chart or a scatter plot) as an additional input, where the chart template will be served as a constraint to limit what could be visualized. We conducted both quantitative evaluation and user study, showing that ncNet achieves good accuracy in the nvBench benchmark and is easy-to-use."
    },
    {
        "title": "What's the Situation with Situated Visualization? A Survey and Perspectives on Situatedness",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Nathalie Bressa",
            "Henrik Korsgaard",
            "Aurélien Tabard",
            "Steven Houben",
            "Jo Vermeulen"
        ],
        "DOI": "10.1109/TVCG.2021.3114835",
        "citation": 33,
        "abstract": "Situated visualization is an emerging concept within visualization, in which data is visualized in situ, where it is relevant to people. The concept has gained interest from multiple research communities, including visualization, human-computer interaction (HCI) and augmented reality. This has led to a range of explorations and applications of the concept, however, this early work has focused on the operational aspect of situatedness leading to inconsistent adoption of the concept and terminology. First, we contribute a literature survey in which we analyze 44 papers that explicitly use the term “situated visualization” to provide an overview of the research area, how it defines situated visualization, common application areas and technology used, as well as type of data and type of visualizations. Our survey shows that research on situated visualization has focused on technology-centric approaches that foreground a spatial understanding of situatedness. Secondly, we contribute five perspectives on situatedness (space, time, place, activity, and community) that together expand on the prevalent notion of situatedness in the corpus. We draw from six case studies and prior theoretical developments in HCI. Each perspective develops a generative way of looking at and working with situatedness in design and research. We outline future directions, including considering technology, material and aesthetics, leveraging the perspectives for design, and methods for stronger engagement with target audiences. We conclude with opportunities to consolidate situated visualization research."
    },
    {
        "title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Xiangtong Chu",
            "Xiao Xie",
            "Shuainan Ye",
            "Haolin Lu",
            "Hongguang Xiao",
            "Zeqing Yuan",
            "Zhutian Chen",
            "Hui Zhang",
            "Yingcai Wu"
        ],
        "DOI": "10.1109/TVCG.2021.3114861",
        "citation": 32,
        "abstract": "Tactic analysis is a major issue in badminton as the effective usage of tactics is the key to win. The tactic in badminton is defined as a sequence of consecutive strokes. Most existing methods use statistical models to find sequential patterns of strokes and apply 2D visualizations such as glyphs and statistical charts to explore and analyze the discovered patterns. However, in badminton, spatial information like the shuttle trajectory, which is inherently 3D, is the core of a tactic. The lack of sufficient spatial awareness in 2D visualizations largely limited the tactic analysis of badminton. In this work, we collaborate with domain experts to study the tactic analysis of badminton in a 3D environment and propose an immersive visual analytics system, TIVEE, to assist users in exploring and explaining badminton tactics from multi-levels. Users can first explore various tactics from the third-person perspective using an unfolded visual presentation of stroke sequences. By selecting a tactic of interest, users can turn to the first-person perspective to perceive the detailed kinematic characteristics and explain its effects on the game result. The effectiveness and usefulness of TIVEE are demonstrated by case studies and an expert interview."
    },
    {
        "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Xingbo Wang",
            "Jianben He",
            "Zhihua Jin",
            "Muqiao Yang",
            "Yong Wang",
            "Huamin Qu"
        ],
        "DOI": "10.1109/TVCG.2021.3114794",
        "citation": 24,
        "abstract": "Multimodal sentiment analysis aims to recognize people's attitudes from multiple communication channels such as verbal content (i.e., text), voice, and facial expressions. It has become a vibrant and important research topic in natural language processing. Much research focuses on modeling the complex intra- and inter-modal interactions between different communication channels. However, current multimodal models with strong performance are often deep-learning-based techniques and work like black boxes. It is not clear how models utilize multimodal information for sentiment predictions. Despite recent advances in techniques for enhancing the explainability of machine learning models, they often target unimodal scenarios (e.g., images, sentences), and little research has been done on explaining multimodal models. In this paper, we present an interactive visual analytics system, M2 Lens, to visualize and explain multimodal models for sentiment analysis. M2 Lens provides explanations on intra- and inter-modal interactions at the global, subset, and local levels. Specifically, it summarizes the influence of three typical interaction types (i.e., dominance, complement, and conflict) on the model predictions. Moreover, M2 Lens identifies frequent and influential multimodal features and supports the multi-faceted exploration of model behaviors from language, acoustic, and visual modalities. Through two case studies and expert interviews, we demonstrate our system can help users gain deep insights into the multimodal models for sentiment analysis."
    },
    {
        "title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Jiang Wu",
            "Dongyu Liu",
            "Ziyang Guo",
            "Qingyang Xu",
            "Yingcai Wu"
        ],
        "DOI": "10.1109/TVCG.2021.3114832",
        "citation": 24,
        "abstract": "Event sequence mining is often used to summarize patterns from hundreds of sequences but faces special challenges when handling racket sports data. In racket sports (e.g., tennis and badminton), a player hitting the ball is considered a multivariate event consisting of multiple attributes (e.g., hit technique and ball position). A rally (i.e., a series of consecutive hits beginning with one player serving the ball and ending with one player winning a point) thereby can be viewed as a multivariate event sequence. Mining frequent patterns and depicting how patterns change over time is instructive and meaningful to players who want to learn more short-term competitive strategies (i.e., tactics) that encompass multiple hits. However, players in racket sports usually change their tactics rapidly according to the opponent's reaction, resulting in ever-changing tactic progression. In this work, we introduce a tailored visualization system built on a novel multivariate sequence pattern mining algorithm to facilitate explorative identification and analysis of various tactics and tactic progression. The algorithm can mine multiple non-overlapping multivariate patterns from hundreds of sequences effectively. Based on the mined results, we propose a glyph-based Sankey diagram to visualize the ever-changing tactic progression and support interactive data exploration. Through two case studies with four domain experts in tennis and badminton, we demonstrate that our system can effectively obtain insights about tactic progression in most racket sports. We further discuss the strengths and the limitations of our system based on domain experts' feedback."
    },
    {
        "title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Alan Lundgard",
            "Arvind Satyanarayan"
        ],
        "DOI": "10.1109/TVCG.2021.3114770",
        "citation": 23,
        "abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization."
    },
    {
        "title": "Revisiting Dimensionality Reduction Techniques for Visual Cluster Analysis: An Empirical Study",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Jiazhi Xia",
            "Yuchen Zhang",
            "Jie Song",
            "Yang Chen",
            "Yunhai Wang",
            "Shixia Liu"
        ],
        "DOI": "10.1109/TVCG.2021.3114694",
        "citation": 23,
        "abstract": "Dimensionality Reduction (DR) techniques can generate 2D projections and enable visual exploration of cluster structures of high-dimensional datasets. However, different DR techniques would yield various patterns, which significantly affect the performance of visual cluster analysis tasks. We present the results of a user study that investigates the influence of different DR techniques on visual cluster analysis. Our study focuses on the most concerned property types, namely the linearity and locality, and evaluates twelve representative DR techniques that cover the concerned properties. Four controlled experiments were conducted to evaluate how the DR techniques facilitate the tasks of 1) cluster identification, 2) membership identification, 3) distance comparison, and 4) density comparison, respectively. We also evaluated users' subjective preference of the DR techniques regarding the quality of projected clusters. The results show that: 1) Non-linear and Local techniques are preferred in cluster identification and membership identification; 2) Linear techniques perform better than non-linear techniques in density comparison; 3) UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-Distributed Stochastic Neighbor Embedding) perform the best in cluster identification and membership identification; 4) NMF (Nonnegative Matrix Factorization) has competitive performance in distance comparison; 5) t-SNLE (t-Distributed Stochastic Neighbor Linear Embedding) has competitive performance in density comparison."
    },
    {
        "title": "STNet: An End-to-End Generative Framework for Synthesizing Spatiotemporal Super-Resolution Volumes",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Jun Han",
            "Hao Zheng",
            "Danny Z. Chen",
            "Chaoli Wang"
        ],
        "DOI": "10.1109/TVCG.2021.3114815",
        "citation": 22,
        "abstract": "We present STNet, an end-to-end generative framework that synthesizes spatiotemporal super-resolution volumes with high fidelity for time-varying data. STNet includes two modules: a generator and a spatiotemporal discriminator. The input to the generator is two low-resolution volumes at both ends, and the output is the intermediate and the two-ending spatiotemporal super-resolution volumes. The spatiotemporal discriminator, leveraging convolutional long short-term memory, accepts a spatiotemporal super-resolution sequence as input and predicts a conditional score for each volume based on its spatial (the volume itself) and temporal (the previous volumes) information. We propose an unsupervised pre-training stage using cycle loss to improve the generalization of STNet. Once trained, STNet can generate spatiotemporal super-resolution volumes from low-resolution ones, offering scientists an option to save data storage (i.e., sparsely sampling the simulation output in both spatial and temporal dimensions). We compare STNet with the baseline bicubic+linear interpolation, two deep learning solutions ($\\mathsf{SSR}+\\mathsf{TSF}$, STD), and a state-of-the-art tensor compression solution (TTHRESH) to show the effectiveness of STNet."
    },
    {
        "title": "Compass: Towards Better Causal Analysis of Urban Time Series",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Zikun Deng",
            "Di Weng",
            "Xiao Xie",
            "Jie Bao",
            "Yu Zheng",
            "Mingliang Xu",
            "Wei Chen",
            "Yingcai Wu"
        ],
        "DOI": "10.1109/TVCG.2021.3114875",
        "citation": 21,
        "abstract": "The spatial time series generated by city sensors allow us to observe urban phenomena like environmental pollution and traffic congestion at an unprecedented scale. However, recovering causal relations from these observations to explain the sources of urban phenomena remains a challenging task because these causal relations tend to be time-varying and demand proper time series partitioning for effective analyses. The prior approaches extract one causal graph given long-time observations, which cannot be directly applied to capturing, interpreting, and validating dynamic urban causality. This paper presents Compass, a novel visual analytics approach for in-depth analyses of the dynamic causality in urban time series. To develop Compass, we identify and address three challenges: detecting urban causality, interpreting dynamic causal relations, and unveiling suspicious causal relations. First, multiple causal graphs over time among urban time series are obtained with a causal detection framework extended from the Granger causality test. Then, a dynamic causal graph visualization is designed to reveal the time-varying causal relations across these causal graphs and facilitate the exploration of the graphs along the time. Finally, a tailored multi-dimensional visualization is developed to support the identification of spurious causal relations, thereby improving the reliability of causal analyses. The effectiveness of Compass is evaluated with two case studies conducted on the real-world urban datasets, including the air pollution and traffic speed datasets, and positive feedback was received from domain experts."
    },
    {
        "title": "Augmenting Sports Videos with VisCommentator",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Zhutian Chen",
            "Shuainan Ye",
            "Xiangtong Chu",
            "Haijun Xia",
            "Hui Zhang",
            "Huamin Qu",
            "Yingcai Wu"
        ],
        "DOI": "10.1109/TVCG.2021.3114806",
        "citation": 20,
        "abstract": "Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level (what the constituents are) and clip-level (how those constituents are organized). We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by selecting the data to visualize instead of manually drawing the graphical marks. Our system can be generalized to other racket sports (e.g., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities."
    },
    {
        "title": "Gosling: A Grammar-based Toolkit for Scalable and Interactive Genomics Data Visualization",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Sehi LYi",
            "Qianwen Wang",
            "Fritz Lekschas",
            "Nils Gehlenborg"
        ],
        "DOI": "10.1109/TVCG.2021.3114876",
        "citation": 20,
        "abstract": "The combination of diverse data types and analysis tasks in genomics has resulted in the development of a wide range of visualization techniques and tools. However, most existing tools are tailored to a specific problem or data type and offer limited customization, making it challenging to optimize visualizations for new analysis tasks or datasets. To address this challenge, we designed Gosling-a grammar for interactive and scalable genomics data visualization. Gosling balances expressiveness for comprehensive multi-scale genomics data visualizations with accessibility for domain scientists. Our accompanying JavaScript toolkit called Gosling.js provides scalable and interactive rendering. Gosling.js is built on top of an existing platform for web-based genomics data visualization to further simplify the visualization of common genomics data formats. We demonstrate the expressiveness of the grammar through a variety of real-world examples. Furthermore, we show how Gosling supports the design of novel genomics visualizations. An online editor and examples of Gosling.js, its source code, and documentation are available at https://gosling.js.org."
    },
    {
        "title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Joscha Eirich",
            "Jakob Bonart",
            "Dominik Jäckle",
            "Michael Sedlmair",
            "Ute Schmid",
            "Kai Fischbach",
            "Tobias Schreck",
            "Jürgen Bernard"
        ],
        "DOI": "10.1109/TVCG.2021.3114797",
        "citation": 20,
        "abstract": "In this design study, we present IRVINE, a Visual Analytics (VA) system, which facilitates the analysis of acoustic data to detect and understand previously unknown errors in the manufacturing of electrical engines. In serial manufacturing processes, signatures from acoustic data provide valuable information on how the relationship between multiple produced engines serves to detect and understand previously unknown errors. To analyze such signatures, IRVINE leverages interactive clustering and data labeling techniques, allowing users to analyze clusters of engines with similar signatures, drill down to groups of engines, and select an engine of interest. Furthermore, IRVINE allows to assign labels to engines and clusters and annotate the cause of an error in the acoustic raw measurement of an engine. Since labels and annotations represent valuable knowledge, they are conserved in a knowledge database to be available for other stakeholders. We contribute a design study, where we developed IRVINE in four main iterations with engineers from a company in the automotive sector. To validate IRVINE, we conducted a field study with six domain experts. Our results suggest a high usability and usefulness of IRVINE as part of the improvement of a real-world manufacturing process. Specifically, with IRVINE domain experts were able to label and annotate produced electrical engines more than 30% faster."
    },
    {
        "title": "Untidy Data: The Unreasonable Effectiveness of Tables",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Lyn Bartram",
            "Michael Correll",
            "Melanie Tory"
        ],
        "DOI": "10.1109/TVCG.2021.3114830",
        "citation": 18,
        "abstract": "Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets — the quintessential table tool — remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers [61], people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and “get their hands on” the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools."
    },
    {
        "title": "Human-in-the-loop Extraction of Interpretable Concepts in Deep Learning Models",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Zhenge Zhao",
            "Panpan Xu",
            "Carlos Scheidegger",
            "Liu Ren"
        ],
        "DOI": "10.1109/TVCG.2021.3114837",
        "citation": 17,
        "abstract": "The interpretation of deep neural networks (DNNs) has become a key topic as more and more people apply them to solve various problems and making critical decisions. Concept-based explanations have recently become a popular approach for post-hoc interpretation of DNNs. However, identifying human-understandable visual concepts that affect model decisions is a challenging task that is not easily addressed with automatic approaches. We present a novel human-in-the-Ioop approach to generate user-defined concepts for model interpretation and diagnostics. Central to our proposal is the use of active learning, where human knowledge and feedback are combined to train a concept extractor with very little human labeling effort. We integrate this process into an interactive system, ConceptExtract. Through two case studies, we show how our approach helps analyze model behavior and extract human-friendly concepts for different machine learning tasks and datasets and how to use these concepts to understand the predictions, compare model performance and make suggestions for model refinement. Quantitative experiments show that our active learning approach can accurately extract meaningful visual concepts. More importantly, by identifying visual concepts that negatively affect model performance, we develop the corresponding data augmentation strategy that consistently improves model performance."
    },
    {
        "title": "Communicating Visualizations without Visuals: Investigation of Visualization Alternative Text for People with Visual Impairments",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Crescentia Jung",
            "Shubham Mehta",
            "Atharva Kulkarni",
            "Yuhang Zhao",
            "Yea-Seul Kim"
        ],
        "DOI": "10.1109/TVCG.2021.3114846",
        "citation": 17,
        "abstract": "Alternative text is critical in communicating graphics to people who are blind or have low vision. Especially for graphics that contain rich information, such as visualizations, poorly written or an absence of alternative texts can worsen the information access inequality for people with visual impairments. In this work, we consolidate existing guidelines and survey current practices to inspect to what extent current practices and recommendations are aligned. Then, to gain more insight into what people want in visualization alternative texts, we interviewed 22 people with visual impairments regarding their experience with visualizations and their information needs in alternative texts. The study findings suggest that participants actively try to construct an image of visualizations in their head while listening to alternative texts and wish to carry out visualization tasks (e.g., retrieve specific values) as sighted viewers would. The study also provides ample support for the need to reference the underlying data instead of visual elements to reduce users' cognitive burden. Informed by the study, we provide a set of recommendations to compose an informative alternative text."
    },
    {
        "title": "KD-Box: Line-segment-based KD-tree for Interactive Exploration of Large-scale Time-Series Data",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Yue Zhao",
            "Yunhai Wang",
            "Jian Zhang",
            "Chi-Wing Fu",
            "Mingliang Xu",
            "Dominik Moritz"
        ],
        "DOI": "10.1109/TVCG.2021.3114865",
        "citation": 16,
        "abstract": "Time-series data-usually presented in the form of lines-plays an important role in many domains such as finance, meteorology, health, and urban informatics. Yet, little has been done to support interactive exploration of large-scale time-series data, which requires a clutter-free visual representation with low-latency interactions. In this paper, we contribute a novel line-segment-based KD-tree method to enable interactive analysis of many time series. Our method enables not only fast queries over time series in selected regions of interest but also a line splatting method for efficient computation of the density field and selection of representative lines. Further, we develop KD-Box, an interactive system that provides rich interactions, e.g., timebox, attribute filtering, and coordinated multiple views. We demonstrate the effectiveness of KD-Box in supporting efficient line query and density field computation through a quantitative comparison and show its usefulness for interactive visual analysis on several real-world datasets."
    },
    {
        "title": "VBridge: Connecting the Dots Between Features and Data to Explain Healthcare Models",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Furui Cheng",
            "Dongyu Liu",
            "Fan Du",
            "Yanna Lin",
            "Alexandra Zytek",
            "Haomin Li",
            "Huamin Qu",
            "Kalyan Veeramachaneni"
        ],
        "DOI": "10.1109/TVCG.2021.3114836",
        "citation": 16,
        "abstract": "Machine learning (ML) is increasingly applied to Electronic Health Records (EHRs) to solve clinical prediction tasks. Although many ML models perform promisingly, issues with model transparency and interpretability limit their adoption in clinical practice. Directly using existing explainable ML techniques in clinical settings can be challenging. Through literature surveys and collaborations with six clinicians with an average of 17 years of clinical experience, we identified three key challenges, including clinicians' unfamiliarity with ML features, lack of contextual information, and the need for cohort-level evidence. Following an iterative design process, we further designed and developed VBridge, a visual analytics tool that seamlessly incorporates ML explanations into clinicians' decision-making workflow. The system includes a novel hierarchical display of contribution-based feature explanations and enriched interactions that connect the dots between ML features, explanations, and data. We demonstrated the effectiveness of VBridge through two case studies and expert interviews with four clinicians, showing that visually associating model explanations with patients' situational records can help clinicians better interpret and use model predictions when making clinician decisions. We further derived a list of design implications for developing future explainable ML tools to support clinical decision-making."
    },
    {
        "title": "Interactive Dimensionality Reduction for Comparative Analysis",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Takanori Fujiwara",
            "Xinhai Wei",
            "Jian Zhao",
            "Kwan-Liu Ma"
        ],
        "DOI": "10.1109/TVCG.2021.3114807",
        "citation": 16,
        "abstract": "Finding the similarities and differences between groups of datasets is a fundamental analysis task. For high-dimensional data, dimensionality reduction (DR) methods are often used to find the characteristics of each group. However, existing DR methods provide limited capability and flexibility for such comparative analysis as each method is designed only for a narrow analysis target, such as identifying factors that most differentiate groups. This paper presents an interactive DR framework where we integrate our new DR method, called ULCA (unified linear comparative analysis), with an interactive visual interface. ULCA unifies two DR schemes, discriminant analysis and contrastive learning, to support various comparative analysis tasks. To provide flexibility for comparative analysis, we develop an optimization algorithm that enables analysts to interactively refine ULCA results. Additionally, the interactive visualization interface facilitates interpretation and refinement of the ULCA results. We evaluate ULCA and the optimization algorithm to show their efficiency as well as present multiple case studies using real-world datasets to demonstrate the usefulness of this framework."
    },
    {
        "title": "A Critical Reflection on Visualization Research: Where Do Decision Making Tasks Hide?",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Evanthia Dimara",
            "John Stasko"
        ],
        "DOI": "10.1109/TVCG.2021.3114813",
        "citation": 15,
        "abstract": "It has been widely suggested that a key goal of visualization systems is to assist decision making, but is this true? We conduct a critical investigation on whether the activity of decision making is indeed central to the visualization domain. By approaching decision making as a user task, we explore the degree to which decision tasks are evident in visualization research and user studies. Our analysis suggests that decision tasks are not commonly found in current visualization task taxonomies and that the visualization field has yet to leverage guidance from decision theory domains on how to study such tasks. We further found that the majority of visualizations addressing decision making were not evaluated based on their ability to assist decision tasks. Finally, to help expand the impact of visual analytics in organizational as well as casual decision making activities, we initiate a research agenda on how decision making assistance could be elevated throughout visualization research."
    },
    {
        "title": "Kori: Interactive Synthesis of Text and Charts in Data Documents",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Shahid Latif",
            "Zheng Zhou",
            "Yoon Kim",
            "Fabian Beck",
            "Nam Wook Kim"
        ],
        "DOI": "10.1109/TVCG.2021.3114802",
        "citation": 15,
        "abstract": "Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents."
    },
    {
        "title": "Where Can We Help? A Visual Analytics Approach to Diagnosing and Improving Semantic Segmentation of Movable Objects",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Wenbin He",
            "Lincan Zou",
            "Arvind Kumar Shekar",
            "Liang Gou",
            "Liu Ren"
        ],
        "DOI": "10.1109/TVCG.2021.3114855",
        "citation": 14,
        "abstract": "Semantic segmentation is a critical component in autonomous driving and has to be thoroughly evaluated due to safety concerns. Deep neural network (DNN) based semantic segmentation models are widely used in autonomous driving. However, it is challenging to evaluate DNN-based models due to their black-box-like nature, and it is even more difficult to assess model performance for crucial objects, such as lost cargos and pedestrians, in autonomous driving applications. In this work, we propose VASS, a Visual Analytics approach to diagnosing and improving the accuracy and robustness of Semantic Segmentation models, especially for critical objects moving in various driving scenes. The key component of our approach is a context-aware spatial representation learning that extracts important spatial information of objects, such as position, size, and aspect ratio, with respect to given scene contexts. Based on this spatial representation, we first use it to create visual summarization to analyze models' performance. We then use it to guide the generation of adversarial examples to evaluate models' spatial robustness and obtain actionable insights. We demonstrate the effectiveness of VASS via two case studies of lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative evaluation on the improvement of models' performance with actionable insights obtained from VASS."
    },
    {
        "title": "GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Lu Ying",
            "Tan Tang",
            "Yuzhe Luo",
            "Lvkeshen Shen",
            "Xiao Xie",
            "Lingyun Yu",
            "Yingcai Wu"
        ],
        "DOI": "10.1109/TVCG.2021.3114877",
        "citation": 14,
        "abstract": "Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews."
    },
    {
        "title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Aoyu Wu",
            "Yun Wang",
            "Mengyu Zhou",
            "Xinyi He",
            "Haidong Zhang",
            "Huamin Qu",
            "Dongmei Zhang"
        ],
        "DOI": "10.1109/TVCG.2021.3114826",
        "citation": 14,
        "abstract": "We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system."
    },
    {
        "title": "An Evaluation-Focused Framework for Visualization Recommendation Algorithms",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Zehua Zeng",
            "Phoebe Moh",
            "Fan Du",
            "Jane Hoffswell",
            "Tak Yeon Lee",
            "Sana Malik",
            "Eunyee Koh",
            "Leilani Battle"
        ],
        "DOI": "10.1109/TVCG.2021.3114814",
        "citation": 14,
        "abstract": "Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an evaluation perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios."
    },
    {
        "title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Yifang Wang",
            "Tai-Quan Peng",
            "Huihua Lu",
            "Haoren Wang",
            "Xiao Xie",
            "Huamin Qu",
            "Yingcai Wu"
        ],
        "DOI": "10.1109/TVCG.2021.3114790",
        "citation": 14,
        "abstract": "How to achieve academic career success has been a long-standing research question in social science research. With the growing availability of large-scale well-documented academic profiles and career trajectories, scholarly interest in career success has been reinvigorated, which has emerged to be an active research domain called the Science of Science (i.e., SciSci). In this study, we adopt an innovative dynamic perspective to examine how individual and social factors will influence career success over time. We propose ACSeeker, an interactive visual analytics approach to explore the potential factors of success and how the influence of multiple factors changes at different stages of academic careers. We first applied a Multi-factor Impact Analysis framework to estimate the effect of different factors on academic career success over time. We then developed a visual analytics system to understand the dynamic effects interactively. A novel timeline is designed to reveal and compare the factor impacts based on the whole population. A customized career line showing the individual career development is provided to allow a detailed inspection. To validate the effectiveness and usability of ACSeeker, we report two case studies and interviews with a social scientist and general researchers."
    },
    {
        "title": "VizLinter: A Linter and Fixer Framework for Data Visualization",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Qing Chen",
            "Fuling Sun",
            "Xinyue Xu",
            "Zui Chen",
            "Jiazhe Wang",
            "Nan Cao"
        ],
        "DOI": "10.1109/TVCG.2021.3114804",
        "citation": 14,
        "abstract": "Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations."
    },
    {
        "title": "A Design Space for Applying the Freytag's Pyramid Structure to Data Stories",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Leni Yang",
            "Xian Xu",
            "XingYu Lan",
            "Ziyan Liu",
            "Shunan Guo",
            "Yang Shi",
            "Huamin Qu",
            "Nan Cao"
        ],
        "DOI": "10.1109/TVCG.2021.3114774",
        "citation": 14,
        "abstract": "Data stories integrate compelling visual content to communicate data insights in the form of narratives. The narrative structure of a data story serves as the backbone that determines its expressiveness, and it can largely influence how audiences perceive the insights. Freytag's Pyramid is a classic narrative structure that has been widely used in film and literature. While there are continuous recommendations and discussions about applying Freytag's Pyramid to data stories, little systematic and practical guidance is available on how to use Freytag's Pyramid for creating structured data stories. To bridge this gap, we examined how existing practices apply Freytag's Pyramid by analyzing stories extracted from 103 data videos. Based on our findings, we proposed a design space of narrative patterns, data flows, and visual communications to provide practical guidance on achieving narrative intents, organizing data facts, and selecting visual design techniques through story creation. We evaluated the proposed design space through a workshop with 25 participants. Results show that our design space provides a clear framework for rapid storyboarding of data stories with Freytag's Pyramid."
    },
    {
        "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Mathieu Pont",
            "Jules Vidal",
            "Julie Delon",
            "Julien Tierny"
        ],
        "DOI": "10.1109/TVCG.2021.3114839",
        "citation": 14,
        "abstract": "This paper presents a unified computational framework for the estimation of distances, geodesics and barycenters of merge trees. We extend recent work on the edit distance [104] and introduce a new metric, called the Wasserstein distance between merge trees, which is purposely designed to enable efficient computations of geodesics and barycenters. Specifically, our new distance is strictly equivalent to the $L$2-Wasserstein distance between extremum persistence diagrams, but it is restricted to a smaller solution space, namely, the space of rooted partial isomorphisms between branch decomposition trees. This enables a simple extension of existing optimization frameworks [110] for geodesics and barycenters from persistence diagrams to merge trees. We introduce a task-based algorithm which can be generically applied to distance, geodesic, barycenter or cluster computation. The task-based nature of our approach enables further accelerations with shared-memory parallelism. Extensive experiments on public ensembles and SciVis contest benchmarks demonstrate the efficiency of our approach - with barycenter computations in the orders of minutes for the largest examples - as well as its qualitative ability to generate representative barycenter merge trees, visually summarizing the features of interest found in the ensemble. We show the utility of our contributions with dedicated visualization applications: feature tracking, temporal reduction and ensemble clustering. We provide a lightweight C++ implementation that can be used to reproduce our results."
    },
    {
        "title": "COVID-view: Diagnosis of COVID-19 using Chest CT",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Shreeraj Jadhav",
            "Gaofeng Deng",
            "Marlene Zawin",
            "Arie E. Kaufman"
        ],
        "DOI": "10.1109/TVCG.2021.3114851",
        "citation": 13,
        "abstract": "Significant work has been done towards deep learning (DL) models for automatic lung and lesion segmentation and classification of COVID-19 on chest CT data. However, comprehensive visualization systems focused on supporting the dual visual+DL diagnosis of COVID-19 are non-existent. We present COVID-view, a visualization application specially tailored for radiologists to diagnose COVID-19 from chest CT data. The system incorporates a complete pipeline of automatic lungs segmentation, localization/isolation of lung abnormalities, followed by visualization, visual and DL analysis, and measurement/quantification tools. Our system combines the traditional 2D workflow of radiologists with newer 2D and 3D visualization techniques with DL support for a more comprehensive diagnosis. COVID-view incorporates a novel DL model for classifying the patients into positive/negative COVID-19 cases, which acts as a reading aid for the radiologist using COVID-view and provides the attention heatmap as an explainable DL for the model output. We designed and evaluated COVID-view through suggestions, close feedback and conducting case studies of real-world patient data by expert radiologists who have substantial experience diagnosing chest CT scans for COVID-19, pulmonary embolism, and other forms of lung infections. We present requirements and task analysis for the diagnosis of COVID-19 that motivate our design choices and results in a practical system which is capable of handling real-world patient cases."
    },
    {
        "title": "Examining Effort in 1D Uncertainty Communication Using Individual Differences in Working Memory and NASA-TLX",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Spencer C. Castro",
            "P. Samuel Quinan",
            "Helia Hosseinpour",
            "Lace Padilla"
        ],
        "DOI": "10.1109/TVCG.2021.3114803",
        "citation": 13,
        "abstract": "As uncertainty visualizations for general audiences become increasingly common, designers must understand the full impact of uncertainty communication techniques on viewers' decision processes. Prior work demonstrates mixed performance outcomes with respect to how individuals make decisions using various visual and textual depictions of uncertainty. Part of the inconsistency across findings may be due to an over-reliance on task accuracy, which cannot, on its own, provide a comprehensive understanding of how uncertainty visualization techniques support reasoning processes. In this work, we advance the debate surrounding the efficacy of modern 1D uncertainty visualizations by conducting converging quantitative and qualitative analyses of both the effort and strategies used by individuals when provided with quantile dotplots, density plots, interval plots, mean plots, and textual descriptions of uncertainty. We utilize two approaches for examining effort across uncertainty communication techniques: a measure of individual differences in working-memory capacity known as an operation span (OSPAN) task and self-reports of perceived workload via the NASA-TLX. The results reveal that both visualization methods and working-memory capacity impact participants' decisions. Specifically, quantile dotplots and density plots (i.e., distributional annotations) result in more accurate judgments than interval plots, textual descriptions of uncertainty, and mean plots (i.e., summary annotations). Additionally, participants' open-ended responses suggest that individuals viewing distributional annotations are more likely to employ a strategy that explicitly incorporates uncertainty into their judgments than those viewing summary annotations. When comparing quantile dotplots to density plots, this work finds that both methods are equally effective for low-working-memory individuals. However, for individuals with high-working-memory capacity, quantile dotplots evoke more accurate responses with less perceived effort. Given these results, we advocate for the inclusion of converging behavioral and subjective workload metrics in addition to accuracy performance to further disambiguate meaningful differences among visualization techniques."
    },
    {
        "title": "Context Matters: A Theory of Semantic Discriminability for Perceptual Encoding Systems",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Kushin Mukherjee",
            "Brian Yin",
            "Brianne E. Sherman",
            "Laurent Lessard",
            "Karen B. Schloss"
        ],
        "DOI": "10.1109/TVCG.2021.3114780",
        "citation": 13,
        "abstract": "People's associations between colors and concepts influence their ability to interpret the meanings of colors in information visualizations. Previous work has suggested such effects are limited to concepts that have strong, specific associations with colors. However, although a concept may not be strongly associated with any colors, its mapping can be disambiguated in the context of other concepts in an encoding system. We articulate this view in semantic discriminability theory, a general framework for understanding conditions determining when people can infer meaning from perceptual features. Semantic discriminability is the degree to which observers can infer a unique mapping between visual features and concepts. Semantic discriminability theory posits that the capacity for semantic discriminability for a set of concepts is constrained by the difference between the feature-concept association distributions across the concepts in the set. We define formal properties of this theory and test its implications in two experiments. The results show that the capacity to produce semantically discriminable colors for sets of concepts was indeed constrained by the statistical distance between color-concept association distributions (Experiment 1). Moreover, people could interpret meanings of colors in bar graphs insofar as the colors were semantically discriminable, even for concepts previously considered “non-colorable” (Experiment 2). The results suggest that colors are more robust for visual communication than previously thought."
    },
    {
        "title": "Differentiable Direct Volume Rendering",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Sebastian Weiss",
            "Rüdiger Westermann"
        ],
        "DOI": "10.1109/TVCG.2021.3114769",
        "citation": 13,
        "abstract": "We present a differentiable volume rendering solution that provides differentiability of all continuous parameters of the volume rendering process. This differentiable renderer is used to steer the parameters towards a setting with an optimal solution of a problem-specific objective function. We have tailored the approach to volume rendering by enforcing a constant memory footprint via analytic inversion of the blending functions. This makes it independent of the number of sampling steps through the volume and facilitates the consideration of small-scale changes. The approach forms the basis for automatic optimizations regarding external parameters of the rendering process and the volumetric density field itself. We demonstrate its use for automatic viewpoint selection using differentiable entropy as objective, and for optimizing a transfer function from rendered images of a given volume. Optimization of per-voxel densities is addressed in two different ways: First, we mimic inverse tomography and optimize a 3D density field from images using an absorption model. This simplification enables comparisons with algebraic reconstruction techniques and state-of-the-art differentiable path tracers. Second, we introduce a novel approach for tomographic reconstruction from images using an emission-absorption model with post-shading via an arbitrary transfer function."
    },
    {
        "title": "Towards Visual Explainable Active Learning for Zero-Shot Classification",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Shichao Jia",
            "Zeyu Li",
            "Nuo Chen",
            "Jiawan Zhang"
        ],
        "DOI": "10.1109/TVCG.2021.3114793",
        "citation": 12,
        "abstract": "Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification."
    },
    {
        "title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Carla Floricel",
            "Nafiul Nipu",
            "Mikayla Biggs",
            "Andrew Wentzel",
            "Guadalupe Canahuate",
            "Lisanne Van Dijk",
            "Abdallah Mohamed",
            "C.David Fuller",
            "G.Elisabeta Marai"
        ],
        "DOI": "10.1109/TVCG.2021.3114810",
        "citation": 12,
        "abstract": "Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research."
    },
    {
        "title": "Rethinking the Ranks of Visual Channels",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Caitlyn M. McColeman",
            "Fumeng Yang",
            "Timothy F. Brady",
            "Steven Franconeri"
        ],
        "DOI": "10.1109/TVCG.2021.3114684",
        "citation": 12,
        "abstract": "Data can be visually represented using visual channels like position, length or luminance. An existing ranking of these visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is surprisingly little existing work that tests this assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs, across displays that almost universally depict more than two values. To simulate the information extracted from a glance at a visualization, we instead asked participants to immediately reproduce a set of values from memory after they were shown the visualization. These values could be shown in a bar graph (position (bar)), line graph (position (line)), heat map (luminance), bubble chart (area), misaligned bar graph (length), or ‘wind map’ (angle). With a Bayesian multilevel modeling approach, we show how the rank positions of visual channels shift across different numbers of marks (2, 4 or 8) and for bias, precision, and error measures. The ranking did not hold, even for reproductions of only 2 marks, and the new probabilistic ranking was highly inconsistent for reproductions of different numbers of marks. Other factors besides channel choice had an order of magnitude more influence on performance, such as the number of values in the series (e.g., more marks led to larger errors), or the value of each mark (e.g., small values were systematically overestimated). Every visual channel was worse for displays with 8 marks than 4, consistent with established limits on visual memory. These results point to the need for a body of empirical studies that move beyond two-value ratio judgments as a baseline for reliably ranking the quality of a visual channel, including testing new tasks (detection of trends or motifs), timescales (immediate computation, or later comparison), and the number of values (from a handful, to thousands)."
    },
    {
        "title": "Perception! Immersion! Empowerment! Superpowers as Inspiration for Visualization",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Wesley Willett",
            "Bon Adriel Aseniero",
            "Sheelagh Carpendale",
            "Pierre Dragicevic",
            "Yvonne Jansen",
            "Lora Oehlberg",
            "Petra Isenberg"
        ],
        "DOI": "10.1109/TVCG.2021.3114844",
        "citation": 11,
        "abstract": "We explore how the lens of fictional superpowers can help characterize how visualizations empower people and provide inspiration for new visualization systems. Researchers and practitioners often tout visualizations' ability to “make the invisible visible” and to “enhance cognitive abilities.” Meanwhile superhero comics and other modern fiction often depict characters with similarly fantastic abilities that allow them to see and interpret the world in ways that transcend traditional human perception. We investigate the intersection of these domains, and show how the language of superpowers can be used to characterize existing visualization systems and suggest opportunities for new and empowering ones. We introduce two frameworks: The first characterizes seven underlying mechanisms that form the basis for a variety of visual superpowers portrayed in fiction. The second identifies seven ways in which visualization tools and interfaces can instill a sense of empowerment in the people who use them. Building on these observations, we illustrate a diverse set of “visualization superpowers” and highlight opportunities for the visualization community to create new systems and interactions that empower new experiences with data Material and illustrations are available under CC-BY 4.0 at osf.io/8yhfz."
    },
    {
        "title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Arpit Narechania",
            "Alireza Karduni",
            "Ryan Wesslen",
            "Emily Wall"
        ],
        "DOI": "10.1109/TVCG.2021.3114820",
        "citation": 11,
        "abstract": "There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues."
    },
    {
        "title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Junxiu Tang",
            "Yuhua Zhou",
            "Tan Tang",
            "Di Weng",
            "Boyang Xie",
            "Lingyun Yu",
            "Huaqiang Zhang",
            "Yingcai Wu"
        ],
        "DOI": "10.1109/TVCG.2021.3114878",
        "citation": 11,
        "abstract": "The efficiency of warehouses is vital to e-commerce. Fast order processing at the warehouses ensures timely deliveries and improves customer satisfaction. However, monitoring, analyzing, and manipulating order processing in the warehouses in real time are challenging for traditional methods due to the sheer volume of incoming orders, the fuzzy definition of delayed order patterns, and the complex decision-making of order handling priorities. In this paper, we adopt a data-driven approach and propose OrderMonitor, a visual analytics system that assists warehouse managers in analyzing and improving order processing efficiency in real time based on streaming warehouse event data. Specifically, the order processing pipeline is visualized with a novel pipeline design based on the sedimentation metaphor to facilitate real-time order monitoring and suggest potentially abnormal orders. We also design a novel visualization that depicts order timelines based on the Gantt charts and Marey's graphs. Such a visualization helps the managers gain insights into the performance of order processing and find major blockers for delayed orders. Furthermore, an evaluating view is provided to assist users in inspecting order details and assigning priorities to improve the processing performance. The effectiveness of OrderMonitor is evaluated with two case studies on a real-world warehouse dataset."
    },
    {
        "title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Tan Tang",
            "Yanhong Wu",
            "Yingcai Wu",
            "Lingyun Yu",
            "Yuhong Li"
        ],
        "DOI": "10.1109/TVCG.2021.3114781",
        "citation": 11,
        "abstract": "Video moderation, which refers to remove deviant or explicit content from e-commerce livestreams, has become prevalent owing to social and engaging features. However, this task is tedious and time consuming due to the difficulties associated with watching and reviewing multimodal video content, including video frames and audio clips. To ensure effective video moderation, we propose VideoModerator, a risk-aware framework that seamlessly integrates human knowledge with machine insights. This framework incorporates a set of advanced machine learning models to extract the risk-aware features from multimodal video content and discover potentially deviant videos. Moreover, this framework introduces an interactive visualization interface with three views, namely, a video view, a frame view, and an audio view. In the video view, we adopt a segmented timeline and highlight high-risk periods that may contain deviant information. In the frame view, we present a novel visual summarization method that combines risk-aware features and video context to enable quick video navigation. In the audio view, we employ a storyline-based design to provide a multi-faceted overview which can be used to explore audio content. Furthermore, we report the usage of VideoModerator through a case scenario and conduct experiments and a controlled user study to validate its effectiveness."
    },
    {
        "title": "Visual Arrangements of Bar Charts Influence Comparisons in Viewer Takeaways",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Cindy Xiong",
            "Vidya Setlur",
            "Benjamin Bach",
            "Eunyee Koh",
            "Kylie Lin",
            "Steven Franconeri"
        ],
        "DOI": "10.1109/TVCG.2021.3114823",
        "citation": 10,
        "abstract": "Well-designed data visualizations can lead to more powerful and intuitive processing by a viewer. To help a viewer intuitively compare values to quickly generate key takeaways, visualization designers can manipulate how data values are arranged in a chart to afford particular comparisons. Using simple bar charts as a case study, we empirically tested the comparison affordances of four common arrangements: vertically juxtaposed, horizontally juxtaposed, overlaid, and stacked. We asked participants to type out what patterns they perceived in a chart and we coded their takeaways into types of comparisons. In a second study, we asked data visualization design experts to predict which arrangement they would use to afford each type of comparison and found both alignments and mismatches with our findings. These results provide concrete guidelines for how both human designers and automatic chart recommendation systems can make visualizations that help viewers extract the “right” takeaway."
    },
    {
        "title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Théo Jaunet",
            "Corentin Kervadec",
            "Romain Vuillemot",
            "Grigory Antipov",
            "Moez Baccouche",
            "Christian Wolf"
        ],
        "DOI": "10.1109/TVCG.2021.3114683",
        "citation": 10,
        "abstract": "Visual Question Answering systems target answering open-ended textual questions given input images. They are a testbed for learning high-level reasoning with a primary use in HCI, for instance assistance for the visually impaired. Recent research has shown that state-of-the-art models tend to produce answers exploiting biases and shortcuts in the training data, and sometimes do not even look at the input image, instead of performing the required reasoning steps. We present VisQA, a visual analytics tool that explores this question of reasoning vs. bias exploitation. It exposes the key element of state-of-the-art neural models — attention maps in transformers. Our working hypothesis is that reasoning steps leading to model predictions are observable from attention distributions, which are particularly useful for visualization. The design process of VisQA was motivated by well-known bias examples from the fields of deep learning and vision-language reasoning and evaluated in two ways. First, as a result of a collaboration of three fields, machine learning, vision and language reasoning, and data analytics, the work lead to a better understanding of bias exploitation of neural models for VQA, which eventually resulted in an impact on its design and training through the proposition of a method for the transfer of reasoning patterns from an oracle model. Second, we also report on the design of VisQA, and a goal-oriented evaluation of VisQA targeting the analysis of a model decision process from multiple experts, providing evidence that it makes the inner workings of models accessible to users."
    },
    {
        "title": "Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Pramod Chundury",
            "Biswaksen Patnaik",
            "Yasmin Reyazuddin",
            "Christine Tang",
            "Jonathan Lazar",
            "Niklas Elmqvist"
        ],
        "DOI": "10.1109/TVCG.2021.3114829",
        "citation": 10,
        "abstract": "For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&M experts—all of them blind—to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible—using sonification and auralization. However, our experts recommended supporting a combination of senses—sound and touch—to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals."
    },
    {
        "title": "Propagating Visual Designs to Numerous Plots and Dashboards",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Saiful Khan",
            "Phong H. Nguyen",
            "Alfie Abdul-Rahman",
            "Benjamin Bach",
            "Min Chen",
            "Euan Freeman",
            "Cagatay Turkay"
        ],
        "DOI": "10.1109/TVCG.2021.3114828",
        "citation": 9,
        "abstract": "In the process of developing an infrastructure for providing visualization and visual analytics (VIS) tools to epidemiologists and modeling scientists, we encountered a technical challenge for applying a number of visual designs to numerous datasets rapidly and reliably with limited development resources. In this paper, we present a technical solution to address this challenge. Operationally, we separate the tasks of data management, visual designs, and plots and dashboard deployment in order to streamline the development workflow. Technically, we utilize: an ontology to bring datasets, visual designs, and deployable plots and dashboards under the same management framework; multi-criteria search and ranking algorithms for discovering potential datasets that match a visual design; and a purposely-design user interface for propagating each visual design to appropriate datasets (often in tens and hundreds) and quality-assuring the propagation before the deployment. This technical solution has been used in the development of the RAMPVIS infrastructure for supporting a consortium of epidemiologists and modeling scientists through visualization."
    },
    {
        "title": "Attribute-based Explanation of Non-Linear Embeddings of High-Dimensional Data",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Jan-Tobias Sohns",
            "Michaela Schmitt",
            "Fabian Jirasek",
            "Hans Hasse",
            "Heike Leitte"
        ],
        "DOI": "10.1109/TVCG.2021.3114870",
        "citation": 9,
        "abstract": "Embeddings of high-dimensional data are widely used to explore data, to verify analysis results, and to communicate information. Their explanation, in particular with respect to the input attributes, is often difficult. With linear projects like PCA the axes can still be annotated meaningfully. With non-linear projections this is no longer possible and alternative strategies such as attribute-based color coding are required. In this paper, we review existing augmentation techniques and discuss their limitations. We present the Non-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation strategy for projected data (rangesets) with interactive analysis in a small multiples setting. Rangesets use a set-based visualization approach for binned attribute values that enable the user to quickly observe structure and detect outliers. We detail the link between algebraic topology and rangesets and demonstrate the utility of NoLiES in case studies with various challenges (complex attribute value distribution, many attributes, many data points) and a real-world application to understand latent features of matrix completion in thermodynamics."
    },
    {
        "title": "Automatic Narrative Summarization for Visualizing Cyber Security Logs and Incident Reports",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Robert Gove"
        ],
        "DOI": "10.1109/TVCG.2021.3114843",
        "citation": 9,
        "abstract": "Cyber security logs and incident reports describe a narrative, but in practice analysts view the data in tables where it can be difficult to follow the narrative. Narrative visualizations are useful, but common examples use a summarized narrative instead of the full story's narrative; it is unclear how to automatically generate these summaries. This paper presents (1) a narrative summarization algorithm to reduce the size and complexity of cyber security narratives with a user-customizable summarization level, and (2) a narrative visualization tailored for incident reports and network logs. An evaluation on real incident reports shows that the summarization algorithm reduces false positives and improves average precision by 41% while reducing average incident report size up to 79%. Together, the visualization and summarization algorithm generate compact representations of cyber narratives that earned praise from a SOC analyst. We further demonstrate that the summarization algorithm can apply to other types of dynamic graphs by automatically generating a summary of the Les Misérables character interaction graph. We find that the list of main characters in the automatically generated summary has substantial agreement with human-generated summaries. A version of this paper, data, and code is freely available at https://osf.io/ekzbp/."
    },
    {
        "title": "Kineticharts: Augmenting Affective Expressiveness of Charts in Data Stories with Animation Design",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Xingyu Lan",
            "Yang Shi",
            "Yanqiu Wu",
            "Xiaohan Jiao",
            "Nan Cao"
        ],
        "DOI": "10.1109/TVCG.2021.3114775",
        "citation": 9,
        "abstract": "Data stories often seek to elicit affective feelings from viewers. However, how to design affective data stories remains under-explored. In this work, we investigate one specific design factor, animation, and present Kineticharts, an animation design scheme for creating charts that express five positive affects: joy, amusement, surprise, tenderness, and excitement. These five affects were found to be frequently communicated through animation in data stories. Regarding each affect, we designed varied kinetic motions represented by bar charts, line charts, and pie charts, resulting in 60 animated charts for the five affects. We designed Kineticharts by first conducting a need-finding study with professional practitioners from data journalism and then analyzing a corpus of affective motion graphics to identify salient kinetic patterns. We evaluated Kineticharts through two user studies. The results suggest that Kineticharts can accurately convey affects, and improve the expressiveness of data stories, as well as enhance user engagement without hindering data comprehension compared to the animation design from DataClips, an authoring tool for data videos."
    },
    {
        "title": "ThreadStates: State-based Visual Analysis of Disease Progression",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Qianwen Wang",
            "Tali Mazor",
            "Theresa A Harbig",
            "Ethan Cerami",
            "Nils Gehlenborg"
        ],
        "DOI": "10.1109/TVCG.2021.3114840",
        "citation": 9,
        "abstract": "A growing number of longitudinal cohort studies are generating data with extensive patient observations across multiple timepoints. Such data offers promising opportunities to better understand the progression of diseases. However, these observations are usually treated as general events in existing visual analysis tools. As a result, their capabilities in modeling disease progression are not fully utilized. To fill this gap, we designed and implemented ThreadStates, an interactive visual analytics tool for the exploration of longitudinal patient cohort data. The focus of ThreadStates is to identify the states of disease progression by learning from observation data in a human-in-the-loop manner. We propose a novel Glyph Matrix design and combine it with a scatter plot to enable seamless identification, observation, and refinement of states. The disease progression patterns are then revealed in terms of state transitions using Sankey-based visualizations. We employ sequence clustering techniques to find patient groups with distinctive progression patterns, and to reveal the association between disease progression and patient-level features. The design and development were driven by a requirement analysis and iteratively refined based on feedback from domain experts over the course of a 10-month design study. Case studies and expert interviews demonstrate that ThreadStates can successively summarize disease states, reveal disease progression, and compare patient groups."
    },
    {
        "title": "Left, Right, and Gender: Exploring Interaction Traces to Mitigate Human Biases",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Emily Wall",
            "Arpit Narechania",
            "Adam Coscia",
            "Jamal Paden",
            "Alex Endert"
        ],
        "DOI": "10.1109/TVCG.2021.3114862",
        "citation": 9,
        "abstract": "Human biases impact the way people analyze data and make decisions. Recent work has shown that some visualization designs can better support cognitive processes and mitigate cognitive biases (i.e., errors that occur due to the use of mental “shortcuts”). In this work, we explore how visualizing a user's interaction history (i.e., which data points and attributes a user has interacted with) can be used to mitigate potential biases that drive decision making by promoting conscious reflection of one's analysis process. Given an interactive scatterplot-based visualization tool, we showed interaction history in real-time while exploring data (by coloring points in the scatterplot that the user has interacted with), and in a summative format after a decision has been made (by comparing the distribution of user interactions to the underlying distribution of the data). We conducted a series of in-lab experiments and a crowd-sourced experiment to evaluate the effectiveness of interaction history interventions toward mitigating bias. We contextualized this work in a political scenario in which participants were instructed to choose a committee of 10 fictitious politicians to review a recent bill passed in the U.S. state of Georgia banning abortion after 6 weeks, where things like gender bias or political party bias may drive one's analysis process. We demonstrate the generalizability of this approach by evaluating a second decision making scenario related to movies. Our results are inconclusive for the effectiveness of interaction history (henceforth referred to as interaction traces) toward mitigating biased decision making. However, we find some mixed support that interaction traces, particularly in a summative format, can increase awareness of potential unconscious biases."
    },
    {
        "title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Natkamon Tovanich",
            "Nicolas Soulié",
            "Nicolas Heulot",
            "Petra Isenberg"
        ],
        "DOI": "10.1109/TVCG.2021.3114821",
        "citation": 8,
        "abstract": "We present a visual analytics tool, MiningVis, to explore the long-term historical evolution and dynamics of the Bitcoin mining ecosystem. Bitcoin is a cryptocurrency that attracts much attention but remains difficult to understand. Particularly important to the success, stability, and security of Bitcoin is a component of the system called “mining.” Miners are responsible for validating transactions and are incentivized to participate by the promise of a monetary reward. Mining pools have emerged as collectives of miners that ensure a more stable and predictable income. MiningVis aims to help analysts understand the evolution and dynamics of the Bitcoin mining ecosystem, including mining market statistics, multi-measure mining pool rankings, and pool hopping behavior. Each of these features can be compared to external data concerning pool characteristics and Bitcoin news. In order to assess the value of MiningVis, we conducted online interviews and insight-based user studies with Bitcoin miners. We describe research questions tackled and insights made by our participants and illustrate practical implications for visual analytics systems for Bitcoin mining."
    },
    {
        "title": "Understanding Data Visualization Design Practice",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Paul Parsons"
        ],
        "DOI": "10.1109/TVCG.2021.3114959",
        "citation": 8,
        "abstract": "Professional roles for data visualization designers are growing in popularity, and interest in relationships between the academic research and professional practice communities is gaining traction. However, despite the potential for knowledge sharing between these communities, we have little understanding of the ways in which practitioners design in real-world, professional settings. Inquiry in numerous design disciplines indicates that practitioners approach complex situations in ways that are fundamentally different from those of researchers. In this work, I take a practice-led approach to understanding visualization design practice on its own terms. Twenty data visualization practitioners were interviewed and asked about their design process, including the steps they take, how they make decisions, and the methods they use. Findings suggest that practitioners do not follow highly systematic processes, but instead rely on situated forms of knowing and acting in which they draw from precedent and use methods and principles that are determined appropriate in the moment. These findings have implications for how visualization researchers understand and engage with practitioners, and how educators approach the training of future data visualization designers."
    },
    {
        "title": "A Mixed-Initiative Approach to Reusing Infographic Charts",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Weiwei Cui",
            "Jinpeng Wang",
            "He Huang",
            "Yun Wang",
            "Chin-Yew Lin",
            "Haidong Zhang",
            "Dongmei Zhang"
        ],
        "DOI": "10.1109/TVCG.2021.3114856",
        "citation": 8,
        "abstract": "Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples."
    },
    {
        "title": "NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Haekyu Park",
            "Nilaksh Das",
            "Rahul Duggal",
            "Austin P. Wright",
            "Omar Shaikh",
            "Fred Hohman",
            "Duen Horng Polo Chau"
        ],
        "DOI": "10.1109/TVCG.2021.3114858",
        "citation": 8,
        "abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting “dog faces” of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting “dog face” and “dog tail” are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced."
    },
    {
        "title": "A Memory Efficient Encoding for Ray Tracing Large Unstructured Data",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Ingo Wald",
            "Nate Morrical",
            "Stefan Zellmann"
        ],
        "DOI": "10.1109/TVCG.2021.3114869",
        "citation": 8,
        "abstract": "In theory, efficient and high-quality rendering of unstructured data should greatly benefit from modern GPUs, but in practice, GPUs are often limited by the large amount of memory that large meshes require for element representation and for sample reconstruction acceleration structures. We describe a memory-optimized encoding for large unstructured meshes that efficiently encodes both the unstructured mesh and corresponding sample reconstruction acceleration structure, while still allowing for fast random-access sampling as required for rendering. We demonstrate that for large data our encoding allows for rendering even the 2.9 billion element Mars Lander on a single off-the-shelf GPU-and the largest 6.3 billion version on a pair of such GPUs."
    },
    {
        "title": "Measuring and Explaining the Inter-Cluster Reliability of Multidimensional Projections",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Hyeon Jeon",
            "Hyung-Kwon Ko",
            "Jaemin Jo",
            "Youngtaek Kim",
            "Jinwook Seo"
        ],
        "DOI": "10.1109/TVCG.2021.3114833",
        "citation": 8,
        "abstract": "We propose Steadiness and Cohesiveness, two novel metrics to measure the inter-cluster reliability of multidimensional projection (MDP), specifically how well the inter-cluster structures are preserved between the original high-dimensional space and the low-dimensional projection space. Measuring inter-cluster reliability is crucial as it directly affects how well inter-cluster tasks (e.g., identifying cluster relationships in the original space from a projected view) can be conducted; however, despite the importance of inter-cluster tasks, we found that previous metrics, such as Trustworthiness and Continuity, fail to measure inter-cluster reliability. Our metrics consider two aspects of the inter-cluster reliability: Steadiness measures the extent to which clusters in the projected space form clusters in the original space, and Cohesiveness measures the opposite. They extract random clusters with arbitrary shapes and positions in one space and evaluate how much the clusters are stretched or dispersed in the other space. Furthermore, our metrics can quantify pointwise distortions, allowing for the visualization of inter-cluster reliability in a projection, which we call a reliability map. Through quantitative experiments, we verify that our metrics precisely capture the distortions that harm inter-cluster reliability while previous metrics have difficulty capturing the distortions. A case study also demonstrates that our metrics and the reliability map 1) support users in selecting the proper projection techniques or hyperparameters and 2) prevent misinterpretation while performing inter-cluster tasks, thus allow an adequate identification of inter-cluster structure."
    },
    {
        "title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Huan Song",
            "Zeng Dai",
            "Panpan Xu",
            "Liu Ren"
        ],
        "DOI": "10.1109/TVCG.2021.3114857",
        "citation": 8,
        "abstract": "Graphs are a ubiquitous data structure to model processes and relations in a wide range of domains. Examples include control-flow graphs in programs and semantic scene graphs in images. Identifying subgraph patterns in graphs is an important approach to understand their structural properties. We propose a visual analytics system GraphQ to support human-in-the-loop, example-based, subgraph pattern search in a database containing many individual graphs. To support fast, interactive queries, we use graph neural networks (GNNs) to encode a graph as fixed-length latent vector representation, and perform subgraph matching in the latent space. Due to the complexity of the problem, it is still difficult to obtain accurate one-to-one node correspondences in the matching results that are crucial for visualization and interpretation. We, therefore, propose a novel GNN for node-alignment called NeuroAlign, to facilitate easy validation and interpretation of the query results. GraphQ provides a visual query interface with a query editor and a multi-scale visualization of the results, as well as a user feedback mechanism for refining the results with additional constraints. We demonstrate GraphQ through two example usage scenarios: analyzing reusable subroutines in program workflows and semantic scene graph search in images. Quantitative experiments show that NeuroAlign achieves 19%-29% improvement in node-alignment accuracy compared to baseline GNN and provides up to 100× speedup compared to combinatorial algorithms. Our qualitative study with domain experts confirms the effectiveness for both usage scenarios."
    },
    {
        "title": "From Jam Session to Recital: Synchronous Communication and Collaboration Around Data in Organizations",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Matthew Brehmer",
            "Robert Kosara"
        ],
        "DOI": "10.1109/TVCG.2021.3114760",
        "citation": 8,
        "abstract": "Prior research on communicating with visualization has focused on public presentation and asynchronous individual consumption, such as in the domain of journalism. The visualization research community knows comparatively little about synchronous and multimodal communication around data within organizations, from team meetings to executive briefings. We conducted two qualitative interview studies with individuals who prepare and deliver presentations about data to audiences in organizations. In contrast to prior work, we did not limit our interviews to those who self-identify as data analysts or data scientists. Both studies examined aspects of speaking about data with visual aids such as charts, dashboards, and tables. One study was a retrospective examination of current practices and difficulties, from which we identified three scenarios involving presentations of data. We describe these scenarios using an analogy to musical performance: small collaborative team meetings are akin to jam session, while more structured presentations can range from semi-improvisational performances among peers to formal recitals given to executives or customers. In our second study, we grounded the discussion around three design probes, each examining a different aspect of presenting data: the progressive reveal of visualization to direct attention and advance a narrative, visualization presentation controls that are hidden from the audience's view, and the coordination of a presenter's video with interactive visualization. Our distillation of interviewees' responses surfaced twelve themes, from ways of authoring presentations to creating accessible and engaging audience experiences."
    },
    {
        "title": "Geo-Context Aware Study of Vision-Based Autonomous Driving Models and Spatial Video Data",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Suphanut Jamonnak",
            "Ye Zhao",
            "Xinyi Huang",
            "Md Amiruzzaman"
        ],
        "DOI": "10.1109/TVCG.2021.3114853",
        "citation": 8,
        "abstract": "Vision-based deep learning (DL) methods have made great progress in learning autonomous driving models from large-scale crowd-sourced video datasets. They are trained to predict instantaneous driving behaviors from video data captured by on-vehicle cameras. In this paper, we develop a geo-context aware visualization system for the study of Autonomous Driving Model (ADM) predictions together with large-scale ADM video data. The visual study is seamlessly integrated with the geographical environment by combining DL model performance with geospatial visualization techniques. Model performance measures can be studied together with a set of geospatial attributes over map views. Users can also discover and compare prediction behaviors of multiple DL models in both city-wide and street-level analysis, together with road images and video contents. Therefore, the system provides a new visual exploration platform for DL model designers in autonomous driving. Use cases and domain expert evaluation show the utility and effectiveness of the visualization system."
    },
    {
        "title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Johannes Knittel",
            "Steffen Koch",
            "Tan Tang",
            "Wei Chen",
            "Yingcai Wu",
            "Shixia Liu",
            "Thomas Ertl"
        ],
        "DOI": "10.1109/TVCG.2021.3114800",
        "citation": 8,
        "abstract": "Breaking news and first-hand reports often trend on social media platforms before traditional news outlets cover them. The real-time analysis of posts on such platforms can reveal valuable and timely insights for journalists, politicians, business analysts, and first responders, but the high number and diversity of new posts pose a challenge. In this work, we present an interactive system that enables the visual analysis of streaming social media data on a large scale in real-time. We propose an efficient and explainable dynamic clustering algorithm that powers a continuously updated visualization of the current thematic landscape as well as detailed visual summaries of specific topics of interest. Our parallel clustering strategy provides an adaptive stream with a digestible but diverse selection of recent posts related to relevant topics. We also integrate familiar visual metaphors that are highly interlinked for enabling both explorative and more focused monitoring tasks. Analysts can gradually increase the resolution to dive deeper into particular topics. In contrast to previous work, our system also works with non-geolocated posts and avoids extensive preprocessing such as detecting events. We evaluated our dynamic clustering algorithm and discuss several use cases that show the utility of our system."
    },
    {
        "title": "Gender in 30 Years of IEEE Visualization",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Natkamon Tovanich",
            "Pierre Dragicevic",
            "Petra Isenberg"
        ],
        "DOI": "10.1109/TVCG.2021.3114787",
        "citation": 7,
        "abstract": "We present an exploratory analysis of gender representation among the authors, committee members, and award winners at the IEEE Visualization (VIS) conference over the last 30 years. Our goal is to provide descriptive data on which diversity discussions and efforts in the community can build. We look in particular at the gender of VIS authors as a proxy for the community at large. We consider measures of overall gender representation among authors, differences in careers, positions in author lists, and collaborations. We found that the proportion of female authors has increased from 9% in the first five years to 22% in the last five years of the conference. Over the years, we found the same representation of women in program committees and slightly more women in organizing committees. Women are less likely to appear in the last author position, but more in the middle positions. In terms of collaboration patterns, female authors tend to collaborate more than expected with other women in the community. All non-gender related data is available on https://osf.io/ydfj4/ and the gender-author matching can be accessed through https://nyu.databrary.org/volume/1301."
    },
    {
        "title": "An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Hyeok Kim",
            "Ryan Rossi",
            "Abhraneel Sarma",
            "Dominik Moritz",
            "Jessica Hullman"
        ],
        "DOI": "10.1109/TVCG.2021.3114782",
        "citation": 7,
        "abstract": "Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation."
    },
    {
        "title": "SightBi: Exploring Cross-View Data Relationships with Biclusters",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Maoyuan Sun",
            "Abdul Rahman Shaikh",
            "Hamed Alhoori",
            "Jian Zhao"
        ],
        "DOI": "10.1109/TVCG.2021.3114801",
        "citation": 7,
        "abstract": "Multiple-view visualization (MV) has been heavily used in visual analysis tools for sensemaking of data in various domains (e.g., bioinformatics, cybersecurity and text analytics). One common task of visual analysis with multiple views is to relate data across different views. For example, to identify threats, an intelligence analyst needs to link people from a social network graph with locations on a crime-map, and then search for and read relevant documents. Currently, exploring cross-view data relationships heavily relies on view-coordination techniques (e.g., brushing and linking), which may require significant user effort on many trial-and-error attempts, such as repetitiously selecting elements in one view, and then observing and following elements highlighted in other views. To address this, we present SightBi, a visual analytics approach for supporting cross-view data relationship explorations. We discuss the design rationale of SightBi in detail, with identified user tasks regarding the use of cross-view data relationships. SightBi formalizes cross-view data relationships as biclusters, computes them from a dataset, and uses a bi-context design that highlights creating stand-alone relationship-views. This helps preserve existing views and offers an overview of cross-view data relationships to guide user exploration. Moreover, SightBi allows users to interactively manage the layout of multiple views by using newly created relationship-views. With a usage scenario, we demonstrate the usefulness of SightBi for sensemaking of cross-view data relationships."
    },
    {
        "title": "CoUX: Collaborative Visual Analysis of Think-Aloud Usability Test Videos for Digital Interfaces",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Ehsan Jahangirzadeh Soure",
            "Emily Kuang",
            "Mingming Fan",
            "Jian Zhao"
        ],
        "DOI": "10.1109/TVCG.2021.3114822",
        "citation": 7,
        "abstract": "Reviewing a think-aloud video is both time-consuming and demanding as it requires UX (user experience) professionals to attend to many behavioral signals of the user in the video. Moreover, challenges arise when multiple UX professionals need to collaborate to reduce bias and errors. We propose a collaborative visual analytics tool, CoUX, to facilitate UX evaluators collectively reviewing think-aloud usability test videos of digital interfaces. CoUX seamlessly supports usability problem identification, annotation, and discussion in an integrated environment. To ease the discovery of usability problems, CoUX visualizes a set of problem-indicators based on acoustic, textual, and visual features extracted from the video and audio of a think-aloud session with machine learning. CoUX further enables collaboration amongst UX evaluators for logging, commenting, and consolidating the discovered problems with a chatbox-like user interface. We designed CoUX based on a formative study with two UX experts and insights derived from the literature. We conducted a user study with six pairs of UX practitioners on collaborative think-aloud video analysis tasks. The results indicate that CoUX is useful and effective in facilitating both problem identification and collaborative teamwork. We provide insights into how different features of CoUX were used to support both independent analysis and collaboration. Furthermore, our work highlights opportunities to improve collaborative usability test video analysis."
    },
    {
        "title": "Joint t-SNE for Comparable Projections of Multiple High-Dimensional Datasets",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Yinqiao Wang",
            "Lu Chen",
            "Jaemin Jo",
            "Yunhai Wang"
        ],
        "DOI": "10.1109/TVCG.2021.3114765",
        "citation": 7,
        "abstract": "We present Joint t-Stochastic Neighbor Embedding (Joint t-SNE), a technique to generate comparable projections of multiple high-dimensional datasets. Although t-SNE has been widely employed to visualize high-dimensional datasets from various domains, it is limited to projecting a single dataset. When a series of high-dimensional datasets, such as datasets changing over time, is projected independently using t-SNE, misaligned layouts are obtained. Even items with identical features across datasets are projected to different locations, making the technique unsuitable for comparison tasks. To tackle this problem, we introduce edge similarity, which captures the similarities between two adjacent time frames based on the Graphlet Frequency Distribution (GFD). We then integrate a novel loss term into the t-SNE loss function, which we call vector constraints, to preserve the vectors between projected points across the projections, allowing these points to serve as visual landmarks for direct comparisons between projections. Using synthetic datasets whose ground-truth structures are known, we show that Joint t-SNE outperforms existing techniques, including Dynamic t-SNE, in terms of local coherence error, Kullback-Leibler divergence, and neighborhood preservation. We also showcase a real-world use case to visualize and compare the activation of different layers of a neural network."
    },
    {
        "title": "Causal Support: Modeling Causal Inferences with Visualizations",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Alex Kale",
            "Yifan Wu",
            "Jessica Hullman"
        ],
        "DOI": "10.1109/TVCG.2021.3114824",
        "citation": 7,
        "abstract": "Analysts often make visual causal inferences about possible data-generating models. However, visual analytics (VA) software tends to leave these models implicit in the mind of the analyst, which casts doubt on the statistical validity of informal visual “insights”. We formally evaluate the quality of causal inferences from visualizations by adopting causal support—a Bayesian cognition model that learns the probability of alternative causal explanations given some data—as a normative benchmark for causal inferences. We contribute two experiments assessing how well crowdworkers can detect (1) a treatment effect and (2) a confounding relationship. We find that chart users' causal inferences tend to be insensitive to sample size such that they deviate from our normative benchmark. While interactively cross-filtering data in visualizations can improve sensitivity, on average users do not perform reliably better with common visualizations than they do with textual contingency tables. These experiments demonstrate the utility of causal support as an evaluation framework for inferences in VA and point to opportunities to make analysts' mental models more explicit in VA software."
    },
    {
        "title": "Scope2Screen: Focus+Context Techniques for Pathology Tumor Assessment in Multivariate Image Data",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Jared Jessup",
            "Robert Krueger",
            "Simon Warchol",
            "John Hoffer",
            "Jeremy Muhlich",
            "Cecily C. Ritch",
            "Giorgio Gaglia",
            "Shannon Coy",
            "Yu-An Chen",
            "Jia-Ren Lin",
            "Sandro Santagata",
            "Peter K. Sorger",
            "Hanspeter Pfister"
        ],
        "DOI": "10.1109/TVCG.2021.3114786",
        "citation": 6,
        "abstract": "Inspection of tissues using a light microscope is the primary method of diagnosing many diseases, notably cancer. Highly multiplexed tissue imaging builds on this foundation, enabling the collection of up to 60 channels of molecular information plus cell and tissue morphology using antibody staining. This provides unique insight into disease biology and promises to help with the design of patient-specific therapies. However, a substantial gap remains with respect to visualizing the resulting multivariate image data and effectively supporting pathology workflows in digital environments on screen. We, therefore, developed Scope2Screen, a scalable software system for focus+context exploration and annotation of whole-slide, high-plex, tissue images. Our approach scales to analyzing 100GB images of 109 or more pixels per channel, containing millions of individual cells. A multidisciplinary team of visualization experts, microscopists, and pathologists identified key image exploration and annotation tasks involving finding, magnifying, quantifying, and organizing regions of interest (ROIs) in an intuitive and cohesive manner. Building on a scope-to-screen metaphor, we present interactive lensing techniques that operate at single-cell and tissue levels. Lenses are equipped with task-specific functionality and descriptive statistics, making it possible to analyze image features, cell types, and spatial arrangements (neighborhoods) across image channels and scales. A fast sliding-window search guides users to regions similar to those under the lens; these regions can be analyzed and considered either separately or as part of a larger image collection. A novel snapshot method enables linked lens configurations and image statistics to be saved, restored, and shared with these regions. We validate our designs with domain experts and apply Scope2Screen in two case studies involving lung and colorectal cancers to discover cancer-relevant image features."
    },
    {
        "title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Tiankai Xie",
            "Yuxin Ma",
            "Jian Kang",
            "Hanghang Tong",
            "Ross Maciejewski"
        ],
        "DOI": "10.1109/TVCG.2021.3114850",
        "citation": 6,
        "abstract": "Graph mining is an essential component of recommender systems and search engines. Outputs of graph mining models typically provide a ranked list sorted by each item's relevance or utility. However, recent research has identified issues of algorithmic bias in such models, and new graph mining algorithms have been proposed to correct for bias. As such, algorithm developers need tools that can help them uncover potential biases in their models while also exploring the impacts of correcting for biases when employing fairness-aware algorithms. In this paper, we present FairRankVis, a visual analytics framework designed to enable the exploration of multi-class bias in graph mining algorithms. We support both group and individual fairness levels of comparison. Our framework is designed to enable model developers to compare multi-class fairness between algorithms (for example, comparing PageRank with a debiased PageRank algorithm) to assess the impacts of algorithmic debiasing with respect to group and individual fairness. We demonstrate our framework through two usage scenarios inspecting algorithmic fairness."
    },
    {
        "title": "Modeling Just Noticeable Differences in Charts",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Min Lu",
            "Joel Lanir",
            "Chufeng Wang",
            "Yucong Yao",
            "Wen Zhang",
            "Oliver Deussen",
            "Hui Huang"
        ],
        "DOI": "10.1109/TVCG.2021.3114874",
        "citation": 6,
        "abstract": "One of the fundamental tasks in visualization is to compare two or more visual elements. However, it is often difficult to visually differentiate graphical elements encoding a small difference in value, such as the heights of similar bars in bar chart or angles of similar sections in pie chart. Perceptual laws can be used in order to model when and how we perceive this difference. In this work, we model the perception of Just Noticeable Differences (JNDs), the minimum difference in visual attributes that allow faithfully comparing similar elements, in charts. Specifically, we explore the relation between JNDs and two major visual variables: the intensity of visual elements and the distance between them, and study it in three charts: bar chart, pie chart and bubble chart. Through an empirical study, we identify main effects on JND for distance in bar charts, intensity in pie charts, and both distance and intensity in bubble charts. By fitting a linear mixed effects model, we model JND and find that JND grows as the exponential function of variables. We highlight several usage scenarios that make use of the JND modeling in which elements below the fitted JND are detected and enhanced with secondary visual cues for better discrimination."
    },
    {
        "title": "Loon: Using Exemplars to Visualize Large-Scale Microscopy Data",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Devin Lange",
            "Eddie Polanco",
            "Robert Judson-Torres",
            "Thomas Zangle",
            "Alexander Lex"
        ],
        "DOI": "10.1109/TVCG.2021.3114766",
        "citation": 6,
        "abstract": "Which drug is most promising for a cancer patient? A new microscopy-based approach for measuring the mass of individual cancer cells treated with different drugs promises to answer this question in only a few hours. However, the analysis pipeline for extracting data from these images is still far from complete automation: human intervention is necessary for quality control for preprocessing steps such as segmentation, adjusting filters, removing noise, and analyzing the result. To address this workflow, we developed Loon, a visualization tool for analyzing drug screening data based on quantitative phase microscopy imaging. Loon visualizes both derived data such as growth rates and imaging data. Since the images are collected automatically at a large scale, manual inspection of images and segmentations is infeasible. However, reviewing representative samples of cells is essential, both for quality control and for data analysis. We introduce a new approach for choosing and visualizing representative exemplar cells that retain a close connection to the low-level data. By tightly integrating the derived data visualization capabilities with the novel exemplar visualization and providing selection and filtering capabilities, Loon is well suited for making decisions about which drugs are suitable for a specific patient."
    },
    {
        "title": "An Efficient Dual-Hierarchy t-SNE Minimization",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Mark van de Ruit",
            "Markus Billeter",
            "Elmar Eisemann"
        ],
        "DOI": "10.1109/TVCG.2021.3114817",
        "citation": 6,
        "abstract": "t-distributed Stochastic Neighbour Embedding (t-SNE) has become a standard for exploratory data analysis, as it is capable of revealing clusters even in complex data while requiring minimal user input. While its run-time complexity limited it to small datasets in the past, recent efforts improved upon the expensive similarity computations and the previously quadratic minimization. Nevertheless, t-SNE still has high runtime and memory costs when operating on millions of points. We present a novel method for executing the t-SNE minimization. While our method overall retains a linear runtime complexity, we obtain a significant performance increase in the most expensive part of the minimization. We achieve a significant improvement without a noticeable decrease in accuracy even when targeting a 3D embedding. Our method constructs a pair of spatial hierarchies over the embedding, which are simultaneously traversed to approximate many N-body interactions at once. We demonstrate an efficient GPGPU implementation and evaluate its performance against state-of-the-art methods on a variety of datasets."
    },
    {
        "title": "STRATISFIMAL LAYOUT: A modular optimization model for laying out layered node-link network visualizations",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Sara di Bartolomeo",
            "Mirek Riedewald",
            "Wolfgang Gatterbauer",
            "Cody Dunne"
        ],
        "DOI": "10.1109/TVCG.2021.3114756",
        "citation": 6,
        "abstract": "Node-link visualizations are a familiar and powerful tool for displaying the relationships in a network. The readability of these visualizations highly depends on the spatial layout used for the nodes. In this paper, we focus on computing layered layouts, in which nodes are aligned on a set of parallel axes to better expose hierarchical or sequential relationships. Heuristic-based layouts are widely used as they scale well to larger networks and usually create readable, albeit sub-optimal, visualizations. We instead use a layout optimization model that prioritizes optimality - as compared to scalability - because an optimal solution not only represents the best attainable result, but can also serve as a baseline to evaluate the effectiveness of layout heuristics. We take an important step towards powerful and flexible network visualization by proposing Stratisfimal Layout, a modular integer-linear-programming formulation that can consider several important readability criteria simultaneously — crossing reduction, edge bendiness, and nested and multi-layer groups. The layout can be adapted to diverse use cases through its modularity. Individual features can be enabled and customized depending on the application. We provide open-source and documented implementations of the layout, both for web-based and desktop visualizations. As a proof-of-concept, we apply it to the problem of visualizing complicated SQL queries, which have features that we believe cannot be addressed by existing layout optimization models. We also include a benchmark network generator and the results of an empirical evaluation to assess the performance trade-offs of our design choices. A full version of this paper with all appendices, data, and source code is available at osf.io/qdyt9 with live examples at https://visdunneright.github.io/stratisfimal/."
    },
    {
        "title": "Lumos: Increasing Awareness of Analytic Behavior during Visual Data Analysis",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Arpit Narechania",
            "Adam Coscia",
            "Emily Wall",
            "Alex Endert"
        ],
        "DOI": "10.1109/TVCG.2021.3114827",
        "citation": 6,
        "abstract": "Visual data analysis tools provide people with the agency and flexibility to explore data using a variety of interactive functionalities. However, this flexibility may introduce potential consequences in situations where users unknowingly overemphasize or underemphasize specific subsets of the data or attribute space they are analyzing. For example, users may overemphasize specific attributes and/or their values (e.g., Gender is always encoded on the X axis), underemphasize others (e.g., Religion is never encoded), ignore a subset of the data (e.g., older people are filtered out), etc. In response, we present Lumos, a visual data analysis tool that captures and shows the interaction history with data to increase awareness of such analytic behaviors. Using in-situ (at the place of interaction) and ex-situ (in an external view) visualization techniques, Lumos provides real-time feedback to users for them to reflect on their activities. For example, Lumos highlights datapoints that have been previously examined in the same visualization (in-situ) and also overlays them on the underlying data distribution (i.e., baseline distribution) in a separate visualization (ex-situ). Through a user study with 24 participants, we investigate how Lumos helps users' data exploration and decision-making processes. We found that Lumos increases users' awareness of visual data analysis practices in real-time, promoting reflection upon and acknowledgement of their intentions and potentially influencing subsequent interactions."
    },
    {
        "title": "Simultaneous Matrix Orderings for Graph Collections",
        "conferenceTitle": "IEEE Transactions on Visualization and Computer Graphics",
        "date": "Jan. 2022",
        "authors": [
            "Nathan van Beusekom",
            "Wouter Meulemans",
            "Bettina Speckmann"
        ],
        "DOI": "10.1109/TVCG.2021.3114773",
        "citation": 6,
        "abstract": "Undirected graphs are frequently used to model phenomena that deal with interacting objects, such as social networks, brain activity and communication networks. The topology of an undirected graph $G$ can be captured by an adjacency matrix; this matrix in turn can be visualized directly to give insight into the graph structure. Which visual patterns appear in such a matrix visualization crucially depends on the ordering of its rows and columns. Formally defining the quality of an ordering and then automatically computing a high-quality ordering are both challenging problems; however, effective heuristics exist and are used in practice. Often, graphs do not exist in isolation but as part of a collection of graphs on the same set of vertices, for example, brain scans over time or of different people. To visualize such graph collections, we need a single ordering that works well for all matrices simultaneously. The current state-of-the-art solves this problem by taking a (weighted) union over all graphs and applying existing heuristics. However, this union leads to a loss of information, specifically in those parts of the graphs which are different. We propose a collection-aware approach to avoid this loss of information and apply it to two popular heuristic methods: leaf order and barycenter. The de-facto standard computational quality metrics for matrix ordering capture only block-diagonal patterns (cliques). Instead, we propose to use Moran's $I$, a spatial auto-correlation metric, which captures the full range of established patterns. Moran's $I$ refines previously proposed stress measures. Furthermore, the popular leaf order method heuristically optimizes a similar measure which further supports the use of Moran's $I$ in this context. An ordering that maximizes Moran's $I$ can be computed via solutions to the Traveling Salesperson Problem (TSP); orderings that approximate the optimal ordering can be computed more efficiently, using any of the approximation algorithms for metric TSP. We evaluated our methods for simultaneous orderings on real-world datasets using Moran's $I$ as the quality metric. Our results show that our collection-aware approach matches or improves performance compared to the union approach, depending on the similarity of the graphs in the collection. Specifically, our Moran's $I$-based collection-aware leaf order implementation consistently outperforms other implementations. Our collection-aware implementations carry no significant additional computational costs."
    }
]